<!DOCTYPE html>
<html>

<head><Title>NASA</Title>
<style>
     body {
        background-color: rgb(255, 0, 0);
        color: rgb(0, 0, 0);
     }

</style>


</head>
 <h1>
    NASA
    
 </h1>
 <Body>

   
    <a href="http://127.0.0.1:5500/folder1/file2.html">
        <button style="font-size: 25px; background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); ">Physics</button>
            </a>
            
            <a href="http://127.0.0.1:5500/folder1/file3.html">
                <button style="font-size: 25px; background-color: rgb(249, 249, 249); color: rgb(0, 0, 0); ">Engineering</button>
                    </a>
                   
                    <a href="http://127.0.0.1:5500/folder1/file4.html">
                        <button style="font-size: 25px; background-color: rgb(244, 238, 238); color: rgb(0, 0, 0); ">DOJ</button>
                            </a>
                            <a href="http://127.0.0.1:5500/folder1/file5.html">
                                <button style="font-size: 25px; background-color: rgb(250, 250, 250); color: rgb(0, 0, 0); ">NSA</button>
                                    </a>
                                    <a href="http://127.0.0.1:5500/folder1/file6.html">
                                        <button style="font-size: 25px; background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); ">Rocket Science</button>
                                    </a>
                                    <a href="http://127.0.0.1:5500/folder1/file7.html">
                                        <button style="font-size: 25px; background-color: rgb(250, 250, 250); color: rgb(0, 0, 0); ">University</button>
                                            </a>
                                           
                                            <a href="http://127.0.0.1:5500/folder1/file8.html">
                                                <button style="font-size: 25px; background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); ">Economics</button>
                                                    </a>
                                                    <a href="http://127.0.0.1:5500/folder1/file9.html">
                                                        <button style="font-size: 25px; background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); ">Network</button>
                                                            </a>
                                                            <a href="http://127.0.0.1:5500/folder1/file10.html">
                                                                <button style="font-size: 25px; background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); ">Notes</button>
                                                                    </a>
                                            

</body>
    <body>
<p>     
    quantum mechanics are an exchange of superposition
</p>

    <video src="video.mp4" width="500px" controls></video>
    <br<</br><br<</br><br<</br><br<</br><br<</br><br<</br>
    <p>
    What is physics? Physics is the branch of science that deals with the structure of matter and how the fundamental constituents of the universe interact. It studies objects ranging from the very small using quantum mechanics to the entire universe using general relativity.What is physics? Physics is the branch of science that deals with the structure of matter and how the fundamental constituents of the universe interact. It studies objects ranging from the very small using quantum mechanics to the entire universe using general relativity.
</p>

<img src="physics 1.jpg" alt="space" height="300">

<p>the branch of science concerned with the nature and properties of matter and energy. The subject matter of physics, distinguished from that of chemistry and biology, includes mechanics, heat, light and other radiation, sound, electricity, magnetism, and the structure of atoms.
</p>

<p>Exploration, as understood by the space community, implies a human presence in space. The purpose of this section is to provide a context for this presence, branching out from the NASA perspective to a history of the space exploration efforts of the other spacefaring nations. The rationale for the US space effort has been evolving continuously since President Kennedy called for putting a man on the moon in response to Sputnik. “Visions” have been iterated several times, reflecting changing perceptions of national purpose. However, the elements of foreseeable missions have been defined and can be expected to be a part of whatever future strategies are adopted. Space exploration is a truly international effort. It is intended that this section will also host descriptions of the efforts of NASA’s partners in the future. In addition to joint efforts with the US Department of Energy described in the current articles, NASA has a long history of collaboration with other agencies of the Fed
    

<p>National Aeronautics and Space Administration (NASA), independent U.S. governmental agency established in 1958 for the research and development of vehicles and activities for the exploration of space within and outside Earth’s atmosphere.

    Johnson Space Center: control room
    Johnson Space Center: control roomControl room of the Johnson Space Center, Houston, Texas.
    The organization is composed of four mission directorates: Aeronautics Research, for the development of advanced aviation technologies; Science, dealing with programs for understanding the origin, structure, and evolution of the universe, the solar system, and Earth; Space Technology, for the development of space science and exploration technologies; and Human Exploration and Operations, concerning the management of crewed space missions, including those to the International Space Station, as well as operations related to launch services, space transportation, and space communications for both crewed and robotic exploration programs. A number of additional research centres are affiliated, including the Goddard Space Flight Center in Greenbelt, Maryland; the Jet Propulsion Laboratory in Pasadena, California; the Johnson Space Center in Houston, Texas; and the Langley Research Center in Hampton, Virginia. Headquarters of NASA are in Washington, D.C.
    
    NASA was created largely in response to the Soviet launching of Sputnik in 1957. It was organized around the National Advisory Committee for Aeronautics (NACA), which had been created by Congress in 1915. NASA’s organization was well under way by the early years of Pres. John F. Kennedy’s administration when he proposed that the United States put a man on the Moon by the end of the 1960s. To that end, the Apollo program was designed, and in 1969 the U.S. astronaut Neil Armstrong became the first person on the Moon. All told, during nine Apollo missions, 24 astronauts (all Americans) went to the Moon, and 12 of them walked on it. Later, uncrewed programs—such as Viking, Mariner, Voyager, and Galileo—explored other bodies of the solar system.
    
    Yellow transportation sign with ASAP (as soon as possible) on violet color sky background. (acronyms)
    Britannica Quiz
    Figure Out the Acronyms ASAP Vocabulary Quiz
    space shuttle liftoff
    space shuttle liftoffLiftoff of the first U.S. space shuttle, April 12, 1981, from John F. Kennedy Space Center, Cape Canaveral, Florida.
    NASA was also responsible for the development and launching of a number of satellites with Earth applications, such as Landsat, a series of satellites designed to collect information on natural resources and other Earth features; communications satellites; and weather satellites. It also planned and developed the space shuttle, a reusable vehicle capable of carrying out missions that could not be conducted with conventional spacecraft.
    
    As part of the Artemis space program, launched in 2017, NASA aims to return humans to the Moon by 2025, with the goal of establishing a sustainable presence there and on other planets. The program also seeks to land the first woman and first person of colour on the Moon, and that woman may be Jessica Meir.</p>
    <img src="astronaut.webp" alt="space" height="300">

    <p>physics, science that deals with the structure of matter and the interactions between the fundamental constituents of the observable universe. In the broadest sense, physics (from the Greek physikos) is concerned with all aspects of nature on both the macroscopic and submicroscopic levels. Its scope of study encompasses not only the behaviour of objects under the action of given forces but also the nature and origin of gravitational, electromagnetic, and nuclear force fields. Its ultimate objective is the formulation of a few comprehensive principles that bring together and explain all such disparate phenomena.

        (Read Einstein’s 1926 Britannica essay on space-time.)
        
        Physics is the basic physical science. Until rather recent times physics and natural philosophy were used interchangeably for the science whose aim is the discovery and formulation of the fundamental laws of nature. As the modern sciences developed and became increasingly specialized, physics came to denote that part of physical science not included in astronomy, chemistry, geology, and engineering. Physics plays an important role in all the natural sciences, however, and all such fields have branches in which physical laws and measurements receive special emphasis, bearing such names as astrophysics, geophysics, biophysics, and even psychophysics. Physics can, at base, be defined as the science of matter, motion, and energy. Its laws are typically expressed with economy and precision in the language of mathematics.
        
        Both experiment, the observation of phenomena under conditions that are controlled as precisely as possible, and theory, the formulation of a unified conceptual framework, play essential and complementary roles in the advancement of physics. Physical experiments result in measurements, which are compared with the outcome predicted by theory. A theory that reliably predicts the results of experiments to which it is applicable is said to embody a law of physics. However, a law is always subject to modification, replacement, or restriction to a more limited domain, if a later experiment makes it necessary.
        
        The ultimate aim of physics is to find a unified set of laws governing matter, motion, and energy at small (microscopic) subatomic distances, at the human (macroscopic) scale of everyday life, and out to the largest distances (e.g., those on the extragalactic scale). This ambitious goal has been realized to a notable extent. Although a completely unified theory of physical phenomena has not yet been achieved (and possibly never will be), a remarkably small set of fundamental physical laws appears able to account for all known phenomena. The body of physics developed up to about the turn of the 20th century, known as classical physics, can largely account for the motions of macroscopic objects that move slowly with respect to the speed of light and for such phenomena as heat, sound, electricity, magnetism, and light. The modern developments of relativity and quantum mechanics modify these laws insofar as they apply to higher speeds, very massive objects, and to the tiny elementary constituents of matter, such as electrons, protons, and neutrons.
        
        Michael Faraday (L) English physicist and chemist (electromagnetism) and John Frederic Daniell (R) British chemist and meteorologist who invented the Daniell cell.
        Britannica Quiz
        Faces of Science
        The scope of physics
        The traditionally organized branches or fields of classical and modern physics are delineated below.
        
        Mechanics
        illustration of Robert Hooke's law of elasticity of materials
        illustration of Robert Hooke's law of elasticity of materialsIllustration of Hooke's law of elasticity of materials, showing the stretching of a spring in proportion to the applied force, from Robert Hooke's Lectures de Potentia Restitutiva (1678).
        Mechanics is generally taken to mean the study of the motion of objects (or their lack of motion) under the action of given forces. Classical mechanics is sometimes considered a branch of applied mathematics. It consists of kinematics, the description of motion, and dynamics, the study of the action of forces in producing either motion or static equilibrium (the latter constituting the science of statics). The 20th-century subjects of quantum mechanics, crucial to treating the structure of matter, subatomic particles, superfluidity, superconductivity, neutron stars, and other major phenomena, and relativistic mechanics, important when speeds approach that of light, are forms of mechanics that will be discussed later in this section.
        
        
        Are you a student?
        Get a special academic rate on Britannica Premium.
        In classical mechanics the laws are initially formulated for point particles in which the dimensions, shapes, and other intrinsic properties of bodies are ignored. Thus in the first approximation even objects as large as Earth and the Sun are treated as pointlike—e.g., in calculating planetary orbital motion. In rigid-body dynamics, the extension of bodies and their mass distributions are considered as well, but they are imagined to be incapable of deformation. The mechanics of deformable solids is elasticity; hydrostatics and hydrodynamics treat, respectively, fluids at rest and in motion.
        
        The three laws of motion set forth by Isaac Newton form the foundation of classical mechanics, together with the recognition that forces are directed quantities (vectors) and combine accordingly. The first law, also called the law of inertia, states that, unless acted upon by an external force, an object at rest remains at rest, or if in motion, it continues to move in a straight line with constant speed. Uniform motion therefore does not require a cause. Accordingly, mechanics concentrates not on motion as such but on the change in the state of motion of an object that results from the net force acting upon it. Newton’s second law equates the net force on an object to the rate of change of its momentum, the latter being the product of the mass of a body and its velocity. Newton’s third law, that of action and reaction, states that when two particles interact, the forces each exerts on the other are equal in magnitude and opposite in direction. Taken together, these mechanical laws in principle permit the determination of the future motions of a set of particles, providing their state of motion is known at some instant, as well as the forces that act between them and upon them from the outside. From this deterministic character of the laws of classical mechanics, profound (and probably incorrect) philosophical conclusions have been drawn in the past and even applied to human history.
        
        Lying at the most basic level of physics, the laws of mechanics are characterized by certain symmetry properties, as exemplified in the aforementioned symmetry between action and reaction forces. Other symmetries, such as the invariance (i.e., unchanging form) of the laws under reflections and rotations carried out in space, reversal of time, or transformation to a different part of space or to a different epoch of time, are present both in classical mechanics and in relativistic mechanics, and with certain restrictions, also in quantum mechanics. The symmetry properties of the theory can be shown to have as mathematical consequences basic principles known as conservation laws, which assert the constancy in time of the values of certain physical quantities under prescribed conditions. The conserved quantities are the most important ones in physics; included among them are mass and energy (in relativity theory, mass and energy are equivalent and are conserved together), momentum, angular momentum, and electric charge.
        
        The study of gravitation
        Laser Interferometer Space Antenna (LISA)
        Laser Interferometer Space Antenna (LISA)Laser Interferometer Space Antenna (LISA), a Beyond Einstein Great Observatory, is scheduled for launch in 2035. Funded by the European Space Agency, LISA will consist of three identical spacecraft that will trail the Earth in its orbit by about 50 million km (30 million miles). The spacecraft will contain thrusters for maneuvering them into an equilateral triangle, with sides of approximately 5 million km (3 million miles), such that the triangle's center will be located along the Earth's orbit. By measuring the transmission of laser signals between the spacecraft (essentially a giant Michelson interferometer in space), scientists hope to detect and accurately measure gravity waves.
        This field of inquiry has in the past been placed within classical mechanics for historical reasons, because both fields were brought to a high state of perfection by Newton and also because of its universal character. Newton’s gravitational law states that every material particle in the universe attracts every other one with a force that acts along the line joining them and whose strength is directly proportional to the product of their masses and inversely proportional to the square of their separation. Newton’s detailed accounting for the orbits of the planets and the Moon, as well as for such subtle gravitational effects as the tides and the precession of the equinoxes (a slow cyclical change in direction of Earth’s axis of rotation), through this fundamental force was the first triumph of classical mechanics. No further principles are required to understand the principal aspects of rocketry and space flight (although, of course, a formidable technology is needed to carry them out).
        
        curved space-time
        curved space-time The four dimensional space-time continuum itself is distorted in the vicinity of any mass, with the amount of distortion depending on the mass and the distance from the mass. Thus, relativity accounts for Newton's inverse square law of gravity through geometry and thereby does away with the need for any mysterious “action at a distance.”
        The modern theory of gravitation was formulated by Albert Einstein and is called the general theory of relativity. From the long-known equality of the quantity “mass” in Newton’s second law of motion and that in his gravitational law, Einstein was struck by the fact that acceleration can locally annul a gravitational force (as occurs in the so-called weightlessness of astronauts in an Earth-orbiting spacecraft) and was led thereby to the concept of curved space-time. Completed in 1915, the theory was valued for many years mainly for its mathematical beauty and for correctly predicting a small number of phenomena, such as the gravitational bending of light around a massive object. Only in recent years, however, has it become a vital subject for both theoretical and experimental research. (Relativistic mechanics refers to Einstein’s special theory of relativity, which is not a theory of gravitation.)
        
        The study of heat, thermodynamics, and statistical mechanics
        temperature scales
        temperature scalesStandard and absolute temperature scales.
        Heat is a form of internal energy associated with the random motion of the molecular constituents of matter or with radiation. Temperature is an average of a part of the internal energy present in a body (it does not include the energy of molecular binding or of molecular rotation). The lowest possible energy state of a substance is defined as the absolute zero (−273.15 °C, or −459.67 °F) of temperature. An isolated body eventually reaches uniform temperature, a state known as thermal equilibrium, as do two or more bodies placed in contact. The formal study of states of matter at (or near) thermal equilibrium is called thermodynamics; it is capable of analyzing a large variety of thermal systems without considering their detailed microstructures.
        
        First law
        The first law of thermodynamics is the energy conservation principle of mechanics (i.e., for all changes in an isolated system, the energy remains constant) generalized to include heat.
        
        Second law
        The second law of thermodynamics asserts that heat will not flow from a place of lower temperature to one where it is higher without the intervention of an external device (e.g., a refrigerator). The concept of entropy involves the measurement of the state of disorder of the particles making up a system. For example, if tossing a coin many times results in a random-appearing sequence of heads and tails, the result has a higher entropy than if heads and tails tend to appear in clusters. Another formulation of the second law is that the entropy of an isolated system never decreases with time.
        
        Third law
        The third law of thermodynamics states that the entropy at the absolute zero of temperature is zero, corresponding to the most ordered possible state.
        
        Statistical mechanics
        Brownian particle
        Brownian particle(Left) Random motion of a Brownian particle and (right) random discrepancy between the molecular pressures on different surfaces of the particle that cause motion.
        The science of statistical mechanics derives bulk properties of systems from the mechanical properties of their molecular constituents, assuming molecular chaos and applying the laws of probability. Regarding each possible configuration of the particles as equally likely, the chaotic state (the state of maximum entropy) is so enormously more likely than ordered states that an isolated system will evolve to it, as stated in the second law of thermodynamics. Such reasoning, placed in mathematically precise form, is typical of statistical mechanics, which is capable of deriving the laws of thermodynamics but goes beyond them in describing fluctuations (i.e., temporary departures) from the thermodynamic laws that describe only average behaviour. An example of a fluctuation phenomenon is the random motion of small particles suspended in a fluid, known as Brownian motion.
        
        Quantum statistical mechanics plays a major role in many other modern fields of science, as, for example, in plasma physics (the study of fully ionized gases), in solid-state physics, and in the study of stellar structure. From a microscopic point of view the laws of thermodynamics imply that, whereas the total quantity of energy of any isolated system is constant, what might be called the quality of this energy is degraded as the system moves inexorably, through the operation of the laws of chance, to states of increasing disorder until it finally reaches the state of maximum disorder (maximum entropy), in which all parts of the system are at the same temperature, and none of the state’s energy may be usefully employed. When applied to the universe as a whole, considered as an isolated system, this ultimate chaotic condition has been called the “heat death.”
        
        The study of electricity and magnetism
        Although conceived of as distinct phenomena until the 19th century, electricity and magnetism are now known to be components of the unified field of electromagnetism. Particles with electric charge interact by an electric force, while charged particles in motion produce and respond to magnetic forces as well. Many subatomic particles, including the electrically charged electron and proton and the electrically neutral neutron, behave like elementary magnets. On the other hand, in spite of systematic searches undertaken, no magnetic monopoles, which would be the magnetic analogues of electric charges, have ever been found.
        
        The field concept plays a central role in the classical formulation of electromagnetism, as well as in many other areas of classical and contemporary physics. Einstein’s gravitational field, for example, replaces Newton’s concept of gravitational action at a distance. The field describing the electric force between a pair of charged particles works in the following manner: each particle creates an electric field in the space surrounding it, and so also at the position occupied by the other particle; each particle responds to the force exerted upon it by the electric field at its own position.
        
        Electromagnetic radiation
        Electromagnetic radiationThe electromagnetic spectrum, showing radio waves, microwaves, infrared rays, visible light, ultraviolet rays, X-rays, and gamma rays.
        Classical electromagnetism is summarized by the laws of action of electric and magnetic fields upon electric charges and upon magnets and by four remarkable equations formulated in the latter part of the 19th century by the Scottish physicist James Clerk Maxwell. The latter equations describe the manner in which electric charges and currents produce electric and magnetic fields, as well as the manner in which changing magnetic fields produce electric fields, and vice versa. From these relations Maxwell inferred the existence of electromagnetic waves—associated electric and magnetic fields in space, detached from the charges that created them, traveling at the speed of light, and endowed with such “mechanical” properties as energy, momentum, and angular momentum. The light to which the human eye is sensitive is but one small segment of an electromagnetic spectrum that extends from long-wavelength radio waves to short-wavelength gamma rays and includes X-rays, microwaves, and infrared (or heat) radiation.
        
        Optics
        Diffraction grating
        Diffraction gratingSpectrum of white light by a diffraction grating. With a prism, the red end of the spectrum is more compressed than the violet end.
        Because light consists of electromagnetic waves, the propagation of light can be regarded as merely a branch of electromagnetism. However, it is usually dealt with as a separate subject called optics: the part that deals with the tracing of light rays is known as geometrical optics, while the part that treats the distinctive wave phenomena of light is called physical optics. More recently, there has developed a new and vital branch, quantum optics, which is concerned with the theory and application of the laser, a device that produces an intense coherent beam of unidirectional radiation useful for many applications.
        
        The formation of images by lenses, microscopes, telescopes, and other optical devices is described by ray optics, which assumes that the passage of light can be represented by straight lines, that is, rays. The subtler effects attributable to the wave property of visible light, however, require the explanations of physical optics. One basic wave effect is interference, whereby two waves present in a region of space combine at certain points to yield an enhanced resultant effect (e.g., the crests of the component waves adding together); at the other extreme, the two waves can annul each other, the crests of one wave filling in the troughs of the other. Another wave effect is diffraction, which causes light to spread into regions of the geometric shadow and causes the image produced by any optical device to be fuzzy to a degree dependent on the wavelength of the light. Optical instruments such as the interferometer and the diffraction grating can be used for measuring the wavelength of light precisely (about 500 micrometres) and for measuring distances to a small fraction of that length.
        
        Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
        Britannica Quiz
        Physics and Natural Law
        Atomic and chemical physics
        Millikan oil-drop experiment
        1 of 2
        Millikan oil-drop experimentBetween 1909 and 1910 the American physicist Robert Millikan conducted a series of oil-drop experiments. By comparing applied electric force with changes in the motion of the oil drops, he was able to determine the electric charge on each drop. He found that all of the drops had charges that were simple multiples of a single number, the fundamental charge of the electron.
        See how physicist Robert Millikan devised a method for measuring the electric charge of single electrons2 of 2
        See how physicist Robert Millikan devised a method for measuring the electric charge of single electronsMillikan oil-drop experiment.
        See all videos for this article
        One of the great achievements of the 20th century was the establishment of the validity of the atomic hypothesis, first proposed in ancient times, that matter is made up of relatively few kinds of small, identical parts—namely, atoms. However, unlike the indivisible atom of Democritus and other ancients, the atom, as it is conceived today, can be separated into constituent electrons and nucleus. Atoms combine to form molecules, whose structure is studied by chemistry and physical chemistry; they also form other types of compounds, such as crystals, studied in the field of condensed-matter physics. Such disciplines study the most important attributes of matter (not excluding biologic matter) that are encountered in normal experience—namely, those that depend almost entirely on the outer parts of the electronic structure of atoms. Only the mass of the atomic nucleus and its charge, which is equal to the total charge of the electrons in the neutral atom, affect the chemical and physical properties of matter.
        
        Although there are some analogies between the solar system and the atom due to the fact that the strengths of gravitational and electrostatic forces both fall off as the inverse square of the distance, the classical forms of electromagnetism and mechanics fail when applied to tiny, rapidly moving atomic constituents. Atomic structure is comprehensible only on the basis of quantum mechanics, and its finer details require as well the use of quantum electrodynamics (QED).
        
        Atomic properties are inferred mostly by the use of indirect experiments. Of greatest importance has been spectroscopy, which is concerned with the measurement and interpretation of the electromagnetic radiations either emitted or absorbed by materials. These radiations have a distinctive character, which quantum mechanics relates quantitatively to the structures that produce and absorb them. It is truly remarkable that these structures are in principle, and often in practice, amenable to precise calculation in terms of a few basic physical constants: the mass and charge of the electron, the speed of light, and Planck’s constant (approximately 6.62606957 × 10−34 joule∙second), the fundamental constant of the quantum theory named for the German physicist Max Planck.
        
        Condensed-matter physics
        transistor
        transistor The first transistor, invented by American physicists John Bardeen, Walter H. Brattain, and William B. Shockley.
        This field, which treats the thermal, elastic, electrical, magnetic, and optical properties of solid and liquid substances, grew at an explosive rate in the second half of the 20th century and scored numerous important scientific and technical achievements, including the transistor. Among solid materials, the greatest theoretical advances have been in the study of crystalline materials whose simple repetitive geometric arrays of atoms are multiple-particle systems that allow treatment by quantum mechanics. Because the atoms in a solid are coordinated with each other over large distances, the theory must go beyond that appropriate for atoms and molecules. Thus conductors, such as metals, contain some so-called free electrons, or valence electrons, which are responsible for the electrical and most of the thermal conductivity of the material and which belong collectively to the whole solid rather than to individual atoms. Semiconductors and insulators, either crystalline or amorphous, are other materials studied in this field of physics.
        
        Other aspects of condensed matter involve the properties of the ordinary liquid state, of liquid crystals, and, at temperatures near absolute zero, of the so-called quantum liquids. The latter exhibit a property known as superfluidity (completely frictionless flow), which is an example of macroscopic quantum phenomena. Such phenomena are also exemplified by superconductivity (completely resistance-less flow of electricity), a low-temperature property of certain metallic and ceramic materials. Besides their significance to technology, macroscopic liquid and solid quantum states are important in astrophysical theories of stellar structure in, for example, neutron stars.
        
        Nuclear physics
        particle tracks from the collision of an accelerated nucleus
        particle tracks from the collision of an accelerated nucleus Particle tracks from the collision of an accelerated nucleus of a niobium atom with another niobium nucleus. The single line on the left is the track of the incoming projectile nucleus, and the other tracks are fragments from the collision.
        This branch of physics deals with the structure of the atomic nucleus and the radiation from unstable nuclei. About 10,000 times smaller than the atom, the constituent particles of the nucleus, protons and neutrons, attract one another so strongly by the nuclear forces that nuclear energies are approximately 1,000,000 times larger than typical atomic energies. Quantum theory is needed for understanding nuclear structure.
        
        Like excited atoms, unstable radioactive nuclei (either naturally occurring or artificially produced) can emit electromagnetic radiation. The energetic nuclear photons are called gamma rays. Radioactive nuclei also emit other particles: negative and positive electrons (beta rays), accompanied by neutrinos, and helium nuclei (alpha rays).
        
        A principal research tool of nuclear physics involves the use of beams of particles (e.g., protons or electrons) directed as projectiles against nuclear targets. Recoiling particles and any resultant nuclear fragments are detected, and their directions and energies are analyzed to reveal details of nuclear structure and to learn more about the strong force. A much weaker nuclear force, the so-called weak interaction, is responsible for the emission of beta rays. Nuclear collision experiments use beams of higher-energy particles, including those of unstable particles called mesons produced by primary nuclear collisions in accelerators dubbed meson factories. Exchange of mesons between protons and neutrons is directly responsible for the strong force. (For the mechanism underlying mesons, see below Fundamental forces and fields.)
        
        In radioactivity and in collisions leading to nuclear breakup, the chemical identity of the nuclear target is altered whenever there is a change in the nuclear charge. In fission and fusion nuclear reactions in which unstable nuclei are, respectively, split into smaller nuclei or amalgamated into larger ones, the energy release far exceeds that of any chemical reaction.
        
        Particle physics
        protons, neutrons, pions, and other hadrons
        protons, neutrons, pions, and other hadronsVery simplified illustrations of protons, neutrons, pions, and other hadrons show that they are made of quarks (yellow spheres) and antiquarks (green spheres), which are bound together by gluons (bent ribbons).
        One of the most significant branches of contemporary physics is the study of the fundamental subatomic constituents of matter, the elementary particles. This field, also called high-energy physics, emerged in the 1930s out of the developing experimental areas of nuclear and cosmic-ray physics. Initially investigators studied cosmic rays, the very-high-energy extraterrestrial radiations that fall upon Earth and interact in the atmosphere (see below The methodology of physics). However, after World War II, scientists gradually began using high-energy particle accelerators to provide subatomic particles for study. Quantum field theory, a generalization of QED to other types of force fields, is essential for the analysis of high-energy physics. Subatomic particles cannot be visualized as tiny analogues of ordinary material objects such as billiard balls, for they have properties that appear contradictory from the classical viewpoint. That is to say, while they possess charge, spin, mass, magnetism, and other complex characteristics, they are nonetheless regarded as pointlike.
        
        Babylonian mathematical tablet
        More From Britannica
        mathematics: Mathematical physics
        During the latter half of the 20th century, a coherent picture evolved of the underlying strata of matter involving two types of subatomic particles: fermions (baryons and leptons), which have odd half-integral angular momentum (spin 1/2, 3/2) and make up ordinary matter; and bosons (gluons, mesons, and photons), which have integral spins and mediate the fundamental forces of physics. Leptons (e.g., electrons, muons, taus), gluons, and photons are believed to be truly fundamental particles. Baryons (e.g., neutrons, protons) and mesons (e.g., pions, kaons), collectively known as hadrons, are believed to be formed from indivisible elements known as quarks, which have never been isolated.
        
        Quarks come in six types, or “flavours,” and have matching antiparticles, known as antiquarks. Quarks have charges that are either positive two-thirds or negative one-third of the electron’s charge, while antiquarks have the opposite charges. Like quarks, each lepton has an antiparticle with properties that mirror those of its partner (the antiparticle of the negatively charged electron is the positive electron, or positron; that of the neutrino is the antineutrino). In addition to their electric and magnetic properties, quarks participate in both the strong force (which binds them together) and the weak force (which underlies certain forms of radioactivity), while leptons take part in only the weak force.
        
        Baryons, such as neutrons and protons, are formed by combining three quarks—thus baryons have a charge of −1, 0, or 1. Mesons, which are the particles that mediate the strong force inside the atomic nucleus, are composed of one quark and one antiquark; all known mesons have a charge of −2, −1, 0, 1, or 2. Most of the possible quark combinations, or hadrons, have very short lifetimes, and many of them have never been seen, though additional ones have been observed with each new generation of more powerful particle accelerators.
        
        The quantum fields through which quarks and leptons interact with each other and with themselves consist of particle-like objects called quanta (from which quantum mechanics derives its name). The first known quanta were those of the electromagnetic field; they are also called photons because light consists of them. A modern unified theory of weak and electromagnetic interactions, known as the electroweak theory, proposes that the weak force involves the exchange of particles about 100 times as massive as protons. These massive quanta have been observed—namely, two charged particles, W+ and W−, and a neutral one, W0.
        
        In the theory of the strong force known as quantum chromodynamics (QCD), eight quanta, called gluons, bind quarks to form baryons and also bind quarks to antiquarks to form mesons, the force itself being dubbed the “colour force.” (This unusual use of the term colour is a somewhat forced analogue of ordinary colour mixing.) Quarks are said to come in three colours—red, blue, and green. (The opposites of these imaginary colours, minus-red, minus-blue, and minus-green, are ascribed to antiquarks.) Only certain colour combinations, namely colour-neutral, or “white” (i.e., equal mixtures of the above colours cancel out one another, resulting in no net colour), are conjectured to exist in nature in an observable form. The gluons and quarks themselves, being coloured, are permanently confined (deeply bound within the particles of which they are a part), while the colour-neutral composites such as protons can be directly observed. One consequence of colour confinement is that the observable particles are either electrically neutral or have charges that are integral multiples of the charge of the electron. A number of specific predictions of QCD have been experimentally tested and found correct.
        
        Quantum mechanics
        quantum mechanics and probability
        quantum mechanics and probabilityBrian Greene explains how the revolutionary idea of quantum mechanics is that reality evolves through a game of chance described by probabilities. This video is an episode in his Daily Equation series.
        See all videos for this article
        Although the various branches of physics differ in their experimental methods and theoretical approaches, certain general principles apply to all of them. The forefront of contemporary advances in physics lies in the submicroscopic regime, whether it be in atomic, nuclear, condensed-matter, plasma, or particle physics, or in quantum optics, or even in the study of stellar structure. All are based upon quantum theory (i.e., quantum mechanics and quantum field theory) and relativity, which together form the theoretical foundations of modern physics. Many physical quantities whose classical counterparts vary continuously over a range of possible values are in quantum theory constrained to have discontinuous, or discrete, values. Furthermore, the intrinsically deterministic character of values in classical physics is replaced in quantum theory by intrinsic uncertainty.
        
        According to quantum theory, electromagnetic radiation does not always consist of continuous waves; instead it must be viewed under some circumstances as a collection of particle-like photons, the energy and momentum of each being directly proportional to its frequency (or inversely proportional to its wavelength, the photons still possessing some wavelike characteristics). Conversely, electrons and other objects that appear as particles in classical physics are endowed by quantum theory with wavelike properties as well, such a particle’s quantum wavelength being inversely proportional to its momentum. In both instances, the proportionality constant is the characteristic quantum of action (action being defined as energy × time)—that is to say, Planck’s constant divided by 2π, or ℏ.
        
        Bohr theory
        Bohr theoryThe Bohr theory sees an electron (left) as a point mass occupying certain energy levels. Wave mechanics sees an electron as a wave washing back and forth in the atom in certain patterns only. The wave patterns and energy levels correspond exactly.
        In principle, all of atomic and molecular physics, including the structure of atoms and their dynamics, the periodic table of elements and their chemical behaviour, as well as the spectroscopic, electrical, and other physical properties of atoms, molecules, and condensed matter, can be accounted for by quantum mechanics. Roughly speaking, the electrons in the atom must fit around the nucleus as some sort of standing wave (as given by the Schrödinger equation) analogous to the waves on a plucked violin or guitar string. As the fit determines the wavelength of the quantum wave, it necessarily determines its energy state. Consequently, atomic systems are restricted to certain discrete, or quantized, energies. When an atom undergoes a discontinuous transition, or quantum jump, its energy changes abruptly by a sharply defined amount, and a photon of that energy is emitted when the energy of the atom decreases, or is absorbed in the opposite case.
        
        Although atomic energies can be sharply defined, the positions of the electrons within the atom cannot be, quantum mechanics giving only the probability for the electrons to have certain locations. This is a consequence of the feature that distinguishes quantum theory from all other approaches to physics, the uncertainty principle of the German physicist Werner Heisenberg. This principle holds that measuring a particle’s position with increasing precision necessarily increases the uncertainty as to the particle’s momentum, and conversely. The ultimate degree of uncertainty is controlled by the magnitude of Planck’s constant, which is so small as to have no apparent effects except in the world of microstructures. In the latter case, however, because both a particle’s position and its velocity or momentum must be known precisely at some instant in order to predict its future history, quantum theory precludes such certain prediction and thus escapes determinism.
        
        Compton effect
        Compton effectWhen a beam of X-rays is aimed at a target material, some of the beam is deflected, and the scattered X-rays have a greater wavelength than the original beam. The physicist Arthur Holly Compton concluded that this phenomenon could only be explained if the X-rays were understood to be made up of discrete bundles or particles, now called photons, that lost some of their energy in the collisions with electrons in the target material and then scattered at lower energy.
        The complementary wave and particle aspects, or wave–particle duality, of electromagnetic radiation and of material particles furnish another illustration of the uncertainty principle. When an electron exhibits wavelike behaviour, as in the phenomenon of electron diffraction, this excludes its exhibiting particle-like behaviour in the same observation. Similarly, when electromagnetic radiation in the form of photons interacts with matter, as in the Compton effect in which X-ray photons collide with electrons, the result resembles a particle-like collision and the wave nature of electromagnetic radiation is precluded. The principle of complementarity, asserted by the Danish physicist Niels Bohr, who pioneered the theory of atomic structure, states that the physical world presents itself in the form of various complementary pictures, no one of which is by itself complete, all of these pictures being essential for our total understanding. Thus both wave and particle pictures are needed for understanding either the electron or the photon.
        
        Although it deals with probabilities and uncertainties, the quantum theory has been spectacularly successful in explaining otherwise inaccessible atomic phenomena and in thus far meeting every experimental test. Its predictions, especially those of QED, are the most precise and the best checked of any in physics; some of them have been tested and found accurate to better than one part per billion.
        
        Relativistic mechanics
        In classical physics, space is conceived as having the absolute character of an empty stage in which events in nature unfold as time flows onward independently; events occurring simultaneously for one observer are presumed to be simultaneous for any other; mass is taken as impossible to create or destroy; and a particle given sufficient energy acquires a velocity that can increase without limit. The special theory of relativity, developed principally by Albert Einstein in 1905 and now so adequately confirmed by experiment as to have the status of physical law, shows that all these, as well as other apparently obvious assumptions, are false.
        
        Specific and unusual relativistic effects flow directly from Einstein’s two basic postulates, which are formulated in terms of so-called inertial reference frames. These are reference systems that move in such a way that in them Isaac Newton’s first law, the law of inertia, is valid. The set of inertial frames consists of all those that move with constant velocity with respect to each other (accelerating frames therefore being excluded). Einstein’s postulates are: (1) All observers, whatever their state of motion relative to a light source, measure the same speed for light; and (2) The laws of physics are the same in all inertial frames.
        
        length contraction and time dilation
        length contraction and time dilation As an object approaches the speed of light, an observer sees the object become shorter and its time interval become longer, relative to the length and time interval when the object is at rest.
        The first postulate, the constancy of the speed of light, is an experimental fact from which follow the distinctive relativistic phenomena of space contraction (or Lorentz-FitzGerald contraction), time dilation, and the relativity of simultaneity: as measured by an observer assumed to be at rest, an object in motion is contracted along the direction of its motion, and moving clocks run slow; two spatially separated events that are simultaneous for a stationary observer occur sequentially for a moving observer. As a consequence, space intervals in three-dimensional space are related to time intervals, thus forming so-called four-dimensional space-time.
        
        The second postulate is called the principle of relativity. It is equally valid in classical mechanics (but not in classical electrodynamics until Einstein reinterpreted it). This postulate implies, for example, that table tennis played on a train moving with constant velocity is just like table tennis played with the train at rest, the states of rest and motion being physically indistinguishable. In relativity theory, mechanical quantities such as momentum and energy have forms that are different from their classical counterparts but give the same values for speeds that are small compared to the speed of light, the maximum permissible speed in nature (about 300,000 kilometres per second, or 186,000 miles per second). According to relativity, mass and energy are equivalent and interchangeable quantities, the equivalence being expressed by Einstein’s famous mass-energy equation E = mc2, where m is an object’s mass and c is the speed of light.
        
        The general theory of relativity is Einstein’s theory of gravitation, which uses the principle of the equivalence of gravitation and locally accelerating frames of reference. Einstein’s theory has special mathematical beauty; it generalizes the “flat” space-time concept of special relativity to one of curvature. It forms the background of all modern cosmological theories. In contrast to some vulgarized popular notions of it, which confuse it with moral and other forms of relativism, Einstein’s theory does not argue that “all is relative.” On the contrary, it is largely a theory based upon those physical attributes that do not change, or, in the language of the theory, that are invariant.
        
        Conservation laws and symmetry
        Since the early period of modern physics, there have been conservation laws, which state that certain physical quantities, such as the total electric charge of an isolated system of bodies, do not change in the course of time. In the 20th century it has been proved mathematically that such laws follow from the symmetry properties of nature, as expressed in the laws of physics. The conservation of mass-energy of an isolated system, for example, follows from the assumption that the laws of physics may depend upon time intervals but not upon the specific time at which the laws are applied. The symmetries and the conservation laws that follow from them are regarded by modern physicists as being even more fundamental than the laws themselves, since they are able to limit the possible forms of laws that may be proposed in the future.
        
        Conservation laws are valid in classical, relativistic, and quantum theory for mass-energy, momentum, angular momentum, and electric charge. (In nonrelativistic physics, mass and energy are separately conserved.) Momentum, a directed quantity equal to the mass of a body multiplied by its velocity or to the total mass of two or more bodies multiplied by the velocity of their centre of mass, is conserved when, and only when, no external force acts. Similarly angular momentum, which is related to spinning motions, is conserved in a system upon which no net turning force, called torque, acts. External forces and torques break the symmetry conditions from which the respective conservation laws follow.
        
        In quantum theory, and especially in the theory of elementary particles, there are additional symmetries and conservation laws, some exact and others only approximately valid, which play no significant role in classical physics. Among these are the conservation of so-called quantum numbers related to left-right reflection symmetry of space (called parity) and to the reversal symmetry of motion (called time reversal). These quantum numbers are conserved in all processes other than the weak force.
        
        Other symmetry properties not obviously related to space and time (and referred to as internal symmetries) characterize the different families of elementary particles and, by extension, their composites. Quarks, for example, have a property called baryon number, as do protons, neutrons, nuclei, and unstable quark composites. All of these except the quarks are known as baryons. A failure of baryon-number conservation would exhibit itself, for instance, by a proton decaying into lighter non-baryonic particles. Indeed, intensive search for such proton decay has been conducted, but so far it has been fruitless. Similar symmetries and conservation laws hold for an analogously defined lepton number, and they also appear, as does the law of baryon conservation, to hold absolutely.
        
        Fundamental forces and fields
        Fission
        FissionSequence of events in the fission of a uranium nucleus by a neutron.
        The four basic forces of nature, in order of increasing strength, are thought to be: (1) the gravitational force between particles with mass; (2) the electromagnetic force between particles with charge or magnetism or both; (3) the colour force, or strong force, between quarks; and (4) the weak force by which, for example, quarks can change their type, so that a neutron decays into a proton, an electron, and an antineutrino. The strong force that binds protons and neutrons into nuclei and is responsible for fission, fusion, and other nuclear reactions is in principle derived from the colour force. Nuclear physics is thus related to QCD as chemistry is to atomic physics.
        
        According to quantum field theory, each of the four fundamental interactions is mediated by the exchange of quanta, called vector gauge bosons, which share certain common characteristics. All have an intrinsic spin of one unit, measured in terms of Planck’s constant ℏ. (Leptons and quarks each have one-half unit of spin.) Gauge theory studies the group of transformations, or Lie group, that leaves the basic physics of a quantum field invariant. Lie groups, which are named for the 19th-century Norwegian mathematician Sophus Lie, possess a special type of symmetry and continuity that made them first useful in the study of differential equations on smooth manifolds (an abstract mathematical space for modeling physical processes). This symmetry was first seen in the equations for electromagnetic potentials, quantities from which electromagnetic fields can be derived. It is possessed in pure form by the eight massless gluons of QCD, but in the electroweak theory—the unified theory of electromagnetic and weak force interactions—gauge symmetry is partially broken, so that only the photon remains massless, with the other gauge bosons (W+, W−, and Z) acquiring large masses. Theoretical physicists continue to seek a further unification of QCD with the electroweak theory and, more ambitiously still, to unify them with a quantum version of gravity in which the force would be transmitted by massless quanta of two units of spin called gravitons.
        
        The methodology of physics
        Physics has evolved and continues to evolve without any single strategy. Essentially an experimental science, refined measurements can reveal unexpected behaviour. On the other hand, mathematical extrapolation of existing theories into new theoretical areas, critical reexamination of apparently obvious but untested assumptions, argument by symmetry or analogy, aesthetic judgment, pure accident, and hunch—each of these plays a role (as in all of science). Thus, for example, the quantum hypothesis proposed by the German physicist Max Planck was based on observed departures of the character of blackbody radiation (radiation emitted by a heated body that absorbs all radiant energy incident upon it) from that predicted by classical electromagnetism. The English physicist P.A.M. Dirac predicted the existence of the positron in making a relativistic extension of the quantum theory of the electron. The elusive neutrino, without mass or charge, was hypothesized by the German physicist Wolfgang Pauli as an alternative to abandoning the conservation laws in the beta-decay process. Maxwell conjectured that if changing magnetic fields create electric fields (which was known to be so), then changing electric fields might create magnetic fields, leading him to the electromagnetic theory of light. Albert Einstein’s special theory of relativity was based on a critical reexamination of the meaning of simultaneity, while his general theory of relativity rests on the equivalence of inertial and gravitational mass.
        
        Although the tactics may vary from problem to problem, the physicist invariably tries to make unsolved problems more tractable by constructing a series of idealized models, with each successive model being a more realistic representation of the actual physical situation. Thus, in the theory of gases, the molecules are at first imagined to be particles that are as structureless as billiard balls with vanishingly small dimensions. This ideal picture is then improved on step by step.
        
        The correspondence principle, a useful guiding principle for extending theoretical interpretations, was formulated by the Danish physicist Niels Bohr in the context of the quantum theory. It asserts that when a valid theory is generalized to a broader arena, the new theory’s predictions must agree with the old one in the overlapping region in which both are applicable. For example, the more comprehensive theory of physical optics must yield the same result as the more restrictive theory of ray optics whenever wave effects proportional to the wavelength of light are negligible on account of the smallness of that wavelength. Similarly, quantum mechanics must yield the same results as classical mechanics in circumstances when Planck’s constant can be considered as negligibly small. Likewise, for speeds small compared to the speed of light (as for baseballs in play), relativistic mechanics must coincide with Newtonian classical mechanics.
        
        Some ways in which experimental and theoretical physicists attack their problems are illustrated by the following examples.
        
        The modern experimental study of elementary particles began with the detection of new types of unstable particles produced in the atmosphere by primary radiation, the latter consisting mainly of high-energy protons arriving from space. The new particles were detected in Geiger counters and identified by the tracks they left in instruments called cloud chambers and in photographic plates. After World War II, particle physics, then known as high-energy nuclear physics, became a major field of science. Today’s high-energy particle accelerators can be several kilometres in length, cost hundreds (or even thousands) of millions of dollars, and accelerate particles to enormous energies (trillions of electron volts). Experimental teams, such as those that discovered the W+, W−, and Z quanta of the weak force at the European Laboratory for Particle Physics (CERN) in Geneva, which is funded by its 20 European member states, can have 100 or more physicists from many countries, along with a larger number of technical workers serving as support personnel. A variety of visual and electronic techniques are used to interpret and sort the huge amounts of data produced by their efforts, and particle-physics laboratories are major users of the most advanced technology, be it superconductive magnets or supercomputers.
        
        Theoretical physicists use mathematics both as a logical tool for the development of theory and for calculating predictions of the theory to be compared with experiment. Newton, for one, invented integral calculus to solve the following problem, which was essential to his formulation of the law of universal gravitation: Assuming that the attractive force between any pair of point particles is inversely proportional to the square of the distance separating them, how does a spherical distribution of particles, such as Earth, attract another nearby object? Integral calculus, a procedure for summing many small contributions, yields the simple solution that Earth itself acts as a point particle with all its mass concentrated at the centre. In modern physics, Dirac predicted the existence of the then-unknown positive electron (or positron) by finding an equation for the electron that would combine quantum mechanics and the special theory of relativity.
        
        Relations between physics and other disciplines and society
        Influence of physics on related disciplines
        Because physics elucidates the simplest fundamental questions in nature on which there can be a consensus, it is hardly surprising that it has had a profound impact on other fields of science, on philosophy, on the worldview of the developed world, and, of course, on technology.
        
        Indeed, whenever a branch of physics has reached such a degree of maturity that its basic elements are comprehended in general principles, it has moved from basic to applied physics and thence to technology. Thus almost all current activity in classical physics consists of applied physics, and its contents form the core of many branches of engineering. Discoveries in modern physics are converted with increasing rapidity into technical innovations and analytical tools for associated disciplines. There are, for example, such nascent fields as nuclear and biomedical engineering, quantum chemistry and quantum optics, and radio, X-ray, and gamma-ray astronomy, as well as such analytic tools as radioisotopes, spectroscopy, and lasers, which all stem directly from basic physics.
        
        Apart from its specific applications, physics—especially Newtonian mechanics—has become the prototype of the scientific method, its experimental and analytic methods sometimes being imitated (and sometimes inappropriately so) in fields far from the related physical sciences. Some of the organizational aspects of physics, based partly on the successes of the radar and atomic-bomb projects of World War II, also have been imitated in large-scale scientific projects, as, for example, in astronomy and space research.
        
        The great influence of physics on the branches of philosophy concerned with the conceptual basis of human perceptions and understanding of nature, such as epistemology, is evidenced by the earlier designation of physics itself as natural philosophy. Present-day philosophy of science deals largely, though not exclusively, with the foundations of physics. Determinism, the philosophical doctrine that the universe is a vast machine operating with strict causality whose future is determined in all detail by its present state, is rooted in Newtonian mechanics, which obeys that principle. Moreover, the schools of materialism, naturalism, and empiricism have in large degree considered physics to be a model for philosophical inquiry. An extreme position is taken by the logical positivists, whose radical distrust of the reality of anything not directly observable leads them to demand that all significant statements must be formulated in the language of physics.
        
        The uncertainty principle of quantum theory has prompted a reexamination of the question of determinism, and its other philosophical implications remain in doubt. Particularly problematic is the matter of the meaning of measurement, for which recent theories and experiments confirm some apparently noncausal predictions of standard quantum theory. It is fair to say that though physicists agree that quantum theory works, they still differ as to what it means.
        
        Influence of related disciplines on physics
        laser-activated fusion
        laser-activated fusionInterior of the U.S. Department of Energy's National Ignition Facility (NIF), located at Lawrence Livermore National Laboratory, Livermore, California. The NIF target chamber uses a high-energy laser to heat fusion fuel to temperatures sufficient for thermonuclear ignition. The facility is used for basic science, fusion energy research, and nuclear weapons testing.
        The relationship of physics to its bordering disciplines is a reciprocal one. Just as technology feeds on fundamental science for new practical innovations, so physics appropriates the techniques and instrumentation of modern technology for advancing itself. Thus experimental physicists utilize increasingly refined and precise electronic devices. Moreover, they work closely with engineers in designing basic scientific equipment, such as high-energy particle accelerators. Mathematics has always been the primary tool of the theoretical physicist, and even abstruse fields of mathematics such as group theory and differential geometry have become invaluable to the theoretician classifying subatomic particles or investigating the symmetry characteristics of atoms and molecules. Much of contemporary research in physics depends on the high-speed computer. It allows the theoretician to perform computations that are too lengthy or complicated to be done with paper and pencil. Also, it allows experimentalists to incorporate the computer into their apparatus, so that the results of measurements can be provided nearly instantaneously on-line as summarized data while an experiment is in progress.
        
        The physicist in society
        proton-antiproton collision
        proton-antiproton collisionTracks emerging from a proton-antiproton collision at the centre of the UA1 detector at CERN include those of an energetic electron (straight down) and a positron (upper right). These two particles have come from pair production through the decay of a Z0; when their energies are added together, the total is equal to the Z0's mass.
        Because of the remoteness of much of contemporary physics from ordinary experience and its reliance on advanced mathematics, physicists have sometimes seemed to the public to be initiates in a latter-day secular priesthood who speak an arcane language and can communicate their findings to laymen only with great difficulty. Yet, the physicist has come to play an increasingly significant role in society, particularly since World War II. Governments have supplied substantial funds for research at academic institutions and at government laboratories through such agencies as the National Science Foundation and the Department of Energy in the United States, which has also established a number of national laboratories, including the Fermi National Accelerator Laboratory in Batavia, Ill., with one of the world’s largest particle accelerators. CERN is composed of 14 European countries and operates a large accelerator at the Swiss–French border. Physics research is supported in Germany by the Max Planck Society for the Advancement of Science and in Japan by the Japan Society for the Promotion of Science. In Trieste, Italy, there is the International Center for Theoretical Physics, which has strong ties to developing countries. These are only a few examples of the widespread international interest in fundamental physics.
        
        Basic research in physics is obviously dependent on public support and funding, and with this development has come, albeit slowly, a growing recognition within the physics community of the social responsibility of scientists for the consequences of their work and for the more general problems of science and society.</p>

        <video src="video2.mp4" width="500px" controls></video>

<p>quantum mechanics, science dealing with the behaviour of matter and light on the atomic and subatomic scale. It attempts to describe and account for the properties of molecules and atoms and their constituents—electrons, protons, neutrons, and other more esoteric particles such as quarks and gluons. These properties include the interactions of the particles with one another and with electromagnetic radiation (i.e., light, X-rays, and gamma rays).

    The behaviour of matter and radiation on the atomic scale often seems peculiar, and the consequences of quantum theory are accordingly difficult to understand and to believe. Its concepts frequently conflict with common-sense notions derived from observations of the everyday world. There is no reason, however, why the behaviour of the atomic world should conform to that of the familiar, large-scale world. It is important to realize that quantum mechanics is a branch of physics and that the business of physics is to describe and account for the way the world—on both the large and the small scale—actually is and not how one imagines it or would like it to be.
    
    The study of quantum mechanics is rewarding for several reasons. First, it illustrates the essential methodology of physics. Second, it has been enormously successful in giving correct results in practically every situation to which it has been applied. There is, however, an intriguing paradox. In spite of the overwhelming practical success of quantum mechanics, the foundations of the subject contain unresolved problems—in particular, problems concerning the nature of measurement. An essential feature of quantum mechanics is that it is generally impossible, even in principle, to measure a system without disturbing it; the detailed nature of this disturbance and the exact point at which it occurs are obscure and controversial. Thus, quantum mechanics attracted some of the ablest scientists of the 20th century, and they erected what is perhaps the finest intellectual edifice of the period.
    
    Historical basis of quantum theory
    Basic considerations
    At a fundamental level, both radiation and matter have characteristics of particles and waves. The gradual recognition by scientists that radiation has particle-like properties and that matter has wavelike properties provided the impetus for the development of quantum mechanics. Influenced by Newton, most physicists of the 18th century believed that light consisted of particles, which they called corpuscles. From about 1800, evidence began to accumulate for a wave theory of light. At about this time Thomas Young showed that, if monochromatic light passes through a pair of slits, the two emerging beams interfere, so that a fringe pattern of alternately bright and dark bands appears on a screen. The bands are readily explained by a wave theory of light. According to the theory, a bright band is produced when the crests (and troughs) of the waves from the two slits arrive together at the screen; a dark band is produced when the crest of one wave arrives at the same time as the trough of the other, and the effects of the two light beams cancel. Beginning in 1815, a series of experiments by Augustin-Jean Fresnel of France and others showed that, when a parallel beam of light passes through a single slit, the emerging beam is no longer parallel but starts to diverge; this phenomenon is known as diffraction. Given the wavelength of the light and the geometry of the apparatus (i.e., the separation and widths of the slits and the distance from the slits to the screen), one can use the wave theory to calculate the expected pattern in each case; the theory agrees precisely with the experimental data.
    
    Early developments
    Planck’s radiation law
    By the end of the 19th century, physicists almost universally accepted the wave theory of light. However, though the ideas of classical physics explain interference and diffraction phenomena relating to the propagation of light, they do not account for the absorption and emission of light. All bodies radiate electromagnetic energy as heat; in fact, a body emits radiation at all wavelengths. The energy radiated at different wavelengths is a maximum at a wavelength that depends on the temperature of the body; the hotter the body, the shorter the wavelength for maximum radiation. Attempts to calculate the energy distribution for the radiation from a blackbody using classical ideas were unsuccessful. (A blackbody is a hypothetical ideal body or surface that absorbs and reemits all radiant energy falling on it.) One formula, proposed by Wilhelm Wien of Germany, did not agree with observations at long wavelengths, and another, proposed by Lord Rayleigh (John William Strutt) of England, disagreed with those at short wavelengths.
    
    Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
    Britannica Quiz
    Physics and Natural Law
    In 1900 the German theoretical physicist Max Planck made a bold suggestion. He assumed that the radiation energy is emitted, not continuously, but rather in discrete packets called quanta. The energy E of the quantum is related to the frequency ν by E = hν. The quantity h, now known as Planck’s constant, is a universal constant with the approximate value of 6.62607 × 10−34 joule∙second. Planck showed that the calculated energy spectrum then agreed with observation over the entire wavelength range.
    
    Einstein and the photoelectric effect
    photoelectric effect: Einstein's Nobel Prize-winning discovery
    photoelectric effect: Einstein's Nobel Prize-winning discoveryBrian Greene discusses the key formula in the photoelectric effect, an insight that helped launch the quantum revolution. This video is an episode in his Daily Equation series.
    See all videos for this article
    In 1905 Einstein extended Planck’s hypothesis to explain the photoelectric effect, which is the emission of electrons by a metal surface when it is irradiated by light or more-energetic photons. The kinetic energy of the emitted electrons depends on the frequency ν of the radiation, not on its intensity; for a given metal, there is a threshold frequency ν0 below which no electrons are emitted. Furthermore, emission takes place as soon as the light shines on the surface; there is no detectable delay. Einstein showed that these results can be explained by two assumptions: (1) that light is composed of corpuscles or photons, the energy of which is given by Planck’s relationship, and (2) that an atom in the metal can absorb either a whole photon or nothing. Part of the energy of the absorbed photon frees an electron, which requires a fixed energy W, known as the work function of the metal; the rest is converted into the kinetic energy meu2/2 of the emitted electron (me is the mass of the electron and u is its velocity). Thus, the energy relation is
    special composition for article "Quantum Mechanics"
    If ν is less than ν0, where hν0 = W, no electrons are emitted. Not all the experimental results mentioned above were known in 1905, but all Einstein’s predictions have been verified since.
    
    Bohr’s theory of the atom
    A major contribution to the subject was made by Niels Bohr of Denmark, who applied the quantum hypothesis to atomic spectra in 1913. The spectra of light emitted by gaseous atoms had been studied extensively since the mid-19th century. It was found that radiation from gaseous atoms at low pressure consists of a set of discrete wavelengths. This is quite unlike the radiation from a solid, which is distributed over a continuous range of wavelengths. The set of discrete wavelengths from gaseous atoms is known as a line spectrum, because the radiation (light) emitted consists of a series of sharp lines. The wavelengths of the lines are characteristic of the element and may form extremely complex patterns. The simplest spectra are those of atomic hydrogen and the alkali atoms (e.g., lithium, sodium, and potassium). For hydrogen, the wavelengths λ are given by the empirical formula
    special composition for article "Quantum Mechanics"
    where m and n are positive integers with n > m and R∞, known as the Rydberg constant, has the value 1.097373157 × 107 per metre. For a given value of m, the lines for varying n form a series. The lines for m = 1, the Lyman series, lie in the ultraviolet part of the spectrum; those for m = 2, the Balmer series, lie in the visible spectrum; and those for m = 3, the Paschen series, lie in the infrared.
    
    Bohr started with a model suggested by the New Zealand-born British physicist Ernest Rutherford. The model was based on the experiments of Hans Geiger and Ernest Marsden, who in 1909 bombarded gold atoms with massive, fast-moving alpha particles; when some of these particles were deflected backward, Rutherford concluded that the atom has a massive, charged nucleus. In Rutherford’s model, the atom resembles a miniature solar system with the nucleus acting as the Sun and the electrons as the circulating planets. Bohr made three assumptions. First, he postulated that, in contrast to classical mechanics, where an infinite number of orbits is possible, an electron can be in only one of a discrete set of orbits, which he termed stationary states. Second, he postulated that the only orbits allowed are those for which the angular momentum of the electron is a whole number n times ℏ (ℏ = h/2π). Third, Bohr assumed that Newton’s laws of motion, so successful in calculating the paths of the planets around the Sun, also applied to electrons orbiting the nucleus. The force on the electron (the analogue of the gravitational force between the Sun and a planet) is the electrostatic attraction between the positively charged nucleus and the negatively charged electron. With these simple assumptions, he showed that the energy of the orbit has the form
    special composition for article "Quantum Mechanics"
    where E0 is a constant that may be expressed by a combination of the known constants e, me, and ℏ. While in a stationary state, the atom does not give off energy as light; however, when an electron makes a transition from a state with energy En to one with lower energy Em, a quantum of energy is radiated with frequency ν, given by the equation
    special composition for article "Quantum Mechanics"
    Inserting the expression for En into this equation and using the relation λν = c, where c is the speed of light, Bohr derived the formula for the wavelengths of the lines in the hydrogen spectrum, with the correct value of the Rydberg constant.
    
    Bohr’s theory was a brilliant step forward. Its two most important features have survived in present-day quantum mechanics. They are (1) the existence of stationary, nonradiating states and (2) the relationship of radiation frequency to the energy difference between the initial and final states in a transition. Prior to Bohr, physicists had thought that the radiation frequency would be the same as the electron’s frequency of rotation in an orbit.
    
    Scattering of X-rays
    Soon scientists were faced with the fact that another form of radiation, X-rays, also exhibits both wave and particle properties. Max von Laue of Germany had shown in 1912 that crystals can be used as three-dimensional diffraction gratings for X-rays; his technique constituted the fundamental evidence for the wavelike nature of X-rays. The atoms of a crystal, which are arranged in a regular lattice, scatter the X-rays. For certain directions of scattering, all the crests of the X-rays coincide. (The scattered X-rays are said to be in phase and to give constructive interference.) For these directions, the scattered X-ray beam is very intense. Clearly, this phenomenon demonstrates wave behaviour. In fact, given the interatomic distances in the crystal and the directions of constructive interference, the wavelength of the waves can be calculated.
    
    In 1922 the American physicist Arthur Holly Compton showed that X-rays scatter from electrons as if they are particles. Compton performed a series of experiments on the scattering of monochromatic, high-energy X-rays by graphite. He found that part of the scattered radiation had the same wavelength λ0 as the incident X-rays but that there was an additional component with a longer wavelength λ. To interpret his results, Compton regarded the X-ray photon as a particle that collides and bounces off an electron in the graphite target as though the photon and the electron were a pair of (dissimilar) billiard balls. Application of the laws of conservation of energy and momentum to the collision leads to a specific relation between the amount of energy transferred to the electron and the angle of scattering. For X-rays scattered through an angle θ, the wavelengths λ and λ0 are related by the equation
    special composition for article "Quantum Mechanics"
    The experimental correctness of Compton’s formula is direct evidence for the corpuscular behaviour of radiation.
    
    De Broglie’s wave hypothesis
    Faced with evidence that electromagnetic radiation has both particle and wave characteristics, Louis-Victor de Broglie of France suggested a great unifying hypothesis in 1924. De Broglie proposed that matter has wave as well as particle properties. He suggested that material particles can behave as waves and that their wavelength λ is related to the linear momentum p of the particle by λ = h/p.
    
    In 1927 Clinton Davisson and Lester Germer of the United States confirmed de Broglie’s hypothesis for electrons. Using a crystal of nickel, they diffracted a beam of monoenergetic electrons and showed that the wavelength of the waves is related to the momentum of the electrons by the de Broglie equation. Since Davisson and Germer’s investigation, similar experiments have been performed with atoms, molecules, neutrons, protons, and many other particles. All behave like waves with the same wavelength-momentum relationship.
    
    Basic concepts and methods
    Bohr’s theory, which assumed that electrons moved in circular orbits, was extended by the German physicist Arnold Sommerfeld and others to include elliptic orbits and other refinements. Attempts were made to apply the theory to more complicated systems than the hydrogen atom. However, the ad hoc mixture of classical and quantum ideas made the theory and calculations increasingly unsatisfactory. Then, in the 12 months started in July 1925, a period of creativity without parallel in the history of physics, there appeared a series of papers by German scientists that set the subject on a firm conceptual foundation. The papers took two approaches: (1) matrix mechanics, proposed by Werner Heisenberg, Max Born, and Pascual Jordan, and (2) wave mechanics, put forward by Erwin Schrödinger. The protagonists were not always polite to each other. Heisenberg found the physical ideas of Schrödinger’s theory “disgusting,” and Schrödinger was “discouraged and repelled” by the lack of visualization in Heisenberg’s method. However, Schrödinger, not allowing his emotions to interfere with his scientific endeavours, showed that, in spite of apparent dissimilarities, the two theories are equivalent mathematically. The present discussion follows Schrödinger’s wave mechanics because it is less abstract and easier to understand than Heisenberg’s matrix mechanics.
    
    Schrödinger’s wave mechanics
    Schrödinger expressed de Broglie’s hypothesis concerning the wave behaviour of matter in a mathematical form that is adaptable to a variety of physical problems without additional arbitrary assumptions. He was guided by a mathematical formulation of optics, in which the straight-line propagation of light rays can be derived from wave motion when the wavelength is small compared to the dimensions of the apparatus employed. In the same way, Schrödinger set out to find a wave equation for matter that would give particle-like propagation when the wavelength becomes comparatively small. According to classical mechanics, if a particle of mass me is subjected to a force such that its potential energy is V(x, y, z) at position x, y, z, then the sum of V(x, y, z) and the kinetic energy p2/2me is equal to a constant, the total energy E of the particle. Thus,
    special composition for article "Quantum Mechanics"
    
    It is assumed that the particle is bound—i.e., confined by the potential to a certain region in space because its energy E is insufficient for it to escape. Since the potential varies with position, two other quantities do also: the momentum and, hence, by extension from the de Broglie relation, the wavelength of the wave. Postulating a wave function Ψ(x, y, z) that varies with position, Schrödinger replaced p in the above energy equation with a differential operator that embodied the de Broglie relation. He then showed that Ψ satisfies the partial differential equation
    special composition for article "Quantum Mechanics"
    
    This is the (time-independent) Schrödinger wave equation, which established quantum mechanics in a widely applicable form. An important advantage of Schrödinger’s theory is that no further arbitrary quantum conditions need be postulated. The required quantum results follow from certain reasonable restrictions placed on the wave function—for example, that it should not become infinitely large at large distances from the centre of the potential.
    
    Schrödinger applied his equation to the hydrogen atom, for which the potential function, given by classical electrostatics, is proportional to −e2/r, where −e is the charge on the electron. The nucleus (a proton of charge e) is situated at the origin, and r is the distance from the origin to the position of the electron. Schrödinger solved the equation for this particular potential with straightforward, though not elementary, mathematics. Only certain discrete values of E lead to acceptable functions Ψ. These functions are characterized by a trio of integers n, l, m, termed quantum numbers. The values of E depend only on the integers n (1, 2, 3, etc.) and are identical with those given by the Bohr theory. The quantum numbers l and m are related to the angular momentum of the electron; Square root of√l(l + 1)ℏ is the magnitude of the angular momentum, and mℏ is its component along some physical direction.
    
    The square of the wave function, Ψ2, has a physical interpretation. Schrödinger originally supposed that the electron was spread out in space and that its density at point x, y, z was given by the value of Ψ2 at that point. Almost immediately Born proposed what is now the accepted interpretation—namely, that Ψ2 gives the probability of finding the electron at x, y, z. The distinction between the two interpretations is important. If Ψ2 is small at a particular position, the original interpretation implies that a small fraction of an electron will always be detected there. In Born’s interpretation, nothing will be detected there most of the time, but, when something is observed, it will be a whole electron. Thus, the concept of the electron as a point particle moving in a well-defined path around the nucleus is replaced in wave mechanics by clouds that describe the probable locations of electrons in different states.
    
    Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
    Britannica Quiz
    Physics and Natural Law
    Electron spin and antiparticles
    In 1928 the English physicist Paul A.M. Dirac produced a wave equation for the electron that combined relativity with quantum mechanics. Schrödinger’s wave equation does not satisfy the requirements of the special theory of relativity because it is based on a nonrelativistic expression for the kinetic energy (p2/2me). Dirac showed that an electron has an additional quantum number ms. Unlike the first three quantum numbers, ms is not a whole integer and can have only the values +1/2 and −1/2. It corresponds to an additional form of angular momentum ascribed to a spinning motion. (The angular momentum mentioned above is due to the orbital motion of the electron, not its spin.) The concept of spin angular momentum was introduced in 1925 by Samuel A. Goudsmit and George E. Uhlenbeck, two graduate students at the University of Leiden, Neth., to explain the magnetic moment measurements made by Otto Stern and Walther Gerlach of Germany several years earlier. The magnetic moment of a particle is closely related to its angular momentum; if the angular momentum is zero, so is the magnetic moment. Yet Stern and Gerlach had observed a magnetic moment for electrons in silver atoms, which were known to have zero orbital angular momentum. Goudsmit and Uhlenbeck proposed that the observed magnetic moment was attributable to spin angular momentum.
    
    The electron-spin hypothesis not only provided an explanation for the observed magnetic moment but also accounted for many other effects in atomic spectroscopy, including changes in spectral lines in the presence of a magnetic field (Zeeman effect), doublet lines in alkali spectra, and fine structure (close doublets and triplets) in the hydrogen spectrum.
    
    The Dirac equation also predicted additional states of the electron that had not yet been observed. Experimental confirmation was provided in 1932 by the discovery of the positron by the American physicist Carl David Anderson. Every particle described by the Dirac equation has to have a corresponding antiparticle, which differs only in charge. The positron is just such an antiparticle of the negatively charged electron, having the same mass as the latter but a positive charge.
    
    Identical particles and multielectron atoms
    Because electrons are identical to (i.e., indistinguishable from) each other, the wave function of an atom with more than one electron must satisfy special conditions. The problem of identical particles does not arise in classical physics, where the objects are large-scale and can always be distinguished, at least in principle. There is no way, however, to differentiate two electrons in the same atom, and the form of the wave function must reflect this fact. The overall wave function Ψ of a system of identical particles depends on the coordinates of all the particles. If the coordinates of two of the particles are interchanged, the wave function must remain unaltered or, at most, undergo a change of sign; the change of sign is permitted because it is Ψ2 that occurs in the physical interpretation of the wave function. If the sign of Ψ remains unchanged, the wave function is said to be symmetric with respect to interchange; if the sign changes, the function is antisymmetric.
    
    The symmetry of the wave function for identical particles is closely related to the spin of the particles. In quantum field theory (see below Quantum electrodynamics), it can be shown that particles with half-integral spin (1/2, 3/2, etc.) have antisymmetric wave functions. They are called fermions after the Italian-born physicist Enrico Fermi. Examples of fermions are electrons, protons, and neutrons, all of which have spin 1/2. Particles with zero or integral spin (e.g., mesons, photons) have symmetric wave functions and are called bosons after the Indian mathematician and physicist Satyendra Nath Bose, who first applied the ideas of symmetry to photons in 1924–25.
    
    The requirement of antisymmetric wave functions for fermions leads to a fundamental result, known as the exclusion principle, first proposed in 1925 by the Austrian physicist Wolfgang Pauli. The exclusion principle states that two fermions in the same system cannot be in the same quantum state. If they were, interchanging the two sets of coordinates would not change the wave function at all, which contradicts the result that the wave function must change sign. Thus, two electrons in the same atom cannot have an identical set of values for the four quantum numbers n, l, m, ms. The exclusion principle forms the basis of many properties of matter, including the periodic classification of the elements, the nature of chemical bonds, and the behaviour of electrons in solids; the last determines in turn whether a solid is a metal, an insulator, or a semiconductor (see atom; matter).
    
    The Schrödinger equation cannot be solved precisely for atoms with more than one electron. The principles of the calculation are well understood, but the problems are complicated by the number of particles and the variety of forces involved. The forces include the electrostatic forces between the nucleus and the electrons and between the electrons themselves, as well as weaker magnetic forces arising from the spin and orbital motions of the electrons. Despite these difficulties, approximation methods introduced by the English physicist Douglas R. Hartree, the Russian physicist Vladimir Fock, and others in the 1920s and 1930s have achieved considerable success. Such schemes start by assuming that each electron moves independently in an average electric field because of the nucleus and the other electrons; i.e., correlations between the positions of the electrons are ignored. Each electron has its own wave function, called an orbital. The overall wave function for all the electrons in the atom satisfies the exclusion principle. Corrections to the calculated energies are then made, which depend on the strengths of the electron-electron correlations and the magnetic forces.
    
    Time-dependent Schrödinger equation
    At the same time that Schrödinger proposed his time-independent equation to describe the stationary states, he also proposed a time-dependent equation to describe how a system changes from one state to another. By replacing the energy E in Schrödinger’s equation with a time-derivative operator, he generalized his wave equation to determine the time variation of the wave function as well as its spatial variation. The time-dependent Schrödinger equation reads
    special composition for article "Quantum Mechanics": Schrodinger equation
    The quantity i is the square root of −1. The function Ψ varies with time t as well as with position x, y, z. For a system with constant energy, E, Ψ has the form
    special composition for article "Quantum Mechanics"
    where exp stands for the exponential function, and the time-dependent Schrödinger equation reduces to the time-independent form.
    
    The probability of a transition between one atomic stationary state and some other state can be calculated with the aid of the time-dependent Schrödinger equation. For example, an atom may change spontaneously from one state to another state with less energy, emitting the difference in energy as a photon with a frequency given by the Bohr relation. If electromagnetic radiation is applied to a set of atoms and if the frequency of the radiation matches the energy difference between two stationary states, transitions can be stimulated. In a stimulated transition, the energy of the atom may increase—i.e., the atom may absorb a photon from the radiation—or the energy of the atom may decrease, with the emission of a photon, which adds to the energy of the radiation. Such stimulated emission processes form the basic mechanism for the operation of lasers. The probability of a transition from one state to another depends on the values of the l, m, ms quantum numbers of the initial and final states. For most values, the transition probability is effectively zero. However, for certain changes in the quantum numbers, summarized as selection rules, there is a finite probability. For example, according to one important selection rule, the l value changes by unity because photons have a spin of 1. The selection rules for radiation relate to the angular momentum properties of the stationary states. The absorbed or emitted photon has its own angular momentum, and the selection rules reflect the conservation of angular momentum between the atoms and the radiation.
    
    Tunneling
    Know about quantum tunneling and how it affects the way a particle behaves toward its barrier1 of 2
    Know about quantum tunneling and how it affects the way a particle behaves toward its barrierLearn how quantum tunneling affects the way a particle reacts to barriers.
    See all videos for this article
    tunneling
    2 of 2
    tunnelingFigure 1: Classically, a particle is bound in the central region C if its energy E is less than V0, but in quantum theory the particle may tunnel through the potential barrier and escape.
    The phenomenon of tunneling, which has no counterpart in classical physics, is an important consequence of quantum mechanics. Consider a particle with energy E in the inner region of a one-dimensional potential well V(x), as shown in Figure 1. (A potential well is a potential that has a lower value in a certain region of space than in the neighbouring regions.) In classical mechanics, if E < V0 (the maximum height of the potential barrier), the particle remains in the well forever; if E > V0, the particle escapes. In quantum mechanics, the situation is not so simple. The particle can escape even if its energy E is below the height of the barrier V0, although the probability of escape is small unless E is close to V0. In that case, the particle may tunnel through the potential barrier and emerge with the same energy E.
    
    The phenomenon of tunneling has many important applications. For example, it describes a type of radioactive decay in which a nucleus emits an alpha particle (a helium nucleus). According to the quantum explanation given independently by George Gamow and by Ronald W. Gurney and Edward Condon in 1928, the alpha particle is confined before the decay by a potential of the shape shown in Figure 1. For a given nuclear species, it is possible to measure the energy E of the emitted alpha particle and the average lifetime τ of the nucleus before decay. The lifetime of the nucleus is a measure of the probability of tunneling through the barrier—the shorter the lifetime, the higher the probability. With plausible assumptions about the general form of the potential function, it is possible to calculate a relationship between τ and E that is applicable to all alpha emitters. This theory, which is borne out by experiment, shows that the probability of tunneling, and hence the value of τ, is extremely sensitive to the value of E. For all known alpha-particle emitters, the value of E varies from about 2 to 8 million electron volts, or MeV (1 MeV = 106 electron volts). Thus, the value of E varies only by a factor of 4, whereas the range of τ is from about 1011 years down to about 10−6 second, a factor of 1024. It would be difficult to account for this sensitivity of τ to the value of E by any theory other than quantum mechanical tunneling.
    
    Axiomatic approach
    Although the two Schrödinger equations form an important part of quantum mechanics, it is possible to present the subject in a more general way. Dirac gave an elegant exposition of an axiomatic approach based on observables and states in a classic textbook entitled The Principles of Quantum Mechanics. (The book, published in 1930, is still in print.) An observable is anything that can be measured—energy, position, a component of angular momentum, and so forth. Every observable has a set of states, each state being represented by an algebraic function. With each state is associated a number that gives the result of a measurement of the observable. Consider an observable with N states, denoted by ψ1, ψ2, . . ., ψN, and corresponding measurement values a1, a2, . . ., aN. A physical system—e.g., an atom in a particular state—is represented by a wave function Ψ, which can be expressed as a linear combination, or mixture, of the states of the observable. Thus, the Ψ may be written as
    special composition for article "Quantum Mechanics"
    For a given Ψ, the quantities c1, c2, etc., are a set of numbers that can be calculated. In general, the numbers are complex, but, in the present discussion, they are assumed to be real numbers.
    
    The theory postulates, first, that the result of a measurement must be an a-value—i.e., a1, a2, or a3, etc. No other value is possible. Second, before the measurement is made, the probability of obtaining the value a1 is c12, and that of obtaining the value a2 is c22, and so on. If the value obtained is, say, a5, the theory asserts that after the measurement the state of the system is no longer the original Ψ but has changed to ψ5, the state corresponding to a5.
    
    A number of consequences follow from these assertions. First, the result of a measurement cannot be predicted with certainty. Only the probability of a particular result can be predicted, even though the initial state (represented by the function Ψ) is known exactly. Second, identical measurements made on a large number of identical systems, all in the identical state Ψ, will produce different values for the measurements. This is, of course, quite contrary to classical physics and common sense, which say that the same measurement on the same object in the same state must produce the same result. Moreover, according to the theory, not only does the act of measurement change the state of the system, but it does so in an indeterminate way. Sometimes it changes the state to ψ1, sometimes to ψ2, and so forth.
    
    There is an important exception to the above statements. Suppose that, before the measurement is made, the state Ψ happens to be one of the ψs—say, Ψ = ψ3. Then c3 = 1 and all the other cs are zero. This means that, before the measurement is made, the probability of obtaining the value a3 is unity and the probability of obtaining any other value of a is zero. In other words, in this particular case, the result of the measurement can be predicted with certainty. Moreover, after the measurement is made, the state will be ψ3, the same as it was before. Thus, in this particular case, measurement does not disturb the system. Whatever the initial state of the system, two measurements made in rapid succession (so that the change in the wave function given by the time-dependent Schrödinger equation is negligible) produce the same result.
    
    The value of one observable can be determined by a single measurement. The value of two observables for a given system may be known at the same time, provided that the two observables have the same set of state functions ψ1, ψ2, . . ., ψN. In this case, measuring the first observable results in a state function that is one of the ψs. Because this is also a state function of the second observable, the result of measuring the latter can be predicted with certainty. Thus the values of both observables are known. (Although the ψs are the same for the two observables, the two sets of a values are, in general, different.) The two observables can be measured repeatedly in any sequence. After the first measurement, none of the measurements disturbs the system, and a unique pair of values for the two observables is obtained.
    
    Incompatible observables
    The measurement of two observables with different sets of state functions is a quite different situation. Measurement of one observable gives a certain result. The state function after the measurement is, as always, one of the states of that observable; however, it is not a state function for the second observable. Measuring the second observable disturbs the system, and the state of the system is no longer one of the states of the first observable. In general, measuring the first observable again does not produce the same result as the first time. To sum up, both quantities cannot be known at the same time, and the two observables are said to be incompatible.
    
    magnet in Stern-Gerlach experiment
    1 of 2
    magnet in Stern-Gerlach experimentFigure 2: N and S are the north and south poles of a magnet. The knife-edge of S results in a much stronger magnetic field at the point P than at Q.
    measurement of angular momentum components
    2 of 2
    measurement of angular momentum componentsFigure 3: Measurements of the x and y components of angular momentum for silver atoms, S, in the ground state. A, B, and C are magnets with inhomogeneous magnetic fields. The arrows show the average direction of each magnetic field.
    A specific example of this behaviour is the measurement of the component of angular momentum along two mutually perpendicular directions. The Stern-Gerlach experiment mentioned above involved measuring the angular momentum of a silver atom in the ground state. In reconstructing this experiment, a beam of silver atoms is passed between the poles of a magnet. The poles are shaped so that the magnetic field varies greatly in strength over a very small distance (Figure 2). The apparatus determines the ms quantum number, which can be +1/2 or −1/2. No other values are obtained. Thus in this case the observable has only two states—i.e., N = 2. The inhomogeneous magnetic field produces a force on the silver atoms in a direction that depends on the spin state of the atoms. The result is shown schematically in Figure 3. A beam of silver atoms is passed through magnet A. The atoms in the state with ms = +1/2 are deflected upward and emerge as beam 1, while those with ms = −1/2 are deflected downward and emerge as beam 2. If the direction of the magnetic field is the x-axis, the apparatus measures Sx, which is the x-component of spin angular momentum. The atoms in beam 1 have Sx = +ℏ/2 while those in beam 2 have Sx = −ℏ/2. In a classical picture, these two states represent atoms spinning about the direction of the x-axis with opposite senses of rotation.
    
    The y-component of spin angular momentum Sy also can have only the values +ℏ/2 and −ℏ/2; however, the two states of Sy are not the same as for Sx. In fact, each of the states of Sx is an equal mixture of the states for Sy, and conversely. Again, the two Sy states may be pictured as representing atoms with opposite senses of rotation about the y-axis. These classical pictures of quantum states are helpful, but only up to a certain point. For example, quantum theory says that each of the states corresponding to spin about the x-axis is a superposition of the two states with spin about the y-axis. There is no way to visualize this; it has absolutely no classical counterpart. One simply has to accept the result as a consequence of the axioms of the theory. Suppose that, as in Figure 3, the atoms in beam 1 are passed into a second magnet B, which has a magnetic field along the y-axis perpendicular to x. The atoms emerge from B and go in equal numbers through its two output channels. Classical theory says that the two magnets together have measured both the x- and y-components of spin angular momentum and that the atoms in beam 3 have Sx = +ℏ/2, Sy = +ℏ/2, while those in beam 4 have Sx = +ℏ/2, Sy = −ℏ/2. However, classical theory is wrong, because if beam 3 is put through still another magnet C, with its magnetic field along x, the atoms divide equally into beams 5 and 6 instead of emerging as a single beam 5 (as they would if they had Sx = +ℏ/2). Thus, the correct statement is that the beam entering B has Sx = +ℏ/2 and is composed of an equal mixture of the states Sy = +ℏ/2 and Sy = −ℏ/2—i.e., the x-component of angular momentum is known but the y-component is not. Correspondingly, beam 3 leaving B has Sy = +ℏ/2 and is an equal mixture of the states Sx = +ℏ/2 and Sx = −ℏ/2; the y-component of angular momentum is known but the x-component is not. The information about Sx is lost because of the disturbance caused by magnet B in the measurement of Sy.
    
    Heisenberg uncertainty principle
    The observables discussed so far have had discrete sets of experimental values. For example, the values of the energy of a bound system are always discrete, and angular momentum components have values that take the form mℏ, where m is either an integer or a half-integer, positive or negative. On the other hand, the position of a particle or the linear momentum of a free particle can take continuous values in both quantum and classical theory. The mathematics of observables with a continuous spectrum of measured values is somewhat more complicated than for the discrete case but presents no problems of principle. An observable with a continuous spectrum of measured values has an infinite number of state functions. The state function Ψ of the system is still regarded as a combination of the state functions of the observable, but the sum in equation (10) must be replaced by an integral.
    
    light passing through a slit
    light passing through a slitFigure 4: (A) Parallel monochromatic light incident normally on a slit, (B) variation in the intensity of the light with direction after it has passed through the slit. If the experiment is repeated with electrons instead of light, the same diagram would represent the variation in the intensity (i.e., relative number) of the electrons.
    Measurements can be made of position x of a particle and the x-component of its linear momentum, denoted by px. These two observables are incompatible because they have different state functions. The phenomenon of diffraction noted above illustrates the impossibility of measuring position and momentum simultaneously and precisely. If a parallel monochromatic light beam passes through a slit (Figure 4A), its intensity varies with direction, as shown in Figure 4B. The light has zero intensity in certain directions. Wave theory shows that the first zero occurs at an angle θ0, given by sin θ0 = λ/b, where λ is the wavelength of the light and b is the width of the slit. If the width of the slit is reduced, θ0 increases—i.e., the diffracted light is more spread out. Thus, θ0 measures the spread of the beam.
    
    The experiment can be repeated with a stream of electrons instead of a beam of light. According to de Broglie, electrons have wavelike properties; therefore, the beam of electrons emerging from the slit should widen and spread out like a beam of light waves. This has been observed in experiments. If the electrons have velocity u in the forward direction (i.e., the y-direction in Figure 4A), their (linear) momentum is p = meu. Consider px, the component of momentum in the x-direction. After the electrons have passed through the aperture, the spread in their directions results in an uncertainty in px by an amount
    special composition for article "Quantum Mechanics"
    where λ is the wavelength of the electrons and, according to the de Broglie formula, equals h/p. Thus, Δpx ≈ h/b. Exactly where an electron passed through the slit is unknown; it is only certain that an electron went through somewhere. Therefore, immediately after an electron goes through, the uncertainty in its x-position is Δx ≈ b/2. Thus, the product of the uncertainties is of the order of ℏ. More exact analysis shows that the product has a lower limit, given by
    special composition for article "Quantum Mechanics": Heisenberg uncertainty principle
    
    This is the well-known Heisenberg uncertainty principle for position and momentum. It states that there is a limit to the precision with which the position and the momentum of an object can be measured at the same time. Depending on the experimental conditions, either quantity can be measured as precisely as desired (at least in principle), but the more precisely one of the quantities is measured, the less precisely the other is known.
    
    The uncertainty principle is significant only on the atomic scale because of the small value of h in everyday units. If the position of a macroscopic object with a mass of, say, one gram is measured with a precision of 10−6 metre, the uncertainty principle states that its velocity cannot be measured to better than about 10−25 metre per second. Such a limitation is hardly worrisome. However, if an electron is located in an atom about 10−10 metre across, the principle gives a minimum uncertainty in the velocity of about 106 metre per second.
    
    The above reasoning leading to the uncertainty principle is based on the wave-particle duality of the electron. When Heisenberg first propounded the principle in 1927 his reasoning was based, however, on the wave-particle duality of the photon. He considered the process of measuring the position of an electron by observing it in a microscope. Diffraction effects due to the wave nature of light result in a blurring of the image; the resulting uncertainty in the position of the electron is approximately equal to the wavelength of the light. To reduce this uncertainty, it is necessary to use light of shorter wavelength—e.g., gamma rays. However, in producing an image of the electron, the gamma-ray photon bounces off the electron, giving the Compton effect (see above Early developments: Scattering of X-rays). As a result of the collision, the electron recoils in a statistically random way. The resulting uncertainty in the momentum of the electron is proportional to the momentum of the photon, which is inversely proportional to the wavelength of the photon. So it is again the case that increased precision in knowledge of the position of the electron is gained only at the expense of decreased precision in knowledge of its momentum. A detailed calculation of the process yields the same result as before (equation [12]). Heisenberg’s reasoning brings out clearly the fact that the smaller the particle being observed, the more significant is the uncertainty principle. When a large body is observed, photons still bounce off it and change its momentum, but, considered as a fraction of the initial momentum of the body, the change is insignificant.
    
    The Schrödinger and Dirac theories give a precise value for the energy of each stationary state, but in reality the states do not have a precise energy. The only exception is in the ground (lowest energy) state. Instead, the energies of the states are spread over a small range. The spread arises from the fact that, because the electron can make a transition to another state, the initial state has a finite lifetime. The transition is a random process, and so different atoms in the same state have different lifetimes. If the mean lifetime is denoted as τ, the theory shows that the energy of the initial state has a spread of energy ΔE, given by
    special composition for article "Quantum Mechanics"
    
    This energy spread is manifested in a spread in the frequencies of emitted radiation. Therefore, the spectral lines are not infinitely sharp. (Some experimental factors can also broaden a line, but their effects can be reduced; however, the present effect, known as natural broadening, is fundamental and cannot be reduced.) Equation (13) is another type of Heisenberg uncertainty relation; generally, if a measurement with duration τ is made of the energy in a system, the measurement disturbs the system, causing the energy to be uncertain by an amount ΔE, the magnitude of which is given by the above equation.
    
    Quantum electrodynamics
    The application of quantum theory to the interaction between electrons and radiation requires a quantum treatment of Maxwell’s field equations, which are the foundations of electromagnetism, and the relativistic theory of the electron formulated by Dirac (see above Electron spin and antiparticles). The resulting quantum field theory is known as quantum electrodynamics, or QED.
    
    QED accounts for the behaviour and interactions of electrons, positrons, and photons. It deals with processes involving the creation of material particles from electromagnetic energy and with the converse processes in which a material particle and its antiparticle annihilate each other and produce energy. Initially the theory was beset with formidable mathematical difficulties, because the calculated values of quantities such as the charge and mass of the electron proved to be infinite. However, an ingenious set of techniques developed (in the late 1940s) by Hans Bethe, Julian S. Schwinger, Tomonaga Shin’ichirō, Richard P. Feynman, and others dealt systematically with the infinities to obtain finite values of the physical quantities. Their method is known as renormalization. The theory has provided some remarkably accurate predictions.
    
    According to the Dirac theory, two particular states in hydrogen with different quantum numbers have the same energy. QED, however, predicts a small difference in their energies; the difference may be determined by measuring the frequency of the electromagnetic radiation that produces transitions between the two states. This effect was first measured by Willis E. Lamb, Jr., and Robert Retherford in 1947. Its physical origin lies in the interaction of the electron with the random fluctuations in the surrounding electromagnetic field. These fluctuations, which exist even in the absence of an applied field, are a quantum phenomenon. The accuracy of experiment and theory in this area may be gauged by two recent values for the separation of the two states, expressed in terms of the frequency of the radiation that produces the transitions:
    Comparison of the experimental and theoretical values for the separation of two states of hydrogen.
    
    An even more spectacular example of the success of QED is provided by the value for μe, the magnetic dipole moment of the free electron. Because the electron is spinning and has electric charge, it behaves like a tiny magnet, the strength of which is expressed by the value of μe. According to the Dirac theory, μe is exactly equal to μB = eℏ/2me, a quantity known as the Bohr magneton; however, QED predicts that μe = (1 + a)μB, where a is a small number, approximately 1/860. Again, the physical origin of the QED correction is the interaction of the electron with random oscillations in the surrounding electromagnetic field. The best experimental determination of μe involves measuring not the quantity itself but the small correction term μe − μB. This greatly enhances the sensitivity of the experiment. The most recent results for the value of a are
    Comparison of the experimental and theoretical values of the magnetic dipole moment.
    
    Since a itself represents a small correction term, the magnetic dipole moment of the electron is measured with an accuracy of about one part in 1011. One of the most precisely determined quantities in physics, the magnetic dipole moment of the electron can be calculated correctly from quantum theory to within about one part in 1010.
    
    The interpretation of quantum mechanics
    Although quantum mechanics has been applied to problems in physics with great success, some of its ideas seem strange. A few of their implications are considered here.
    
    The electron: wave or particle?
    double-slit experiment
    double-slit experimentFigure 5: (A) Monochromatic light incident on a pair of slits gives interference fringes (alternate light and dark bands) on a screen, (B) variation in the intensity of the light at the screen when both slits are open. With a single slit, there is no interference pattern; the intensity variation is shown by the broken line. As with Figure 4B, the same diagram would give the variation in the intensity of electrons in the corresponding electron experiment.
    Young’s aforementioned experiment in which a parallel beam of monochromatic light is passed through a pair of narrow parallel slits (Figure 5A) has an electron counterpart. In Young’s original experiment, the intensity of the light varies with direction after passing through the slits (Figure 5B). The intensity oscillates because of interference between the light waves emerging from the two slits, the rate of oscillation depending on the wavelength of the light and the separation of the slits. The oscillation creates a fringe pattern of alternating light and dark bands that is modulated by the diffraction pattern from each slit. If one of the slits is covered, the interference fringes disappear, and only the diffraction pattern (shown as a broken line in Figure 5B) is observed.
    
    Young’s experiment can be repeated with electrons all with the same momentum. The screen in the optical experiment is replaced by a closely spaced grid of electron detectors. There are many devices for detecting electrons; the most common are scintillators. When an electron passes through a scintillating material, such as sodium iodide, the material produces a light flash which gives a voltage pulse that can be amplified and recorded. The pattern of electrons recorded by each detector is the same as that predicted for waves with wavelengths given by the de Broglie formula. Thus, the experiment provides conclusive evidence for the wave behaviour of electrons.
    
    If the experiment is repeated with a very weak source of electrons so that only one electron passes through the slits, a single detector registers the arrival of an electron. This is a well-localized event characteristic of a particle. Each time the experiment is repeated, one electron passes through the slits and is detected. A graph plotted with detector position along one axis and the number of electrons along the other looks exactly like the oscillating interference pattern in Figure 5B. Thus, the intensity function in the figure is proportional to the probability of the electron moving in a particular direction after it has passed through the slits. Apart from its units, the function is identical to Ψ2, where Ψ is the solution of the time-independent Schrödinger equation for this particular experiment.
    
    If one of the slits is covered, the fringe pattern disappears and is replaced by the diffraction pattern for a single slit. Thus, both slits are needed to produce the fringe pattern. However, if the electron is a particle, it seems reasonable to suppose that it passed through only one of the slits. The apparatus can be modified to ascertain which slit by placing a thin wire loop around each slit. When an electron passes through a loop, it generates a small electric signal, showing which slit it passed through. However, the interference fringe pattern then disappears, and the single-slit diffraction pattern returns. Since both slits are needed for the interference pattern to appear and since it is impossible to know which slit the electron passed through without destroying that pattern, one is forced to the conclusion that the electron goes through both slits at the same time.
    
    In summary, the experiment shows both the wave and particle properties of the electron. The wave property predicts the probability of direction of travel before the electron is detected; on the other hand, the fact that the electron is detected in a particular place shows that it has particle properties. Therefore, the answer to the question whether the electron is a wave or a particle is that it is neither. It is an object exhibiting either wave or particle properties, depending on the type of measurement that is made on it. In other words, one cannot talk about the intrinsic properties of an electron; instead, one must consider the properties of the electron and measuring apparatus together.
    
    Hidden variables
    Understand why quantum mechanics does not make absolute predictions but only predicts the different outcomes to happen1 of 3
    Understand why quantum mechanics does not make absolute predictions but only predicts the different outcomes to happenA review of quantum mechanics and its effect on making factual predictions.
    See all videos for this article
    Learn about Niels Bohr and the difference of opinion between Bohr and Albert Einstein on quantum mechanics2 of 3
    Learn about Niels Bohr and the difference of opinion between Bohr and Albert Einstein on quantum mechanicsHear Abraham Pais, Paul Davies, and other authorities discuss Niels Bohr as well as Albert Einstein's objections to Bohr's interpretation of quantum mechanics.
    See all videos for this article
    Know about the element of uncertainty of nature in Niels Bohr's interpretation of quantum theory and its success despite Albert Einstein's objections3 of 3
    Know about the element of uncertainty of nature in Niels Bohr's interpretation of quantum theory and its success despite Albert Einstein's objectionsLearn about the element of indeterminacy in Niels Bohr's interpretation of quantum mechanics.
    See all videos for this article
    A fundamental concept in quantum mechanics is that of randomness, or indeterminacy. In general, the theory predicts only the probability of a certain result. Consider the case of radioactivity. Imagine a box of atoms with identical nuclei that can undergo decay with the emission of an alpha particle. In a given time interval, a certain fraction will decay. The theory may tell precisely what that fraction will be, but it cannot predict which particular nuclei will decay. The theory asserts that, at the beginning of the time interval, all the nuclei are in an identical state and that the decay is a completely random process. Even in classical physics, many processes appear random. For example, one says that, when a roulette wheel is spun, the ball will drop at random into one of the numbered compartments in the wheel. Based on this belief, the casino owner and the players give and accept identical odds against each number for each throw. However, the fact is that the winning number could be predicted if one noted the exact location of the wheel when the croupier released the ball, the initial speed of the wheel, and various other physical parameters. It is only ignorance of the initial conditions and the difficulty of doing the calculations that makes the outcome appear to be random. In quantum mechanics, on the other hand, the randomness is asserted to be absolutely fundamental. The theory says that, though one nucleus decayed and the other did not, they were previously in the identical state.
    
    Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum theory
    Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum theoryLearn about the element of indeterminacy in Niels Bohr's interpretation of quantum mechanics and about Albert Einstein's objections to indeterminacy.
    See all videos for this article
    Many eminent physicists, including Einstein, have not accepted this indeterminacy. They have rejected the notion that the nuclei were initially in the identical state. Instead, they postulated that there must be some other property—presently unknown, but existing nonetheless—that is different for the two nuclei. This type of unknown property is termed a hidden variable; if it existed, it would restore determinacy to physics. If the initial values of the hidden variables were known, it would be possible to predict which nuclei would decay. Such a theory would, of course, also have to account for the wealth of experimental data which conventional quantum mechanics explains from a few simple assumptions. Attempts have been made by de Broglie, David Bohm, and others to construct theories based on hidden variables, but the theories are very complicated and contrived. For example, the electron would definitely have to go through only one slit in the two-slit experiment. To explain that interference occurs only when the other slit is open, it is necessary to postulate a special force on the electron which exists only when that slit is open. Such artificial additions make hidden variable theories unattractive, and there is little support for them among physicists.
    
    The orthodox view of quantum mechanics—and the one adopted in the present article—is known as the Copenhagen interpretation because its main protagonist, Niels Bohr, worked in that city. The Copenhagen view of understanding the physical world stresses the importance of basing theory on what can be observed and measured experimentally. It therefore rejects the idea of hidden variables as quantities that cannot be measured. The Copenhagen view is that the indeterminacy observed in nature is fundamental and does not reflect an inadequacy in present scientific knowledge. One should therefore accept the indeterminacy without trying to “explain” it and see what consequences come from it.
    
    Attempts have been made to link the existence of free will with the indeterminacy of quantum mechanics, but it is difficult to see how this feature of the theory makes free will more plausible. On the contrary, free will presumably implies rational thought and decision, whereas the essence of the indeterminism in quantum mechanics is that it is due to intrinsic randomness.
    
    Paradox of Einstein, Podolsky, and Rosen
    Know about Nicolas Gisin and his team's experiment to test the Einstein-Podolsky-Rosen paradox
    Know about Nicolas Gisin and his team's experiment to test the Einstein-Podolsky-Rosen paradoxLearn how the Einstein-Podolsky-Rosen paradox was put to the test by Nicolas Gisin's group at the University of Geneva, Switzerland.
    See all videos for this article
    In 1935 Einstein and two other physicists in the United States, Boris Podolsky and Nathan Rosen, analyzed a thought experiment to measure position and momentum in a pair of interacting systems. Employing conventional quantum mechanics, they obtained some startling results, which led them to conclude that the theory does not give a complete description of physical reality. Their results, which are so peculiar as to seem paradoxical, are based on impeccable reasoning, but their conclusion that the theory is incomplete does not necessarily follow. Bohm simplified their experiment while retaining the central point of their reasoning; this discussion follows his account.
    
    The proton, like the electron, has spin 1/2; thus, no matter what direction is chosen for measuring the component of its spin angular momentum, the values are always +ℏ/2 or −ℏ/2. (The present discussion relates only to spin angular momentum, and the word spin is omitted from now on.) It is possible to obtain a system consisting of a pair of protons in close proximity and with total angular momentum equal to zero. Thus, if the value of one of the components of angular momentum for one of the protons is +ℏ/2 along any selected direction, the value for the component in the same direction for the other particle must be −ℏ/2. Suppose the two protons move in opposite directions until they are far apart. The total angular momentum of the system remains zero, and if the component of angular momentum along the same direction for each of the two particles is measured, the result is a pair of equal and opposite values. Therefore, after the quantity is measured for one of the protons, it can be predicted for the other proton; the second measurement is unnecessary. As previously noted, measuring a quantity changes the state of the system. Thus, if measuring Sx (the x-component of angular momentum) for proton 1 produces the value +ℏ/2, the state of proton 1 after measurement corresponds to Sx = +ℏ/2, and the state of proton 2 corresponds to Sx = −ℏ/2. Any direction, however, can be chosen for measuring the component of angular momentum. Whichever direction is selected, the state of proton 1 after measurement corresponds to a definite component of angular momentum about that direction. Furthermore, since proton 2 must have the opposite value for the same component, it follows that the measurement on proton 1 results in a definite state for proton 2 relative to the chosen direction, notwithstanding the fact that the two particles may be millions of kilometres apart and are not interacting with each other at the time. Einstein and his two collaborators thought that this conclusion was so obviously false that the quantum mechanical theory on which it was based must be incomplete. They concluded that the correct theory would contain some hidden variable feature that would restore the determinism of classical physics.
    
    A comparison of how quantum theory and classical theory describe angular momentum for particle pairs illustrates the essential difference between the two outlooks. In both theories, if a system of two particles has a total angular momentum of zero, then the angular momenta of the two particles are equal and opposite. If the components of angular momentum are measured along the same direction, the two values are numerically equal, one positive and the other negative. Thus, if one component is measured, the other can be predicted. The crucial difference between the two theories is that, in classical physics, the system under investigation is assumed to have possessed the quantity being measured beforehand. The measurement does not disturb the system; it merely reveals the preexisting state. It may be noted that, if a particle were actually to possess components of angular momentum prior to measurement, such quantities would constitute hidden variables.
    
    Understand the concept of teleportation and how quantum mechanics makes photon teleportation possible1 of 2
    Understand the concept of teleportation and how quantum mechanics makes photon teleportation possibleHow quantum mechanics makes photon teleportation possible.
    See all videos for this article
    measuring correlation between photons
    2 of 2
    measuring correlation between photonsFigure 6: Experiment to determine the correlation in measured angular momentum values for a pair of protons with zero total angular momentum. The two protons are initially at the point 0 and move in opposite directions toward the two magnets.
    Does nature behave as quantum mechanics predicts? The answer comes from measuring the components of angular momenta for the two protons along different directions with an angle θ between them. A measurement on one proton can give only the result +ℏ/2 or −ℏ/2. The experiment consists of measuring correlations between the plus and minus values for pairs of protons with a fixed value of θ, and then repeating the measurements for different values of θ, as in Figure 6. The interpretation of the results rests on an important theorem by the Irish-born physicist John Stewart Bell. Bell began by assuming the existence of some form of hidden variable with a value that would determine whether the measured angular momentum gives a plus or minus result. He further assumed locality—namely, that measurement on one proton (i.e., the choice of the measurement direction) cannot affect the result of the measurement on the other proton. Both these assumptions agree with classical, commonsense ideas. He then showed quite generally that these two assumptions lead to a certain relationship, now known as Bell’s inequality, for the correlation values mentioned above. Experiments have been conducted at several laboratories with photons instead of protons (the analysis is similar), and the results show fairly conclusively that Bell’s inequality is violated. That is to say, the observed results agree with those of quantum mechanics and cannot be accounted for by a hidden variable (or deterministic) theory based on the concept of locality. One is forced to conclude that the two protons are a correlated pair and that a measurement on one affects the state of both, no matter how far apart they are. This may strike one as highly peculiar, but such is the way nature appears to be.
    
    It may be noted that the effect on the state of proton 2 following a measurement on proton 1 is believed to be instantaneous; the effect happens before a light signal initiated by the measuring event at proton 1 reaches proton 2. Alain Aspect and his coworkers in Paris demonstrated this result in 1982 with an ingenious experiment in which the correlation between the two angular momenta was measured, within a very short time interval, by a high-frequency switching device. The interval was less than the time taken for a light signal to travel from one particle to the other at the two measurement positions. Einstein’s special theory of relativity states that no message can travel with a speed greater than that of light. Thus, there is no way that the information concerning the direction of the measurement on the first proton could reach the second proton before the measurement was made on it.
    
    Measurement in quantum mechanics
    The way quantum mechanics treats the process of measurement has caused considerable debate. Schrödinger’s time-dependent wave equation (equation [8]) is an exact recipe for determining the way the wave function varies with time for a given physical system in a given physical environment. According to the Schrödinger equation, the wave function varies in a strictly determinate way. On the other hand, in the axiomatic approach to quantum mechanics described above, a measurement changes the wave function abruptly and discontinuously. Before the measurement is made, the wave function Ψ is a mixture of the ψs as indicated in equation (10). The measurement changes Ψ from a mixture of ψs to a single ψ. This change, brought about by the process of measurement, is termed the collapse or reduction of the wave function. The collapse is a discontinuous change in Ψ; it is also unpredictable, because, starting with the same Ψ represented by the right-hand side of equation (10), the end result can be any one of the individual ψs.
    
    The Schrödinger equation, which gives a smooth and predictable variation of Ψ, applies between the measurements. The measurement process itself, however, cannot be described by the Schrödinger equation; it is somehow a thing apart. This appears unsatisfactory, inasmuch as a measurement is a physical process and ought to be the subject of the Schrödinger equation just like any other physical process.
    
    The difficulty is related to the fact that quantum mechanics applies to microscopic systems containing one (or a few) electrons, protons, or photons. Measurements, however, are made with large-scale objects (e.g., detectors, amplifiers, and meters) in the macroscopic world, which obeys the laws of classical physics. Thus, another way of formulating the question of what happens in a measurement is to ask how the microscopic quantum world relates and interacts with the macroscopic classical world. More narrowly, it can be asked how and at what point in the measurement process does the wave function collapse? So far, there are no satisfactory answers to these questions, although there are several schools of thought.
    
    One approach stresses the role of a conscious observer in the measurement process and suggests that the wave function collapses when the observer reads the measuring instrument. Bringing the conscious mind into the measurement problem seems to raise more questions than it answers, however.
    
    As discussed above, the Copenhagen interpretation of the measurement process is essentially pragmatic. It distinguishes between microscopic quantum systems and macroscopic measuring instruments. The initial object or event—e.g., the passage of an electron, photon, or atom—triggers the classical measuring device into giving a reading; somewhere along the chain of events, the result of the measurement becomes fixed (i.e., the wave function collapses). This does not answer the basic question but says, in effect, not to worry about it. This is probably the view of most practicing physicists.
    
    A third school of thought notes that an essential feature of the measuring process is irreversibility. This contrasts with the behaviour of the wave function when it varies according to the Schrödinger equation; in principle, any such variation in the wave function can be reversed by an appropriate experimental arrangement. However, once a classical measuring instrument has given a reading, the process is not reversible. It is possible that the key to the nature of the measurement process lies somewhere here. The Schrödinger equation is known to apply only to relatively simple systems. It is an enormous extrapolation to assume that the same equation applies to the large and complex system of a classical measuring device. It may be that the appropriate equation for such a system has features that produce irreversible effects (e.g., wave-function collapse) which differ in kind from those for a simple system.
    
    One may also mention the so-called many-worlds interpretation, proposed by Hugh Everett III in 1957, which suggests that, when a measurement is made for a system in which the wave function is a mixture of states, the universe branches into a number of noninteracting universes. Each of the possible outcomes of the measurement occurs, but in a different universe. Thus, if Sx = 
    1
    /
    2
     is the result of a Stern-Gerlach measurement on a silver atom (see above Incompatible observables), there is another universe identical to ours in every way (including clones of people), except that the result of the measurement is Sx = −1/2. Although this fanciful model solves some measurement problems, it has few adherents among physicists.
    
    Because the various ways of looking at the measurement process lead to the same experimental consequences, trying to distinguish between them on scientific grounds may be fruitless. One or another may be preferred on the grounds of plausibility, elegance, or economy of hypotheses, but these are matters of individual taste. Whether one day a satisfactory quantum theory of measurement will emerge, distinguished from the others by its verifiable predictions, remains an open question.
    
    Applications of quantum mechanics
    Know about the future of quantum technology and how quantum devices can be present in everyday technology
    Know about the future of quantum technology and how quantum devices can be present in everyday technologyLearn about the future of quantum technology.
    See all videos for this article
    As has been noted, quantum mechanics has been enormously successful in explaining microscopic phenomena in all branches of physics. The three phenomena described in this section are examples that demonstrate the quintessence of the theory.
    
    Decay of the kaon
    The kaon (also called the K0 meson), discovered in 1947, is produced in high-energy collisions between nuclei and other particles. It has zero electric charge, and its mass is about one-half the mass of the proton. It is unstable and, once formed, rapidly decays into either 2 or 3 pi-mesons. The average lifetime of the kaon is about 10−10 second.
    
    In spite of the fact that the kaon is uncharged, quantum theory predicts the existence of an antiparticle with the same mass, decay products, and average lifetime; the antiparticle is denoted by K0. During the early 1950s, several physicists questioned the justification for postulating the existence of two particles with such similar properties. In 1955, however, Murray Gell-Mann and Abraham Pais made an interesting prediction about the decay of the kaon. Their reasoning provides an excellent illustration of the quantum mechanical axiom that the wave function Ψ can be a superposition of states; in this case, there are two states, the K0 and K0 mesons themselves.
    
    A K0 meson may be represented formally by writing the wave function as Ψ = K0; similarly Ψ = K0 represents a K0 meson. From the two states, K0 and K0, the following two new states are constructed:
    special composition for article "Quantum Mechanics"
    special composition for article "Quantum Mechanics"
    
    From these two equations it follows that
    special composition for article "Quantum Mechanics"
    special composition for article "Quantum Mechanics"
    
    The reason for defining the two states K1 and K2 is that, according to quantum theory, when the K0 decays, it does not do so as an isolated particle; instead, it combines with its antiparticle to form the states K1 and K2. The state K1 (called the K-short [K0S]) decays into two pi-mesons with a very short lifetime (about 9 × 10−11 second), while K2 (called the K-long [K0L]) decays into three pi-mesons with a longer lifetime (about 5 × 10−8 second).
    
    decay of K0 meson
    decay of K0 mesonFigure 7: Decay of the K0 meson.
    The physical consequences of these results may be demonstrated in the following experiment. K0 particles are produced in a nuclear reaction at the point A (Figure 7). They move to the right in the figure and start to decay. At point A, the wave function is Ψ = K0, which, from equation (16), can be expressed as the sum of K1 and K2. As the particles move to the right, the K1 state begins to decay rapidly. If the particles reach point B in about 10−8 second, nearly all the K1 component has decayed, although hardly any of the K2 component has done so. Thus, at point B, the beam has changed from one of pure K0 to one of almost pure K2, which equation (15) shows is an equal mixture of K0 and K0. In other words, K0 particles appear in the beam simply because K1 and K2 decay at different rates. At point B, the beam enters a block of absorbing material. Both the K0 and K0 are absorbed by the nuclei in the block, but the K0 are absorbed more strongly. As a result, even though the beam is an equal mixture of K0 and K0 when it enters the absorber, it is almost pure K0 when it exits at point C. The beam thus begins and ends as K0.
    
    Gell-Mann and Pais predicted all this, and experiments subsequently verified it. The experimental observations are that the decay products are primarily two pi-mesons with a short decay time near A, three pi-mesons with longer decay time near B, and two pi-mesons again near C. (This account exaggerates the changes in the K1 and K2 components between A and B and in the K0 and K0 components between B and C; the argument, however, is unchanged.) The phenomenon of generating the K0 and regenerating the K1 decay is purely quantum. It rests on the quantum axiom of the superposition of states and has no classical counterpart.
    
    Cesium clock
    The cesium clock is the most accurate type of clock yet developed. This device makes use of transitions between the spin states of the cesium nucleus and produces a frequency which is so regular that it has been adopted for establishing the time standard.
    
    Like electrons, many atomic nuclei have spin. The spin of these nuclei produces a set of small effects in the spectra, known as hyperfine structure. (The effects are small because, though the angular momentum of a spinning nucleus is of the same magnitude as that of an electron, its magnetic moment, which governs the energies of the atomic levels, is relatively small.) The nucleus of the cesium atom has spin quantum number 7/2. The total angular momentum of the lowest energy states of the cesium atom is obtained by combining the spin angular momentum of the nucleus with that of the single valence electron in the atom. (Only the valence electron contributes to the angular momentum because the angular momenta of all the other electrons total zero. Another simplifying feature is that the ground states have zero orbital momenta, so only spin angular momenta need to be considered.) When nuclear spin is taken into account, the total angular momentum of the atom is characterized by a quantum number, conventionally denoted by F, which for cesium is 4 or 3. These values come from the spin value 7/2 for the nucleus and 1/2 for the electron. If the nucleus and the electron are visualized as tiny spinning tops, the value F = 4 (7/2 + 1/2) corresponds to the tops spinning in the same sense, and F = 3 (7/2 − 1/2) corresponds to spins in opposite senses. The energy difference ΔE of the states with the two F values is a precise quantity. If electromagnetic radiation of frequency ν0, where
    special composition for article "Quantum Mechanics"
    is applied to a system of cesium atoms, transitions will occur between the two states. An apparatus that can detect the occurrence of transitions thus provides an extremely precise frequency standard. This is the principle of the cesium clock.
    
    cesium clock
    cesium clockFigure 8: Cesium clock.
    The apparatus is shown schematically in Figure 8. A beam of cesium atoms emerges from an oven at a temperature of about 100 °C. The atoms pass through an inhomogeneous magnet A, which deflects the atoms in state F = 4 downward and those in state F = 3 by an equal amount upward. The atoms pass through slit S and continue into a second inhomogeneous magnet B. Magnet B is arranged so that it deflects atoms with an unchanged state in the same direction that magnet A deflected them. The atoms follow the paths indicated by the broken lines in the figure and are lost to the beam. However, if an alternating electromagnetic field of frequency ν0 is applied to the beam as it traverses the centre region C, transitions between states will occur. Some atoms in state F = 4 will change to F = 3, and vice versa. For such atoms, the deflections in magnet B are reversed. The atoms follow the whole lines in the diagram and strike a tungsten wire, which gives electric signals in proportion to the number of cesium atoms striking the wire. As the frequency ν of the alternating field is varied, the signal has a sharp maximum for ν = ν0. The length of the apparatus from the oven to the tungsten detector is about one metre.
    
    cesium-133 states
    cesium-133 statesFigure 9: Variation of energy with magnetic-field strength for the F = 4 and F = 3 states in cesium-133.
    Each atomic state is characterized not only by the quantum number F but also by a second quantum number mF. For F = 4, mF can take integral values from 4 to −4. In the absence of a magnetic field, these states have the same energy. A magnetic field, however, causes a small change in energy proportional to the magnitude of the field and to the mF value. Similarly, a magnetic field changes the energy for the F = 3 states according to the mF value which, in this case, may vary from 3 to −3. The energy changes are indicated in Figure 9. In the cesium clock, a weak constant magnetic field is superposed on the alternating electromagnetic field in region C. The theory shows that the alternating field can bring about a transition only between pairs of states with mF values that are the same or that differ by unity. However, as can be seen from the figure, the only transitions occurring at the frequency ν0 are those between the two states with mF = 0. The apparatus is so sensitive that it can discriminate easily between such transitions and all the others.
    
    If the frequency of the oscillator drifts slightly so that it does not quite equal ν0, the detector output drops. The change in signal strength produces a signal to the oscillator to bring the frequency back to the correct value. This feedback system keeps the oscillator frequency automatically locked to ν0.
    
    The cesium clock is exceedingly stable. The frequency of the oscillator remains constant to about one part in 1013. For this reason, the device is used to redefine the second. This base unit of time in the SI system is defined as equal to 9,192,631,770 cycles of the radiation corresponding to the transition between the levels F = 4, mF = 0 and F = 3, mF = 0 of the ground state of the cesium-133 atom. Prior to 1967, the second was defined in terms of the motion of Earth. The latter, however, is not nearly as stable as the cesium clock. Specifically, the fractional variation of Earth’s rotation period is a few hundred times larger than that of the frequency of the cesium clock.</p>


<p>A quantum voltage standard
    Quantum theory has been used to establish a voltage standard, and this standard has proven to be extraordinarily accurate and consistent from laboratory to laboratory.
    
    If two layers of superconducting material are separated by a thin insulating barrier, a supercurrent (i.e., a current of paired electrons) can pass from one superconductor to the other. This is another example of the tunneling process described earlier. Several effects based on this phenomenon were predicted in 1962 by the British physicist Brian D. Josephson. Demonstrated experimentally soon afterwards, they are now referred to as the Josephson effects.
    
    If a DC (direct-current) voltage V is applied across the two superconductors, the energy of an electron pair changes by an amount of 2eV as it crosses the junction. As a result, the supercurrent oscillates with frequency ν given by the Planck relationship (E = hν). Thus,
    special composition for article "Quantum Mechanics"
    
    This oscillatory behaviour of the supercurrent is known as the AC (alternating-current) Josephson effect. Measurement of V and ν permits a direct verification of the Planck relationship. Although the oscillating supercurrent has been detected directly, it is extremely weak. A more sensitive method of investigating equation (19) is to study effects resulting from the interaction of microwave radiation with the supercurrent.
    
    Several carefully conducted experiments have verified equation (19) to such a high degree of precision that it has been used to determine the value of 2e/h. This value can in fact be determined more precisely by the AC Josephson effect than by any other method. The result is so reliable that laboratories now employ the AC Josephson effect to set a voltage standard. The numerical relationship between V and ν is
    Special composition for "Quantum Mechanics" article. equation 20
    
    In this way, measuring a frequency, which can be done with great precision, gives the value of the voltage. Before the Josephson method was used, the voltage standard in metrological laboratories devoted to the maintenance of physical units was based on high-stability Weston cadmium cells. These cells, however, tend to drift and so caused inconsistencies between standards in different laboratories. The Josephson method has provided a standard giving agreement to within a few parts in 108 for measurements made at different times and in different laboratories.
    
    The experiments described in the preceding two sections are only two examples of high-precision measurements in physics. The values of the fundamental constants, such as c, h, e, and me, are determined from a wide variety of experiments based on quantum phenomena. The results are so consistent that the values of the constants are thought to be known in most cases to better than one part in 108. Physicists may not know what they are doing when they make a measurement, but they do it extremely well.
    
    Gordon Leslie Squires
    Science
    Physics
    Matter & Energy
    relativity
    physics
        
    Written by 
    Fact-checked by 
    Article History
    Key People: Albert Einstein Henri Poincaré Bernhard Riemann Arthur Eddington Hermann Weyl
    Related Topics: time dilation twin paradox special relativity general relativity hypertime
    relativity, wide-ranging physical theories formed by the German-born physicist Albert Einstein. With his theories of special relativity (1905) and general relativity (1915), Einstein overthrew many assumptions underlying earlier physical theories, redefining in the process the fundamental concepts of space, time, matter, energy, and gravity. Along with quantum mechanics, relativity is central to modern physics. In particular, relativity provides the basis for understanding cosmic processes and the geometry of the universe itself.
    
    (Read Einstein’s 1926 Britannica essay on space-time.)
    
    Explaining E = mc2
    Explaining E = mc2Brian Greene kicking off his Daily Equation video series with Albert Einstein's famous equation E = mc2.
    See all videos for this article
    “Special relativity” is limited to objects that are moving with respect to inertial frames of reference—i.e, in a state of uniform motion with respect to one another such that an observer cannot, by purely mechanical experiments, distinguish one from the other. Beginning with the behaviour of light (and all other electromagnetic radiation), the theory of special relativity draws conclusions that are contrary to everyday experience but fully confirmed by experiments. Special relativity revealed that the speed of light is a limit that can be approached but not reached by any material object; it is the origin of the most famous equation in science, E = mc2; and it has led to other tantalizing outcomes, such as the “twin paradox.”
    
    “General relativity” is concerned with gravity, one of the fundamental forces in the universe. (The others are electromagnetism, the strong force, and the weak force.) Gravity defines macroscopic behaviour, and so general relativity describes large-scale physical phenomena such as planetary dynamics, the birth and death of stars, black holes, and the evolution of the universe.
    
    Special and general relativity have profoundly affected physical science and human existence, most dramatically in applications of nuclear energy and nuclear weapons. Additionally, relativity and its rethinking of the fundamental categories of space and time have provided a basis for certain philosophical, social, and artistic interpretations that have influenced human culture in different ways.
    
    Italian physicist Guglielmo Marconi at work in the wireless room of his yacht Electra, c. 1920.
    Britannica Quiz
    All About Physics Quiz
    Cosmology before relativity
    The mechanical universe
    Relativity changed the scientific conception of the universe, which began in efforts to grasp the dynamic behaviour of matter. In Renaissance times, the great Italian physicist Galileo Galilei moved beyond Aristotle’s philosophy to introduce the modern study of mechanics, which requires quantitative measurements of bodies moving in space and time. His work and that of others led to basic concepts, such as velocity, which is the distance a body covers in a given direction per unit time; acceleration, the rate of change of velocity; mass, the amount of material in a body; and force, a push or pull on a body.
    
    The next major stride occurred in the late 17th century, when the British scientific genius Isaac Newton formulated his three famous laws of motion, the first and second of which are of special concern in relativity. Newton’s first law, known as the law of inertia, states that a body that is not acted upon by external forces undergoes no acceleration—either remaining at rest or continuing to move in a straight line at constant speed. Newton’s second law states that a force applied to a body changes its velocity by producing an acceleration that is proportional to the force and inversely proportional to the mass of the body. In constructing his system, Newton also defined space and time, taking both to be absolutes that are unaffected by anything external. Time, he wrote, “flows equably,” while space “remains always similar and immovable.”
    
    
    Are you a student?
    Get a special academic rate on Britannica Premium.
    Newton’s laws proved valid in every application, as in calculating the behaviour of falling bodies, but they also provided the framework for his landmark law of gravity (the term, derived from the Latin gravis, or “heavy,” had been in use since at least the 16th century). Beginning with the (perhaps mythical) observation of a falling apple and then considering the Moon as it orbits Earth, Newton concluded that an invisible force acts between the Sun and its planets. He formulated a comparatively simple mathematical expression for the gravitational force; it states that every object in the universe attracts every other object with a force that operates through empty space and that varies with the masses of the objects and the distance between them.
    
    The law of gravity was brilliantly successful in explaining the mechanism behind Kepler’s laws of planetary motion, which the German astronomer Johannes Kepler had formulated at the beginning of the 17th century. Newton’s mechanics and law of gravity, along with his assumptions about the nature of space and time, seemed wholly successful in explaining the dynamics of the universe, from motion on Earth to cosmic events.
    
    Light and the ether
    However, this success at explaining natural phenomena came to be tested from an unexpected direction—the behaviour of light, whose intangible nature had puzzled philosophers and scientists for centuries. In 1865 the Scottish physicist James Clerk Maxwell showed that light is an electromagnetic wave with oscillating electrical and magnetic components. Maxwell’s equations predicted that electromagnetic waves would travel through empty space at a speed of almost exactly 3 × 108 metres per second (186,000 miles per second)—i.e., according with the measured speed of light. Experiments soon confirmed the electromagnetic nature of light and established its speed as a fundamental parameter of the universe.
    
    Maxwell’s remarkable result answered long-standing questions about light, but it raised another fundamental issue: if light is a moving wave, what medium supports it? Ocean waves and sound waves consist of the progressive oscillatory motion of molecules of water and of atmospheric gases, respectively. But what is it that vibrates to make a moving light wave? Or to put it another way, how does the energy embodied in light travel from point to point?
    
    For Maxwell and other scientists of the time, the answer was that light traveled in a hypothetical medium called the ether (aether). Supposedly, this medium permeated all space without impeding the motion of planets and stars; yet it had to be more rigid than steel so that light waves could move through it at high speed, in the same way that a taut guitar string supports fast mechanical vibrations. Despite this contradiction, the idea of the ether seemed essential—until a definitive experiment disproved it.
    
    In 1887 the German-born American physicist A.A. Michelson and the American chemist Edward Morley made exquisitely precise measurements to determine how Earth’s motion through the ether affected the measured speed of light. In classical mechanics, Earth’s movement would add to or subtract from the measured speed of light waves, just as the speed of a ship would add to or subtract from the speed of ocean waves as measured from the ship. But the Michelson-Morley experiment had an unexpected outcome, for the measured speed of light remained the same regardless of Earth’s motion. This could only mean that the ether had no meaning and that the behaviour of light could not be explained by classical physics. The explanation emerged, instead, from Einstein’s theory of special relativity.
    
    Special relativity
    Einstein’s Gedankenexperiments
    Scientists such as Austrian physicist Ernst Mach and French mathematician Henri Poincaré had critiqued classical mechanics or contemplated the behaviour of light and the meaning of the ether before Einstein. Their efforts provided a background for Einstein’s unique approach to understanding the universe, which he called in his native German a Gedankenexperiment, or “thought experiment.”
    
    curvature and parallel motion
    curvature and parallel motionAlbert Einstein described gravity in terms of the curvature of space and time. Brian Greene introduces the mathematics of curvature, which Einstein used to fashion his gravitational field equations. This video is an episode in Greene's Daily Equation series.
    See all videos for this article
    Einstein described how at age 16 he watched himself in his mind’s eye as he rode on a light wave and gazed at another light wave moving parallel to his. According to classical physics, Einstein should have seen the second light wave moving at a relative speed of zero. However, Einstein knew that Maxwell’s electromagnetic equations absolutely require that light always move at 3 × 108 metres per second in a vacuum. Nothing in the theory allows a light wave to have a speed of zero. Another problem arose as well: if a fixed observer sees light as having a speed of 3 × 108 metres per second, whereas an observer moving at the speed of light sees light as having a speed of zero, it would mean that the laws of electromagnetism depend on the observer. But in classical mechanics the same laws apply for all observers, and Einstein saw no reason why the electromagnetic laws should not be equally universal. The constancy of the speed of light and the universality of the laws of physics for all observers are cornerstones of special relativity.
    
    Starting points and postulates
    Understand Albert Einstein's theory of relativity, about what is absolute and not relative
    Understand Albert Einstein's theory of relativity, about what is absolute and not relativeDescription of Albert Einstein's theory of relativity as a theory about what is absolute and not relative.
    See all videos for this article
    In developing special relativity, Einstein began by accepting what experiment and his own thinking showed to be the true behaviour of light, even when this contradicted classical physics or the usual perceptions about the world.
    
    invariance of the speed of light
    invariance of the speed of light Arrows shot from a moving train (A) and from a stationary location (B) will arrive at a target at different velocities—in this case, 300 and 200 km/hr, respectively, because of the motion of the train. However, such commonsense addition of velocities does not apply to light. Even for a train traveling at the speed of light, both laser beams, A and B, have the same velocity: c.
    The fact that the speed of light is the same for all observers is inexplicable in ordinary terms. If a passenger in a train moving at 100 km per hour shoots an arrow in the train’s direction of motion at 200 km per hour, a trackside observer would measure the speed of the arrow as the sum of the two speeds, or 300 km per hour. In analogy, if the train moves at the speed of light and a passenger shines a laser in the same direction, then common sense indicates that a trackside observer should see the light moving at the sum of the two speeds, or twice the speed of light (6 × 108 metres per second).
    
    While such a law of addition of velocities is valid in classical mechanics, the Michelson-Morley experiment showed that light does not obey this law. This contradicts common sense; it implies, for instance, that both a train moving at the speed of light and a light beam emitted from the train arrive at a point farther along the track at the same instant.
    
    Nevertheless, Einstein made the constancy of the speed of light for all observers a postulate of his new theory. As a second postulate, he required that the laws of physics have the same form for all observers. Then Einstein extended his postulates to their logical conclusions to form special relativity.
    
    Consequences of the postulates
    Relativistic space and time
    In order to make the speed of light constant, Einstein replaced absolute space and time with new definitions that depend on the state of motion of an observer. Einstein explained his approach by considering two observers and a train. One observer stands alongside a straight track; the other rides a train moving at constant speed along the track. Each views the world relative to his own surroundings. The fixed observer measures distance from a mark inscribed on the track and measures time with his watch; the train passenger measures distance from a mark inscribed on his railroad car and measures time with his own watch.
    
    Newtonian reference frames
    Newtonian reference frames Isaac Newton reconciled different frames of reference with the equation x′ = x − vt, where time (t) is assumed to be synchronous (that is, running at the same rate in both frames), x indicates the distance between an event and a stationary observer, x′ indicates the distance between the same event and a moving observer, and v is the moving observer's velocity.
    If time flows the same for both observers, as Newton believed, then the two frames of reference are reconciled by the relation: x′ = x − vt. Here x is the distance to some specific event that happens along the track, as measured by the fixed observer; x′ is the distance to the same event as measured by the moving observer; v is the speed of the train—that is, the speed of one observer relative to the other; and t is the time at which the event happens, the same for both observers. For example, suppose the train moves at 40 km per hour. One hour after it sets out, a tree 60 km from the train’s starting point is struck by lightning. The fixed observer measures x as 60 km and t as one hour. The moving observer also measures t as one hour, and so, according to Newton’s equation, he measures x′ as 20 km.
    
    simultaneous events
    simultaneous events Simultaneous events may appear to coincide in time for one observer but not for another because of differences in their spatial positions.
    This analysis seems obvious, but Einstein saw a subtlety hidden in its underlying assumptions—in particular, the issue of simultaneity. The two people do not actually observe the lightning strike at the same time. Even at the speed of light, the image of the strike takes time to reach each observer, and, since each is at a different distance from the event, the travel times differ. Taking this insight further, suppose lightning strikes two trees, one 60 km ahead of the fixed observer and the other 60 km behind, exactly as the moving observer passes the fixed observer. Each image travels the same distance to the fixed observer, and so he certainly sees the events simultaneously. The motion of the moving observer brings him closer to one event than the other, however, and he thus sees the events at different times.
    
    Einstein concluded that simultaneity is relative; events that are simultaneous for one observer may not be for another. This led him to the counterintuitive idea that time flows differently according to the state of motion and to the conclusion that distance is also relative. In the example, the train passenger and the fixed observer can each stretch a tape measure from back to front of a railroad car to find its length. The two ends of the tape must be placed in position at the same instant—that is, simultaneously—to obtain a true value. However, because the meaning of simultaneous is different for the two observers, they measure different lengths.
    
    This reasoning led Einstein to new equations for time and space, called the Lorentz transformations, after the Dutch physicist Hendrik Lorentz, who first proposed them. They are:
    Lorentz transformations
    where t′ is time as measured by the moving observer and c is the speed of light.
    
    From these equations, Einstein derived a new relationship that replaces the classical law of addition of velocities,
    Einstein's velocity addition
    where u and u′ are the speed of any moving object as seen by each observer and v is again the speed of one observer relative to the other. This relation guarantees Einstein’s first postulate (that the speed of light is constant for all observers). In the case of the flashlight beam projected from a train moving at the speed of light, an observer on the train measures the speed of the beam as c. According to the equation above, so does the trackside observer, instead of the value 2c that classical physics predicts.
    
    length contraction and time dilation
    length contraction and time dilation As an object approaches the speed of light, an observer sees the object become shorter and its time interval become longer, relative to the length and time interval when the object is at rest.
    To make the speed of light constant, the theory requires that space and time change in a moving body, according to its speed, as seen by an outside observer. The body becomes shorter along its direction of motion; that is, its length contracts. Time intervals become longer, meaning that time runs more slowly in a moving body; that is, time dilates. In the train example, the person next to the track measures a shorter length for the train and a longer time interval for clocks on the train than does the train passenger. The relations describing these changes are
    relativistic length-time
    where L0 and T0, called proper length and proper time, respectively, are the values measured by an observer on the moving body, and L and T are the corresponding quantities as measured by a fixed observer.
    
    The relativistic effects become large at speeds near that of light, although it is worth noting again that they appear only when an observer looks at a moving body. He never sees changes in space or time within his own reference frame (whether on a train or spacecraft), even at the speed of light. These effects do not appear in ordinary life, because the factor v2/c2 is minute at even the highest speeds attained by humans, so that Einstein’s equations become virtually the same as the classical ones.
    
    Relativistic mass
    Cosmic speed limit
    To derive further results, Einstein combined his redefinitions of time and space with two powerful physical principles: conservation of energy and conservation of mass, which state that the total amount of each remains constant in a closed system. Einstein’s second postulate ensured that these laws remained valid for all observers in the new theory, and he used them to derive the relativistic meanings of mass and energy.
    
    One result is that the mass of a body increases with its speed. An observer on a moving body, such as a spacecraft, measures its so-called rest mass m0, while a fixed observer measures its mass m as
    relativistic mass
    which is greater than m0. In fact, as the spacecraft’s speed approaches that of light, the mass m approaches infinity. However, as the object’s mass increases, so does the energy required to keep accelerating it; thus, it would take infinite energy to accelerate a material body to the speed of light. For this reason, no material object can reach the speed of light, which is the speed limit for the universe. (Light itself can attain this speed because the rest mass of a photon, the quantum particle of light, is zero.)
    
    E = mc2
    Einstein’s treatment of mass showed that the increased relativistic mass comes from the energy of motion of the body—that is, its kinetic energy E—divided by c2. This is the origin of the famous equation E = mc2, which expresses the fact that mass and energy are the same physical entity and can be changed into each other.
    
    The twin paradox
    The counterintuitive nature of Einstein’s ideas makes them difficult to absorb and gives rise to situations that seem unfathomable. One well-known case is the twin paradox, a seeming anomaly in how special relativity describes time.
    
    Suppose that one of two identical twin sisters flies off into space at nearly the speed of light. According to relativity, time runs more slowly on her spacecraft than on Earth; therefore, when she returns to Earth, she will be younger than her Earth-bound sister. But in relativity, what one observer sees as happening to a second one, the second one sees as happening to the first one. To the space-going sister, time moves more slowly on Earth than in her spacecraft; when she returns, her Earth-bound sister is the one who is younger. How can the space-going twin be both younger and older than her Earth-bound sister?
    
    Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
    Britannica Quiz
    Physics and Natural Law
    The answer is that the paradox is only apparent, for the situation is not appropriately treated by special relativity. To return to Earth, the spacecraft must change direction, which violates the condition of steady straight-line motion central to special relativity. A full treatment requires general relativity, which shows that there would be an asymmetrical change in time between the two sisters. Thus, the “paradox” does not cast doubt on how special relativity describes time, which has been confirmed by numerous experiments.
    
    Four-dimensional space-time
    Special relativity is less definite than classical physics in that both the distance D and time interval T between two events depend on the observer. Einstein noted, however, that a particular combination of D and T, the quantity D2 − c2T2, has the same value for all observers.
    
    The term cT in this invariant quantity elevates time to a kind of mathematical parity with space. Noting this, the German mathematical physicist Hermann Minkowski showed that the universe resembles a four-dimensional structure with coordinates x, y, z, and ct representing length, width, height, and time, respectively. Hence, the universe can be described as a four-dimensional space-time continuum, a central concept in general relativity.
    
    Experimental evidence for special relativity
    Because relativistic changes are small at typical speeds for macroscopic objects, the confirmation of special relativity has relied on either the examination of subatomic bodies at high speeds or the measurement of small changes by sensitive instrumentation. For example, ultra-accurate clocks were placed on a variety of commercial airliners flying at one-millionth the speed of light. After two days of continuous flight, the time shown by the airborne clocks differed by fractions of a microsecond from that shown by a synchronized clock left on Earth, as predicted.
    
    Larger effects are seen with elementary particles moving at speeds close to that of light. One such experiment involved muons, elementary particles created by cosmic rays in Earth’s atmosphere at an altitude of about 9 km (30,000 feet). At 99.8 percent of the speed of light, the muons should reach sea level in 31 microseconds, but measurements showed that it took only 2 microseconds. The reason is that, relative to the moving muons, the distance of 9 km contracted to 0.58 km (1,900 feet). Similarly, a relativistic mass increase has been confirmed in measurements on fast-moving elementary particles, where the change is large (see below Particle accelerators).
    
    Such results leave no doubt that special relativity correctly describes the universe, although the theory is difficult to accept at a visceral level. Some insight comes from Einstein’s comment that in relativity the limiting speed of light plays the role of an infinite speed. At infinite speed, light would traverse any distance in zero time. Similarly, according to the relativistic equations, an observer riding a light wave would see lengths contract to zero and clocks stop ticking as the universe approached him at the speed of light. Effectively, relativity replaces an infinite speed limit with the finite value of 3 × 108 metres per second.
    
    General relativity
    Roots of general relativity
    Because Isaac Newton’s law of gravity served so well in explaining the behaviour of the solar system, the question arises why it was necessary to develop a new theory of gravity. The answer is that Newton’s theory violates special relativity, for it requires an unspecified “action at a distance” through which any two objects—such as the Sun and Earth—instantaneously pull each other, no matter how far apart. However, instantaneous response would require the gravitational interaction to propagate at infinite speed, which is precluded by special relativity.
    
    In practice, this is no great problem for describing our solar system, for Newton’s law gives valid answers for objects moving slowly compared with light. Nevertheless, since Newton’s theory cannot be conceptually reconciled with special relativity, Einstein turned to the development of general relativity as a new way to understand gravitation.
    
    Principle of equivalence
    In order to begin building his theory, Einstein seized on an insight that came to him in 1907. As he explained in a lecture in 1922:
    
    I was sitting on a chair in my patent office in Bern. Suddenly a thought struck me: If a man falls freely, he would not feel his weight. I was taken aback. This simple thought experiment made a deep impression on me. This led me to the theory of gravity.
    
    Einstein was alluding to a curious fact known in Newton’s time: no matter what the mass of an object, it falls toward Earth with the same acceleration (ignoring air resistance) of 9.8 metres per second squared. Newton explained this by postulating two types of mass: inertial mass, which resists motion and enters into his general laws of motion, and gravitational mass, which enters into his equation for the force of gravity. He showed that, if the two masses were equal, then all objects would fall with that same gravitational acceleration.
    
    Hubble Space Telescope
    More From Britannica
    astronomy: Relativity
    Einstein, however, realized something more profound. A person standing in an elevator with a broken cable feels weightless as the enclosure falls freely toward Earth. The reason is that both he and the elevator accelerate downward at the same rate and so fall at exactly the same speed; hence, short of looking outside the elevator at his surroundings, he cannot determine that he is being pulled downward. In fact, there is no experiment he can do within a sealed falling elevator to determine that he is within a gravitational field. If he releases a ball from his hand, it will fall at the same rate, simply remaining where he releases it. And if he were to see the ball sink toward the floor, he could not tell if that was because he was at rest within a gravitational field that pulled the ball down or because a cable was yanking the elevator up so that its floor rose toward the ball.
    
    Einstein expressed these ideas in his deceptively simple principle of equivalence, which is the basis of general relativity: on a local scale—meaning within a given system, without looking at other systems—it is impossible to distinguish between physical effects due to gravity and those due to acceleration.
    
    In that case, continued Einstein’s Gedankenexperiment, light must be affected by gravity. Imagine that the elevator has a hole bored straight through two opposite walls. When the elevator is at rest, a beam of light entering one hole travels in a straight line parallel to the floor and exits through the other hole. But if the elevator is accelerated upward, by the time the ray reaches the second hole, the opening has moved and is no longer aligned with the ray. As the passenger sees the light miss the second hole, he concludes that the ray has followed a curved path (in fact, a parabola).
    
    If a light ray is bent in an accelerated system, then, according to the principle of equivalence, light should also be bent by gravity, contradicting the everyday expectation that light will travel in a straight line (unless it passes from one medium to another). If its path is curved by gravity, that must mean that “straight line” has a different meaning near a massive gravitational body such as a star than it does in empty space. This was a hint that gravity should be treated as a geometric phenomenon.
    
    Curved space-time and geometric gravitation
    The singular feature of Einstein’s view of gravity is its geometric nature. (See also geometry: The real world.) Whereas Newton thought that gravity was a force, Einstein showed that gravity arises from the shape of space-time. While this is difficult to visualize, there is an analogy that provides some insight—although it is only a guide, not a definitive statement of the theory.
    
    The analogy begins by considering space-time as a rubber sheet that can be deformed. In any region distant from massive cosmic objects such as stars, space-time is uncurved—that is, the rubber sheet is absolutely flat. If one were to probe space-time in that region by sending out a ray of light or a test body, both the ray and the body would travel in perfectly straight lines, like a child’s marble rolling across the rubber sheet.
    
    curved space-time
    curved space-time The four dimensional space-time continuum itself is distorted in the vicinity of any mass, with the amount of distortion depending on the mass and the distance from the mass. Thus, relativity accounts for Newton's inverse square law of gravity through geometry and thereby does away with the need for any mysterious “action at a distance.”
    However, the presence of a massive body curves space-time, as if a bowling ball were placed on the rubber sheet to create a cuplike depression. In the analogy, a marble placed near the depression rolls down the slope toward the bowling ball as if pulled by a force. In addition, if the marble is given a sideways push, it will describe an orbit around the bowling ball, as if a steady pull toward the ball is swinging the marble into a closed path.
    
    In this way, the curvature of space-time near a star defines the shortest natural paths, or geodesics—much as the shortest path between any two points on Earth is not a straight line, which cannot be constructed on that curved surface, but the arc of a great circle route. In Einstein’s theory, space-time geodesics define the deflection of light and the orbits of planets. As the American theoretical physicist John Wheeler put it, matter tells space-time how to curve, and space-time tells matter how to move.
    
    The mathematics of general relativity
    The rubber sheet analogy helps with visualization of space-time, but Einstein himself developed a complete quantitative theory that describes space-time through highly abstract mathematics. General relativity is expressed in a set of interlinked differential equations that define how the shape of space-time depends on the amount of matter (or, equivalently, energy) in the region. The solution of these so-called field equations can yield answers to different physical situations, including the behaviour of individual bodies and of the entire universe.
    
    Cosmological solutions
    Einstein immediately understood that the field equations could describe the entire cosmos. In 1917 he modified the original version of his equations by adding what he called the “cosmological term.” This represented a force that acted to make the universe expand, thus counteracting gravity, which tends to make the universe contract. The result was a static universe, in accordance with the best knowledge of the time.
    
    In 1922, however, the Soviet mathematician Aleksandr Aleksandrovich Friedmann showed that the field equations predict a dynamic universe, which can either expand forever or go through cycles of alternating expansion and contraction. Einstein came to agree with this result and abandoned his cosmological term. Later work, notably pioneering measurements by the American astronomer Edwin Hubble and the development of the big-bang model, has confirmed and amplified the concept of an expanding universe.
    
    Black holes
    In 1916 the German astronomer Karl Schwarzschild used the field equations to calculate the gravitational effect of a single spherical body such as a star. If the mass is neither very large nor highly concentrated, the resulting calculation will be the same as that given by Newton’s theory of gravity. Thus, Newton’s theory is not incorrect; rather, it constitutes a valid approximation to general relativity under certain conditions.
    
    Schwarzschild also described a new effect. If the mass is concentrated in a vanishingly small volume—a singularity—gravity will become so strong that nothing pulled into the surrounding region can ever leave. Even light cannot escape. In the rubber sheet analogy, it as if a tiny massive object creates a depression so steep that nothing can escape it. In recognition that this severe space-time distortion would be invisible—because it would absorb light and never emit any—it was dubbed a black hole.
    
    In quantitative terms, Schwarzschild’s result defines a sphere that is centred at the singularity and whose radius depends on the density of the enclosed mass. Events within the sphere are forever isolated from the remainder of the universe; for this reason, the Schwarzschild radius is called the event horizon.
    
    Black holes and wormholes
    No human technology could compact matter sufficiently to make black holes, but they occur as final steps in the life cycle of stars. After millions or billions of years, a star uses up all of its hydrogen and other elements that produce energy through nuclear fusion. With its nuclear furnace banked, the star no longer maintains an internal pressure to expand, and gravity is left unopposed to pull inward and compress the star. For stars above a certain mass, this gravitational collapse will produce a black hole containing several times the mass of the Sun. In other cases, the gravitational collapse of huge dust clouds can create supermassive black holes containing millions or billions of solar masses.
    
    black hole in M87
    black hole in M87Black hole at the center of the massive galaxy M87, about 55 million light-years from Earth, as imaged by the Event Horizon Telescope (EHT). The black hole is 6.5 billion times more massive than the Sun. This picture was the first direct visual evidence of a supermassive black hole and its shadow. The ring is brighter on one side because the black hole is rotating, and thus material on the side of the black hole turning toward Earth has its emission boosted by the Doppler effect. The shadow of the black hole is about five and a half times larger than the event horizon, the boundary marking the black hole's limits, where the escape velocity is equal to the speed of light. Created from data collected in 2017, this picture was released in 2019.
    Astrophysicists have found many cosmic objects that contain such a dense concentration of mass in a small volume. These black holes include one at the centre of the Milky Way Galaxy (Sagittarius A*) and certain binary stars that emit X-rays as they orbit each other. One, at the centre of the galaxy M87, has even been directly imaged.
    
    The theory of black holes has led to another predicted entity, a wormhole. This is a solution of the field equations that resembles a tunnel between two black holes or other points in space-time. Such a tunnel would provide a shortcut between its end points. In analogy, consider an ant walking across a flat sheet of paper from point A to point B. If the paper is curved through the third dimension, so that A and B overlap, the ant can step directly from one point to the other, thus avoiding a long trek.
    
    The possibility of short-circuiting the enormous distances between stars makes wormholes attractive for space travel. Because the tunnel links moments in time as well as locations in space, it also has been argued that a wormhole would allow travel into the past. However, wormholes are intrinsically unstable. While exotic stabilization schemes have been proposed, there is as yet no evidence that these can work or indeed that wormholes exist.
    
    Experimental evidence for general relativity
    experimental evidence for general relativity
    experimental evidence for general relativity In 1919 observation of a solar eclipse confirmed Einstein's prediction that light is bent in the presence of mass. This experimental support for his general theory of relativity garnered him instant worldwide acclaim.
    Soon after the theory of general relativity was published in 1915, the English astronomer Arthur Eddington considered Einstein’s prediction that light rays are bent near a massive body, and he realized that it could be verified by carefully comparing star positions in images of the Sun taken during a solar eclipse with images of the same region of space taken when the Sun was in a different portion of the sky. Verification was delayed by World War I, but in 1919 an excellent opportunity presented itself with an especially long total solar eclipse, in the vicinity of the bright Hyades star cluster, that was visible from northern Brazil to the African coast. Eddington led one expedition to Príncipe, an island off the African coast, and Andrew Crommelin of the Royal Greenwich Observatory led a second expedition to Sobral, Brazil. After carefully comparing photographs from both expeditions with reference photographs of the Hyades, Eddington declared that the starlight had been deflected about 1.75 seconds of arc, as predicted by general relativity. (The same effect produces gravitational lensing, where a massive cosmic object focuses light from another object beyond it to produce a distorted or magnified image. The astronomical discovery of gravitational lenses in 1979 gave additional support for general relativity.)
    
    Further evidence came from the planet Mercury. In the 19th century, it was found that Mercury does not return to exactly the same spot every time it completes its elliptical orbit. Instead, the ellipse rotates slowly in space, so that on each orbit the perihelion—the point of closest approach to the Sun—moves to a slightly different angle. Newton’s law of gravity could not explain this perihelion shift, but general relativity gave the correct orbit.
    
    Another confirmed prediction of general relativity is that time dilates in a gravitational field, meaning that clocks run slower as they approach the mass that is producing the field. This has been measured directly and also through the gravitational redshift of light. Time dilation causes light to vibrate at a lower frequency within a gravitational field; thus, the light is shifted toward a longer wavelength—that is, toward the red. Other measurements have verified the equivalence principle by showing that inertial and gravitational mass are precisely the same.
    
    The most striking prediction of general relativity is that of gravitational waves. Electromagnetic waves are caused by accelerated electrical charges and are detected when they put other charges into motion. Similarly, gravitational waves would be caused by masses in motion and are detected when they initiate motion in other masses. However, gravity is very weak compared with electromagnetism. Only a huge cosmic event, such as the collision of two stars, can generate detectable gravitational waves. Efforts to sense gravitational waves began in the 1960s, and such waves were first detected in 2015 when LIGO observed two black holes 1.3 million light-years away spiralling into each other.
    
    Applications of relativistic ideas
    Although relativistic effects are negligible in ordinary life, relativistic ideas appear in a range of areas from fundamental science to civilian and military technology.
    
    Elementary particles
    The relationship E = mc2 is essential in the study of subatomic particles. It determines the energy required to create particles or to convert one type into another and the energy released when a particle is annihilated. For example, two photons, each of energy E, can collide to form two particles, each with mass m = E/c2. This pair-production process is one step in the early evolution of the universe, as described in the big-bang model.
    
    Particle accelerators
    Knowledge of elementary particles comes primarily from particle accelerators. These machines raise subatomic particles, usually electrons or protons, to nearly the speed of light. When these energetic bullets smash into selected targets, they elucidate how subatomic particles interact and often produce new species of elementary particles.
    
    Particle accelerators could not be properly designed without special relativity. In the type called an electron synchrotron, for instance, electrons gain energy as they traverse a huge circular raceway. At barely below the speed of light, their mass is thousands of times larger than their rest mass. As a result, the magnetic field used to hold the electrons in circular orbits must be thousands of times stronger than if the mass did not change.
    
    Fission and fusion: bombs and stellar processes
    Energy is released in two kinds of nuclear processes. In nuclear fission a heavy nucleus, such as uranium, splits into two lighter nuclei; in nuclear fusion two light nuclei combine into a heavier one. In each process the total final mass is less than the starting mass. The difference appears as energy according to the relation E = Δmc2, where Δm is the mass deficit.
    
    Fission is used in atomic bombs and in reactors that produce power for civilian and military applications. The fusion of hydrogen into helium is the energy source in stars and provides the power of a hydrogen bomb. Efforts are now under way to develop controllable hydrogen fusion as a clean, abundant power source.
    
    The global positioning system
    How are gravitational waves applied in science and in everyday life?
    How are gravitational waves applied in science and in everyday life?Learn about the significance of gravitational waves in science and in everyday life.
    See all videos for this article
    The global positioning system (GPS) depends on relativistic principles. A GPS receiver determines its location on Earth’s surface by processing radio signals from four or more satellites. The distance to each satellite is calculated as the product of the speed of light and the time lag between transmission and reception of the signal. However, Earth’s gravitational field and the motion of the satellites cause time-dilation effects, and Earth’s rotation also has relativistic implications. Hence, GPS technology includes relativistic corrections that enable positions to be calculated to within several centimetres.
    
    Cosmology
    Cosmology, the study of the structure and origin of the universe, is intimately connected with gravity, which determines the macroscopic behaviour of all matter. General relativity has played a role in cosmology since the early calculations of Einstein and Friedmann. Since then, the theory has provided a framework for accommodating observational results, such as Hubble’s discovery of the expanding universe in 1929, as well as the big-bang model, which is the generally accepted explanation of the origin of the universe.
    
    The latest solutions of Einstein’s field equations depend on specific parameters that characterize the fate and shape of the universe. One is Hubble’s constant, which defines how rapidly the universe is expanding; the other is the density of matter in the universe, which determines the strength of gravity. Below a certain critical density, gravity would be weak enough that the universe would expand forever, so that space would be unlimited. Above that value, gravity would be strong enough to make the universe shrink back to its original minute size after a finite period of expansion, a process called the “big crunch.” In this case, space would be limited or bounded like the surface of a sphere. Current efforts in observational cosmology focus on measuring the most accurate possible values of Hubble’s constant and of critical density.
    
    Relativity, quantum theory, and unified theories
    Cosmic behaviour on the biggest scale is described by general relativity. Behaviour on the subatomic scale is described by quantum mechanics, which began with the work of the German physicist Max Planck in 1900 and treats energy and other physical quantities in discrete units called quanta. A central goal of physics has been to combine relativity theory and quantum theory into an overarching “theory of everything” describing all physical phenomena. Quantum theory explains electromagnetism and the strong and weak forces, but a quantum description of the remaining fundamental force of gravity has not been achieved.
    
    After Einstein developed relativity, he unsuccessfully sought a so-called unified field theory with a space-time geometry that would encompass all the fundamental forces. Other theorists have attempted to merge general relativity with quantum theory, but the two approaches treat forces in fundamentally different ways. In quantum theory, forces arise from the interchange of certain elementary particles, not from the shape of space-time. Furthermore, quantum effects are thought to cause a serious distortion of space-time at an extremely small scale called the Planck length, which is much smaller than the size of elementary particles. This suggests that quantum gravity cannot be understood without treating space-time at unheard-of scales.
    
    Although the connection between general relativity and quantum mechanics remains elusive, some progress has been made toward a fully unified theory. In the 1960s, the electroweak theory provided partial unification, showing a common basis for electromagnetism and the weak force within quantum theory. Recent research suggests that superstring theory, in which elementary particles are represented not as mathematical points but as extremely small strings vibrating in 10 or more dimensions, shows promise for supporting complete unification, including gravitation. However, until confirmed by experimental results, superstring theory will remain an untested hypothesis.
    
    Intellectual and cultural impact of relativity
    Reactions in general culture
    The impact of relativity has not been limited to science. Special relativity arrived on the scene at the beginning of the 20th century, and general relativity became widely known after World War I—eras when a new sensibility of “modernism” was becoming defined in art and literature. In addition, the confirmation of general relativity provided by the solar eclipse of 1919 received wide publicity. Einstein’s 1921 Nobel Prize for Physics (awarded for his work on the photon nature of light), as well as the popular perception that relativity was so complex that few could grasp it, quickly turned Einstein and his theories into cultural icons.
    
    The ideas of relativity were widely applied—and misapplied—soon after their advent. Some thinkers interpreted the theory as meaning simply that all things are relative, and they employed this concept in arenas distant from physics. The Spanish humanist philosopher and essayist José Ortega y Gasset, for instance, wrote in The Modern Theme (1923),
    
    The theory of Einstein is a marvelous proof of the harmonious multiplicity of all possible points of view. If the idea is extended to morals and aesthetics, we shall come to experience history and life in a new way.
    
    The revolutionary aspect of Einstein’s thought was also seized upon, as by the American art critic Thomas Craven, who in 1921 compared the break between classical and modern art to the break between Newtonian and Einsteinian ideas about space and time.
    
    Know the different kinds of dimensions and how they are different from one another
    Know the different kinds of dimensions and how they are different from one anotherLearn how to rethink the way dimensions are labeled and distinguished from one another.
    See all videos for this article
    Some saw specific relations between relativity and art arising from the idea of a four-dimensional space-time continuum. In the 19th century, developments in geometry led to popular interest in a fourth spatial dimension, imagined as somehow lying at right angles to all three of the ordinary dimensions of length, width, and height. Edwin Abbott’s Flatland (1884) was the first popular presentation of these ideas. Other works of fantasy that followed spoke of the fourth dimension as an arena apart from ordinary existence.
    
    Einstein’s four-dimensional universe, with three spatial dimensions and one of time, is conceptually different from four spatial dimensions. But the two kinds of four-dimensional world became conflated in interpreting the new art of the 20th century. Early Cubist works by Pablo Picasso that simultaneously portrayed all sides of their subjects became connected with the idea of higher dimensions in space, which some writers attempted to relate to relativity. In 1949, for example, the art historian Paul LaPorte wrote that “the new pictorial idiom created by [C]ubism is most satisfactorily explained by applying to it the concept of the space-time continuum.” Einstein specifically rejected this view, saying, “This new artistic ‘language’ has nothing in common with the Theory of Relativity.” Nevertheless, some artists explicitly explored Einstein’s ideas. In the new Soviet Union of the 1920s, for example, the poet and illustrator Vladimir Mayakovsky, a founder of the artistic movement called Russian Futurism, or Suprematism, hired an expert to explain relativity to him.
    
    The widespread general interest in relativity was reflected in the number of books written to elucidate the subject for nonexperts. Einstein’s popular exposition of special and general relativity appeared almost immediately, in 1916, and his article on space-time appeared in the 13th edition of Encyclopædia Britannica in 1926. Other scientists, such as the Russian mathematician Aleksandr Friedmann and the British astronomer Arthur Eddington, wrote popular books on the subjects in the 1920s. Such books continued to appear decades later.
    
    When relativity was first announced, the public was typically awestruck by its complexity, a justified response to the intricate mathematics of general relativity. But the abstract, nonvisceral nature of the theory also generated reactions against its apparent violation of common sense. These reactions included a political undertone; in some quarters, it was considered undemocratic to present or support a theory that could not be immediately understood by the common person.
    
    In contemporary usage, general culture has accepted the ideas of relativity—the impossibility of faster-than-light travel, E = mc2, time dilation and the twin paradox, the expanding universe, and black holes and wormholes—to the point where they are immediately recognized in the media and provide plot devices for works of science fiction. Some of these ideas have gained meaning beyond their strictly scientific ones; in the business world, for instance, “black hole” can mean an unrecoverable financial drain.
    
    Philosophical considerations
    In 1925 the British philosopher Bertrand Russell, in his ABC of Relativity, suggested that Einstein’s work would lead to new philosophical concepts. Relativity has indeed had a great effect on philosophy, illuminating some issues that go back to the ancient Greeks. The idea of the ether, invoked in the late 19th century to carry light waves, harks back to Aristotle. He divided the world into earth, air, fire, and water, with the ether (aether) as the fifth element representing the pure celestial sphere. The Michelson-Morley experiment and relativity eliminated the last vestiges of this idea.
    
    Relativity also changed the meaning of geometry as it was developed in Euclid’s Elements (c. 300 bce). Euclid’s system relied on the axiom “a straight line is the shortest distance between two points,” among others that seemed self-evidently true. Straight lines also played a special role in Euclid’s Optics as the paths followed by light rays. To philosophers such as the German Immanuel Kant, Euclid’s straight-line axiom represented a deep level of truth. But general relativity makes it possible scientifically to examine space like any other physical quantity—that is, to investigate Euclid’s premises. It is now known that space-time is curved near stars; no straight lines exist there, and light follows curved geodesics. Like Newton’s law of gravity, Euclid’s geometry correctly describes reality under certain conditions, but its axioms are not absolutely fundamental and universal, for the cosmos includes non-Euclidean geometries as well.
    
    Considering its scientific breadth, its recasting of people’s view of reality, its ability to describe the entire universe, and its influence outside science, Einstein’s relativity stands among the most significant and influential of scientific theories.</p>







<p>cosmology, field of study that brings together the natural sciences, particularly astronomy and physics, in a joint effort to understand the physical universe as a unified whole. The “observable universe” is the region of space that humans can actually or theoretically observe with the aid of technology. It can be thought of as a bubble with Earth at its centre. It is differentiated from the entirety of the universe, which is the whole cosmic system of matter and energy, including the human race. Unlike the observable universe, the  universe is possibly infinite and without spatial edges.

    If one looks up on a clear night, one will see that the sky is full of stars. During the summer months in the Northern Hemisphere, a faint band of light stretches from horizon to horizon, a swath of pale white cutting across a background of deepest black. For the early Egyptians, this was the heavenly Nile, flowing through the land of the dead ruled by Osiris. The ancient Greeks likened it to a river of milk. Astronomers now know that the band is actually composed of countless stars in a flattened disk seen edge on. The stars are so close to one another along the line of sight that the unaided eye has difficulty discerning the individual members. Through a large telescope, astronomers find myriads of like systems sprinkled throughout the depths of space. They call such vast collections of stars galaxies, after the Greek word for milk, and call the local galaxy to which the Sun belongs the Milky Way Galaxy or simply the Galaxy.
    
    The Sun is a star around which Earth and the other planets revolve, and by extension every visible star in the sky is a sun in its own right. Some stars are intrinsically brighter than the Sun; others, fainter. Much less light is received from the stars than from the Sun because the stars are all much farther away. Indeed, they appear densely packed in the Milky Way only because there are so many of them. The actual separations of the stars are enormous, so large that it is conventional to measure their distances in units of how far light can travel in a given amount of time. The speed of light (in a vacuum) equals 3 × 1010 cm/sec (centimetres per second); at such a speed, it is possible to circle the Earth seven times in a single second. Thus in terrestrial terms the Sun, which lies 500 light-seconds from the Earth, is very far away; however, even the next closest star, Proxima Centauri, at a distance of 4.3 light-years (4.1 × 1018 cm), is 270,000 times farther yet. The stars that lie on the opposite side of the Milky Way from the Sun have distances that are on the order of 100,000 light-years, which is the typical diameter of a large spiral galaxy.
    
    Andromeda Galaxy
    Andromeda GalaxyThe Andromeda Galaxy, also known as the Andromeda Nebula or M31. It is the closest spiral galaxy to Earth, at a distance of 2.48 million light-years.
    If the kingdom of the stars seems vast, the realm of the galaxies is larger still. The nearest galaxies to the Milky Way system are the Large and Small Magellanic Clouds, two irregular satellites of the Galaxy visible to the naked eye in the Southern Hemisphere. The Magellanic Clouds are relatively small (containing roughly 109 stars) compared to the Galaxy (with some 1011 stars), and they lie at a distance of about 200,000 light-years. The nearest large galaxy comparable to the Galaxy is the Andromeda Galaxy (also called M31 because it was the 31st entry in a catalog of astronomical objects compiled by the French astronomer Charles Messier in 1781), and it lies at a distance of about 2,000,000 light-years. The Magellanic Clouds, the Andromeda Galaxy, and the Milky Way system all are part of an aggregation of two dozen or so neighbouring galaxies known as the Local Group. The Galaxy and M31 are the largest members of this group.
    
    The Galaxy and M31 are both spiral galaxies, and they are among the brighter and more massive of all spiral galaxies. The most luminous and brightest galaxies, however, are not spirals but rather supergiant ellipticals (also called cD galaxies by astronomers for historical reasons that are not particularly illuminating). Elliptical galaxies have roundish shapes rather than the flattened distributions that characterize spiral galaxies, and they tend to occur in rich clusters (those containing thousands of members) rather than in the loose groups favoured by spirals. The brightest member galaxies of rich clusters have been detected at distances exceeding several thousand million light-years from the Earth. The branch of learning that deals with phenomena at the scale of many millions of light-years is called cosmology—a term derived from combining two Greek words, kosmos, meaning “order,” “harmony,” and “the world,” and logos, signifying “word” or “discourse.” Cosmology is, in effect, the study of the universe at large.
    
    Horologist Roman Piekarski starts the time consuming task of adjusting the 600 antique clocks at Cuckooland Museum in readiness for this weekends change to British summer time on March 23, 2009 in Knutsford, England.
    Britannica Quiz
    Ologies Quiz
    The cosmological expansion
    big bang model
    big bang modelAccording to the evolutionary, or big bang, theory of the universe, the universe is expanding while the total energy and matter it contains remain constant. Therefore, as the universe expands, the density of its energy and matter must become progressively thinner. At left is a two-dimensional representation of the universe as it appears now, with galaxies occupying a typical section of space. At right, billions of years later the same amount of matter will fill a larger volume of space.
    When the universe is viewed in the large, a dramatic new feature, not present on small scales, emerges—namely, the cosmological expansion. On cosmological scales, galaxies (or, at least, clusters of galaxies) appear to be racing away from one another with the apparent velocity of recession being linearly proportional to the distance of the object. This relation is known as the Hubble law (after its discoverer, the American astronomer Edwin Powell Hubble). Interpreted in the simplest fashion, the Hubble law implies that 13.8 billion years ago all of the matter in the universe was closely packed together in an incredibly dense state and that everything then exploded in a “big bang,” the signature of the explosion being written eventually in the galaxies of stars that formed out of the expanding debris of matter. Strong scientific support for this interpretation of a big bang origin of the universe comes from the detection by radio telescopes of a steady and uniform background of microwave radiation. The cosmic microwave background is believed to be a ghostly remnant of the fierce light of the primeval fireball reduced by cosmic expansion to a shadow of its former splendour but still pervading every corner of the known universe.
    
    The simple (and most common) interpretation of the Hubble law as a recession of the galaxies over time through space, however, contains a misleading notion. In a sense, as will be made more precise later in the article, the expansion of the universe represents not so much a fundamental motion of galaxies within a framework of absolute time and absolute space, but an expansion of time and space themselves. On cosmological scales, the use of light-travel times to measure distances assumes a special significance because the lengths become so vast that even light, traveling at the fastest speed attainable by any physical entity, takes a significant fraction of the age of the universe (13.8 billion years old) to travel from an object to an observer. Thus, when astronomers measure objects at cosmological distances from the Local Group, they are seeing the objects as they existed during a time when the universe was much younger than it is today. Under these circumstances, Albert Einstein taught in his theory of general relativity that the gravitational field of everything in the universe so warps space and time as to require a very careful reevaluation of quantities whose seemingly elementary natures are normally taken for granted.</p>

<p>The nature of space and time
    Finite or infinite?
    An issue that arises when one contemplates the universe at large is whether space and time are infinite or finite. After many centuries of thought by some of the best minds, humanity has still not arrived at conclusive answers to these questions. Aristotle’s answer was that the material universe must be spatially finite, for if stars extended to infinity, they could not perform a complete rotation around Earth in 24 hours. Space must then itself also be finite because it is merely a receptacle for material bodies. On the other hand, the heavens must be temporally infinite, without beginning or end, since they are imperishable and cannot be created or destroyed.
    
    Except for the infinity of time, these views came to be accepted religious teachings in Europe before the period of modern science. The most notable person to publicly express doubts about restricted space was the Italian philosopher-mathematician Giordano Bruno, who asked the obvious question that, if there is a boundary or edge to space, what is on the other side? For his advocacy of an infinity of suns and earths, he was burned at the stake in 1600.
    
    In 1610 the German astronomer Johannes Kepler provided a profound reason for believing that the number of stars in the universe had to be finite. If there were an infinity of stars, he argued, then the sky would be completely filled with them and night would not be dark! This point was rediscussed by the astronomers Edmond Halley of England and Jean-Philippe-Loys de Chéseaux of Switzerland in the 18th century, but it was not popularized as a paradox until Wilhelm Olbers of Germany took up the problem in the 19th century. The difficulty became potentially very real with American astronomer Edwin Hubble’s measurement of the enormous extent of the universe of galaxies with its large-scale homogeneity and isotropy. His discovery of the systematic recession of the galaxies provided an escape, however. At first people thought that the redshift effect alone would suffice to explain why the sky is dark at night—namely, that the light from the stars in distant galaxies would be redshifted to long wavelengths beyond the visible regime. The modern consensus is, however, that a finite age for the universe is a far more important effect. Even if the universe is spatially infinite, photons from very distant galaxies simply do not have the time to travel to Earth because of the finite speed of light. There is a spherical surface, the cosmic event horizon (13.8 billion light-years in radial distance from Earth at the current epoch), beyond which nothing can be seen even in principle; and the number (roughly 1010) of galaxies within this cosmic horizon, the observable universe, are too few to make the night sky bright.
    
    When one looks to great distances, one is seeing things as they were a long time ago, again because light takes a finite time to travel to Earth. Over such great spans, do the classical notions of Euclid concerning the properties of space necessarily continue to hold? The answer given by Einstein was: No, the gravitation of the mass contained in cosmologically large regions may warp one’s usual perceptions of space and time; in particular, the Euclidean postulate that parallel lines never cross need not be a correct description of the geometry of the actual universe. And in 1917 Einstein presented a mathematical model of the universe in which the total volume of space was finite yet had no boundary or edge. The model was based on his theory of general relativity that utilized a more generalized approach to geometry devised in the 19th century by the German mathematician Bernhard Riemann.
    
    Gravitation and the geometry of space-time
    The physical foundation of Einstein’s view of gravitation, general relativity, lies on two empirical findings that he elevated to the status of basic postulates. The first postulate is the relativity principle: local physics is governed by the theory of special relativity. The second postulate is the equivalence principle: there is no way for an observer to distinguish locally between gravity and acceleration. The motivation for the second postulate comes from Galileo’s observation that all objects—independent of mass, shape, colour, or any other property—accelerate at the same rate in a (uniform) gravitational field.
    
    Horologist Roman Piekarski starts the time consuming task of adjusting the 600 antique clocks at Cuckooland Museum in readiness for this weekends change to British summer time on March 23, 2009 in Knutsford, England.
    Britannica Quiz
    Ologies Quiz
    Einstein’s theory of special relativity, which he developed in 1905, had as its basic premises (1) the notion (also dating back to Galileo) that the laws of physics are the same for all inertial observers and (2) the constancy of the speed of light in a vacuum—namely, that the speed of light has the same value (3 × 1010 centimetres per second [cm/sec], or 2 × 105 miles per second [miles/sec]) for all inertial observers independent of their motion relative to the source of the light. Clearly, this second premise is incompatible with Euclidean and Newtonian precepts of absolute space and absolute time, resulting in a program that merged space and time into a single structure, with well-known consequences. The space-time structure of special relativity is often called “flat” because, among other things, the propagation of photons is easily represented on a flat sheet of graph paper with equal-sized squares. Let each tick on the vertical axis represent one light-year (9.46 × 1017 cm [5.88 × 1012 miles]) of distance in the direction of the flight of the photon, and each tick on the horizontal axis represent the passage of one year (3.16 × 107 seconds) of time. The propagation path of the photon is then a 45° line because it flies one light-year in one year (with respect to the space and time measurements of all inertial observers no matter how fast they move relative to the photon).
    
    curved space-time
    curved space-time The four dimensional space-time continuum itself is distorted in the vicinity of any mass, with the amount of distortion depending on the mass and the distance from the mass. Thus, relativity accounts for Newton's inverse square law of gravity through geometry and thereby does away with the need for any mysterious “action at a distance.”
    The principle of equivalence in general relativity allows the locally flat space-time structure of special relativity to be warped by gravitation, so that (in the cosmological case) the propagation of the photon over thousands of millions of light-years can no longer be plotted on a globally flat sheet of paper. To be sure, the curvature of the paper may not be apparent when only a small piece is examined, thereby giving the local impression that space-time is flat (i.e., satisfies special relativity). It is only when the graph paper is examined globally that one realizes it is curved (i.e., satisfies general relativity).
    
    In Einstein’s 1917 model of the universe, the curvature occurs only in space, with the graph paper being rolled up into a cylinder on its side, a loop around the cylinder at constant time having a circumference of 2πR—the total spatial extent of the universe. Notice that the “radius of the universe” is measured in a “direction” perpendicular to the space-time surface of the graph paper. Since the ringed space axis corresponds to one of three dimensions of the actual world (any will do since all directions are equivalent in an isotropic model), the radius of the universe exists in a fourth spatial dimension (not time) which is not part of the real world. This fourth spatial dimension is a mathematical artifice introduced to represent diagrammatically the solution (in this case) of equations for curved three-dimensional space that need not refer to any dimensions other than the three physical ones. Photons traveling in a straight line in any physical direction have trajectories that go diagonally (at 45° angles to the space and time axes) from corner to corner of each little square cell of the space-time grid; thus, they describe helical paths on the cylindrical surface of the graph paper, making one turn after traveling a spatial distance 2πR. In other words, always flying dead ahead, photons would return to where they started from after going a finite distance without ever coming to an edge or boundary. The distance to the “other side” of the universe is therefore πR, and it would lie in any and every direction; space would be closed on itself.
    
    Now, except by analogy with the closed two-dimensional surface of a sphere that is uniformly curved toward a centre in a third dimension lying nowhere on the two-dimensional surface, no three-dimensional creature can visualize a closed three-dimensional volume that is uniformly curved toward a centre in a fourth dimension lying nowhere in the three-dimensional volume. Nevertheless, three-dimensional creatures could discover the curvature of their three-dimensional world by performing surveying experiments of sufficient spatial scope. They could draw circles, for example, by tacking down one end of a string and tracing along a single plane the locus described by the other end when the string is always kept taut in between (a straight line) and walked around by a surveyor. In Einstein’s universe, if the string were short compared to the quantity R, the circumference of the circle divided by the length of the string (the circle’s radius) would nearly equal 2π = 6.2837853…, thereby fooling the three-dimensional creatures into thinking that Euclidean geometry gives a correct description of their world. However, the ratio of circumference to length of string would become less than 2π when the length of string became comparable to R. Indeed, if a string of length πR could be pulled taut to the antipode of a positively curved universe, the ratio would go to zero. In short, at the tacked-down end the string could be seen to sweep out a great arc in the sky from horizon to horizon and back again; yet, to make the string do this, the surveyor at the other end need only walk around a circle of vanishingly small circumference.
    
    To understand why gravitation can curve space (or more generally, space-time) in such startling ways, consider the following thought experiment that was originally conceived by Einstein. Imagine an elevator in free space accelerating upward, from the viewpoint of a woman in inertial space, at a rate numerically equal to g, the gravitational field at the surface of Earth. Let this elevator have parallel windows on two sides, and let the woman shine a brief pulse of light toward the windows. She will see the photons enter close to the top of the near window and exit near the bottom of the far window because the elevator has accelerated upward in the interval it takes light to travel across the elevator. For her, photons travel in a straight line, and it is merely the acceleration of the elevator that has caused the windows and floor of the elevator to curve up to the flight path of the photons.
    
    Let there now be a man standing inside the elevator. Because the floor of the elevator accelerates him upward at a rate g, he may—if he chooses to regard himself as stationary—think that he is standing still on the surface of Earth and is being pulled to the ground by its gravitational field g. Indeed, in accordance with the equivalence principle, without looking out the windows (the outside is not part of his local environment), he cannot perform any local experiment that would inform him otherwise. Let the woman shine her pulse of light. The man sees, just like the woman, that the photons enter near the top edge of one window and exit near the bottom of the other. And just like the woman, he knows that photons propagate in straight lines in free space. (By the relativity principle, they must agree on the laws of physics if they are both inertial observers.) However, since he actually sees the photons follow a curved path relative to himself, he concludes that they must be bent by the force of gravity. The woman tries to tell him there is no such force at work; he is not an inertial observer. Nonetheless, he has the solidity of Earth beneath him, so he insists on attributing his acceleration to the force of gravity. According to Einstein, they are both right. There is no need to distinguish locally between acceleration and gravity—the two are in some sense equivalent. But if that is the case, then it must be true that gravity—“real” gravity—can actually bend light. And indeed it can, as many experiments have shown since Einstein’s first discussion of the phenomenon.
    
    experimental evidence for general relativity
    experimental evidence for general relativity In 1919 observation of a solar eclipse confirmed Einstein's prediction that light is bent in the presence of mass. This experimental support for his general theory of relativity garnered him instant worldwide acclaim.
    It was the genius of Einstein to go even further. Rather than speak of the force of gravitation having bent the photons into a curved path, might it not be more fruitful to think of photons as always flying in straight lines—in the sense that a straight line is the shortest distance between two points—and that what really happens is that gravitation bends space-time? In other words, perhaps gravitation is curved space-time, and photons fly along the shortest paths possible in this curved space-time, thus giving the appearance of being bent by a “force” when one insists on thinking that space-time is flat. The utility of taking this approach is that it becomes automatic that all test bodies fall at the same rate under the “force” of gravitation, for they are merely producing their natural trajectories in a background space-time that is curved in a certain fashion independent of the test bodies. What was a minor miracle for Galileo and Newton becomes the most natural thing in the world for Einstein.
    
    To complete the program and to conform with Newton’s theory of gravitation in the limit of weak curvature (weak field), the source of space-time curvature would have to be ascribed to mass (and energy). The mathematical expression of these ideas constitutes Einstein’s theory of general relativity, one of the most beautiful artifacts of pure thought ever produced. The American physicist John Archibald Wheeler and his colleagues summarized Einstein’s view of the universe in these terms:
    
    Curved spacetime tells mass-energy how to move;
    
    mass-energy tells spacetime how to curve.
    
    Contrast this with Newton’s view of the mechanics of the heavens:
    
    Force tells mass how to accelerate;
    
    mass tells gravity how to exert force.
    
    Notice therefore that Einstein’s worldview is not merely a quantitative modification of Newton’s picture (which is also possible via an equivalent route using the methods of quantum field theory) but represents a qualitative change of perspective. And modern experiments have amply justified the fruitfulness of Einstein’s alternative interpretation of gravitation as geometry rather than as force. His theory would have undoubtedly delighted the Greeks.
    
    Relativistic cosmologies
    Einstein’s model
    To derive his 1917 cosmological model, Einstein made three assumptions that lay outside the scope of his equations. The first was to suppose that the universe is homogeneous and isotropic in the large (i.e., the same everywhere on average at any instant in time), an assumption that the English astrophysicist Edward A. Milne later elevated to an entire philosophical outlook by naming it the cosmological principle. Given the success of the Copernican revolution, this outlook is a natural one. Newton himself had it implicitly in mind when he took the initial state of the universe to be everywhere the same before it developed “ye Sun and Fixt stars.”
    
    The second assumption was to suppose that this homogeneous and isotropic universe had a closed spatial geometry. As described above, the total volume of a three-dimensional space with uniform positive curvature would be finite but possess no edges or boundaries (to be consistent with the first assumption).
    
    The third assumption made by Einstein was that the universe as a whole is static—i.e., its large-scale properties do not vary with time. This assumption, made before Hubble’s observational discovery of the expansion of the universe, was also natural; it was the simplest approach, as Aristotle had discovered, if one wishes to avoid a discussion of a creation event. Indeed, the philosophical attraction of the notion that the universe on average is not only homogeneous and isotropic in space but also constant in time was so appealing that a school of English cosmologists—Hermann Bondi, Fred Hoyle, and Thomas Gold—would call it the perfect cosmological principle and carry its implications in the 1950s to the ultimate refinement in the so-called steady-state theory.
    
    To his great chagrin Einstein found in 1917 that with his three adopted assumptions, his equations of general relativity—as originally written down—had no meaningful solutions. To obtain a solution, Einstein realized that he had to add to his equations an extra term, which came to be called the cosmological constant. If one speaks in Newtonian terms, the cosmological constant could be interpreted as a repulsive force of unknown origin that could exactly balance the attraction of gravitation of all the matter in Einstein’s closed universe and keep it from moving. The inclusion of such a term in a more general context, however, meant that the universe in the absence of any mass-energy (i.e., consisting of a vacuum) would not have a space-time structure that was flat (i.e., would not have satisfied the dictates of special relativity exactly). Einstein was prepared to make such a sacrifice only very reluctantly, and, when he later learned of Hubble’s discovery of the expansion of the universe and realized that he could have predicted it had he only had more faith in the original form of his equations, he regretted the introduction of the cosmological constant as the “biggest blunder” of his life. Ironically, observations of distant supernovas have shown the existence of dark energy, a repulsive force that is the dominant component of the universe.
    
    De Sitter’s model
    It was also in 1917 that the Dutch astronomer Willem de Sitter recognized that he could obtain a static cosmological model differing from Einstein’s simply by removing all matter. The solution remains stationary essentially because there is no matter to move about. If some test particles are reintroduced into the model, the cosmological term would propel them away from each other. Astronomers now began to wonder if this effect might not underlie the recession of the spiral galaxies.
    
    Galaxy clusters like Abell 2744 can act as a natural cosmic lens, magnifying light from more distant, background objects through gravity. NASA's James Webb Space Telescope may be able to detect light from the first stars in the universe if they are gravitationally lensed by such clusters. (astronomy, space exploration, galaxies)
    More From Britannica
    How Fast Is the Universe Expanding?
    Friedmann-Lemaître models
    intrinsic curvature of a surface
    intrinsic curvature of a surface
    In 1922 Aleksandr A. Friedmann, a Russian meteorologist and mathematician, and in 1927 Georges Lemaître, a Belgian cleric, independently discovered solutions to Einstein’s equations that contained realistic amounts of matter. These evolutionary models correspond to big bang cosmologies. Friedmann and Lemaître adopted Einstein’s assumption of spatial homogeneity and isotropy (the cosmological principle). They rejected, however, his assumption of time independence and considered both positively curved spaces (“closed” universes) as well as negatively curved spaces (“open” universes). The difference between the approaches of Friedmann and Lemaître is that the former set the cosmological constant equal to zero, whereas the latter retained the possibility that it might have a nonzero value. To simplify the discussion, only the Friedmann models are considered here.
    
    The decision to abandon a static model meant that the Friedmann models evolve with time. As such, neighbouring pieces of matter have recessional (or contractional) phases when they separate from (or approach) one another with an apparent velocity that increases linearly with increasing distance. Friedmann’s models thus anticipated Hubble’s law before it had been formulated on an observational basis. It was Lemaître, however, who had the good fortune of deriving the results at the time when the recession of the galaxies was being recognized as a fundamental cosmological observation, and it was he who clarified the theoretical basis for the phenomenon.
    
    The geometry of space in Friedmann’s closed models is similar to that of Einstein’s original model; however, there is a curvature to time as well as one to space. Unlike Einstein’s model, where time runs eternally at each spatial point on an uninterrupted horizontal line that extends infinitely into the past and future, there is a beginning and end to time in Friedmann’s version of a closed universe when material expands from or is recompressed to infinite densities. These instants are called the instants of the “big bang” and the “big squeeze,” respectively. The global space-time diagram for the middle half of the expansion-compression phases can be depicted as a barrel lying on its side. The space axis corresponds again to any one direction in the universe, and it wraps around the barrel. Through each spatial point runs a time axis that extends along the length of the barrel on its (space-time) surface. Because the barrel is curved in both space and time, the little squares in the grid of the curved sheet of graph paper marking the space-time surface are of nonuniform size, stretching to become bigger when the barrel broadens (universe expands) and shrinking to become smaller when the barrel narrows (universe contracts).
    
    It should be remembered that only the surface of the barrel has physical significance; the dimension off the surface toward the axle of the barrel represents the fourth spatial dimension, which is not part of the real three-dimensional world. The space axis circles the barrel and closes upon itself after traversing a circumference equal to 2πR, where R, the radius of the universe (in the fourth dimension), is now a function of the time t. In a closed Friedmann model, R starts equal to zero at time t = 0 (not shown in barrel diagram), expands to a maximum value at time t = tm (the middle of the barrel), and recontracts to zero (not shown) at time t = 2tm, with the value of tm dependent on the total amount of mass that exists in the universe.
    
    Imagine now that galaxies reside on equally spaced tick marks along the space axis. Each galaxy on average does not move spatially with respect to its tick mark in the spatial (ringed) direction but is carried forward horizontally by the march of time. The total number of galaxies on the spatial ring is conserved as time changes, and therefore their average spacing increases or decreases as the total circumference 2πR on the ring increases or decreases (during the expansion or contraction phases). Thus, without in a sense actually moving in the spatial direction, galaxies can be carried apart by the expansion of space itself. From this point of view, the recession of galaxies is not a “velocity” in the usual sense of the word. For example, in a closed Friedmann model, there could be galaxies that started, when R was small, very close to the Milky Way system on the opposite side of the universe. Now, 1010 years later, they are still on the opposite side of the universe but at a distance much greater than 1010 light-years away. They reached those distances without ever having had to move (relative to any local observer) at speeds faster than light—indeed, in a sense without having had to move at all. The separation rate of nearby galaxies can be thought of as a velocity without confusion in the sense of Hubble’s law, if one wants, but only if the inferred velocity is much less than the speed of light.
    
    On the other hand, if the recession of the galaxies is not viewed in terms of a velocity, then the cosmological redshift cannot be viewed as a Doppler shift. How, then, does it arise? The answer is contained in the barrel diagram when one notices that, as the universe expands, each small cell in the space-time grid also expands. Consider the propagation of electromagnetic radiation whose wavelength initially spans exactly one cell length (for simplicity of discussion), so that its head lies at a vertex and its tail at one vertex back. Suppose an elliptical galaxy emits such a wave at some time t1. The head of the wave propagates from corner to corner on the little square grids that look locally flat, and the tail propagates from corner to corner one vertex back. At a later time t2, a spiral galaxy begins to intercept the head of the wave. At time t2, the tail is still one vertex back, and therefore the wave train, still containing one wavelength, now spans one current spatial grid spacing. In other words, the wavelength has grown in direct proportion to the linear expansion factor of the universe. Since the same conclusion would have held if n wavelengths had been involved instead of one, all electromagnetic radiation from a given object will show the same cosmological redshift if the universe (or, equivalently, the average spacing between galaxies) was smaller at the epoch of transmission than at the epoch of reception. Each wavelength will have been stretched in direct proportion to the expansion of the universe in between.
    
    A nonzero peculiar velocity for an emitting galaxy with respect to its local cosmological frame can be taken into account by Doppler-shifting the emitted photons before applying the cosmological redshift factor; i.e., the observed redshift would be a product of two factors. When the observed redshift is large, one usually assumes that the dominant contribution is of cosmological origin. When this assumption is valid, the redshift is a monotonic function of both distance and time during the expansional phase of any cosmological model. Thus, astronomers often use the redshift z as a shorthand indicator of both distance and elapsed time. Following from this, the statement “object X lies at z = a” means that “object X lies at a distance associated with redshift a”; the statement “event Y occurred at redshift z = b” means that “event Y occurred a time ago associated with redshift b.”
    
    The open Friedmann models differ from the closed models in both spatial and temporal behaviour. In an open universe the total volume of space and the number of galaxies contained in it are infinite. The three-dimensional spatial geometry is one of uniform negative curvature in the sense that, if circles are drawn with very large lengths of string, the ratio of circumferences to lengths of string are greater than 2π. The temporal history begins again with expansion from a big bang of infinite density, but now the expansion continues indefinitely, and the average density of matter and radiation in the universe would eventually become vanishingly small. Time in such a model has a beginning but no end.
    
    The Einstein–de Sitter universe
    In 1932 Einstein and de Sitter proposed that the cosmological constant should be set equal to zero, and they derived a homogeneous and isotropic model that provides the separating case between the closed and open Friedmann models; i.e., Einstein and de Sitter assumed that the spatial curvature of the universe is neither positive nor negative but rather zero. The spatial geometry of the Einstein–de Sitter universe is Euclidean (infinite total volume), but space-time is not globally flat (i.e., not exactly the space-time of special relativity). Time again commences with a big bang and the galaxies recede forever, but the recession rate (Hubble’s “constant”) asymptotically coasts to zero as time advances to infinity. Because the geometry of space and the gross evolutionary properties are uniquely defined in the Einstein–de Sitter model, many people with a philosophical bent long considered it the most fitting candidate to describe the actual universe.
    
    Bound and unbound universes and the closure density
    relative size of the universe
    relative size of the universeHow the relative size of the universe changes with time in four different models. The red line shows a universe devoid of matter, with constant expansion. Pink shows a collapsing universe, with six times the critical density of matter. Green shows a model favoured until 1998, with exactly the critical density and a universe 100 percent matter. Blue shows the currently favoured scenario, with exactly the critical density, of which 27 percent is visible and dark matter and 73 percent is dark energy.
    The different separation behaviours of galaxies at large timescales in the Friedmann closed and open models and the Einstein–de Sitter model allow a different classification scheme than one based on the global structure of space-time. The alternative way of looking at things is in terms of gravitationally bound and unbound systems: closed models where galaxies initially separate but later come back together again represent bound universes; open models where galaxies continue to separate forever represent unbound universes; the Einstein–de Sitter model where galaxies separate forever but slow to a halt at infinite time represents the critical case.
    
    The advantage of this alternative view is that it focuses attention on local quantities where it is possible to think in the simpler terms of Newtonian physics—attractive forces, for example. In this picture it is intuitively clear that the feature that should distinguish whether or not gravity is capable of bringing a given expansion rate to a halt depends on the amount of mass (per unit volume) present. This is indeed the case; the Newtonian and relativistic formalisms give the same criterion for the critical, or closure, density (in mass equivalent of matter and radiation) that separates closed or bound universes from open or unbound ones. If Hubble’s constant at the present epoch is denoted as H0, then the closure density (corresponding to an Einstein–de Sitter model) equals 3H02/8πG, where G is the universal gravitational constant in both Newton’s and Einstein’s theories of gravity. The numerical value of Hubble’s constant H0 is 22 kilometres per second per million light-years; the closure density then equals 10−29 gram per cubic centimetre, the equivalent of about six hydrogen atoms on average per cubic metre of cosmic space. If the actual cosmic average is greater than this value, the universe is bound (closed) and, though currently expanding, will end in a crush of unimaginable proportion. If it is less, the universe is unbound (open) and will expand forever. The result is intuitively plausible since the smaller the mass density, the smaller the role for gravitation, so the more the universe will approach free expansion (assuming that the cosmological constant is zero).
    
    The mass in galaxies observed directly, when averaged over cosmological distances, is estimated to be only a few percent of the amount required to close the universe. The amount contained in the radiation field (most of which is in the cosmic microwave background) contributes negligibly to the total at present. If this were all, the universe would be open and unbound. However, the dark matter that has been deduced from various dynamic arguments is about 23 percent of the universe, and dark energy supplies the remaining amount, bringing the total average mass density up to 100 percent of the closure density.
    
    The hot big bang
    Wilkinson Microwave Anisotropy Probe
    Wilkinson Microwave Anisotropy ProbeA full-sky map produced by the Wilkinson Microwave Anisotropy Probe (WMAP) showing cosmic background radiation, a very uniform glow of microwaves emitted by the infant universe more than 13 billion years ago. Colour differences indicate tiny fluctuations in the intensity of the radiation, a result of tiny variations in the density of matter in the early universe. According to inflation theory, these irregularities were the “seeds” that became the galaxies. WMAP's data support the big bang and inflation models, and cosmic microwave background is at the farthest limits of the observable universe.
    Given the measured radiation temperature of 2.735 kelvins (K), the energy density of the cosmic microwave background can be shown to be about 1,000 times smaller than the average rest-energy density of ordinary matter in the universe. Thus, the current universe is matter-dominated. If one goes back in time to redshift z, the average number densities of particles and photons were both bigger by the same factor (1 + z)3 because the universe was more compressed by this factor, and the ratio of these two numbers would have maintained its current value of about one hydrogen nucleus, or proton, for every 109 photons. The wavelength of each photon, however, was shorter by the factor 1 + z in the past than it is now; therefore, the energy density of radiation increases faster by one factor of 1 + z than the rest-energy density of matter. Thus, the radiation energy density becomes comparable to the energy density of ordinary matter at a redshift of about 1,000. At redshifts larger than 10,000, radiation would have dominated even over the dark matter of the universe. Between these two values radiation would have decoupled from matter when hydrogen recombined. It is not possible to use photons to observe redshifts larger than about 1,090, because the cosmic plasma at temperatures above 4,000 K is essentially opaque before recombination. One can think of the spherical surface as an inverted “photosphere” of the observable universe. This spherical surface of last scattering probably has slight ripples in it that account for the slight anisotropies observed in the cosmic microwave background today. In any case, the earliest stages of the universe’s history—for example, when temperatures were 109 K and higher—cannot be examined by light received through any telescope. Clues must be sought by comparing the matter content with theoretical calculations.
    
    For this purpose, fortunately, the cosmological evolution of model universes is especially simple and amenable to computation at redshifts much larger than 10,000 (or temperatures substantially above 30,000 K) because the physical properties of the dominant component, photons, then are completely known. In a radiation-dominated early universe, for example, the radiation temperature T is very precisely known as a function of the age of the universe, the time t after the big bang.
    
    Primordial nucleosynthesis
    evolution of the universe
    evolution of the universeImmediately after the big bang (1), the universe was filled with a dense “soup” of subatomic particles (2), called quarks and leptons (such as electrons), and their antiparticle equivalents. By 0.01 second after the big bang (3), some of the quarks had united to form neutrons and protons. (After another 2 seconds, the only leptons remaining were electrons; the antiparticles had been annihilated.) After 3.5 minutes (4), hydrogen and helium nuclei had formed. After a million years (5), the universe was populated with hydrogen and helium atoms, the raw material of stars and galaxies. The initial radiation from the big bang had grown less energetic.
    According to the considerations outlined above, at a time t less than 10-4 seconds, the creation of matter-antimatter pairs would have been in thermodynamic equilibrium with the ambient radiation field at a temperature T of about 1012 K. Nevertheless, there was a slight excess of matter particles (e.g., protons) compared to antimatter particles (e.g., antiprotons) of roughly a few parts in 109. This is known because, as the universe aged and expanded, the radiation temperature would have dropped and each antiproton and each antineutron would have annihilated with a proton and a neutron to yield two gamma rays; and later each antielectron would have done the same with an electron to give two more gamma rays. After annihilation, however, the ratio of the number of remaining protons to photons would be conserved in the subsequent expansion to the present day. Since that ratio is known to be one part in 109, it is easy to work out that the original matter-antimatter asymmetry must have been a few parts per 109.
    
    In any case, after proton-antiproton and neutron-antineutron annihilation but before electron-antielectron annihilation, it is possible to calculate that for every excess neutron there were about five excess protons in thermodynamic equilibrium with one another through neutrino and antineutrino interactions at a temperature of about 1010 K. When the universe reached an age of a few seconds, the temperature would have dropped significantly below 1010 K, and electron-antielectron annihilation would have occurred, liberating the neutrinos and antineutrinos to stream freely through the universe. With no neutrino-antineutrino reactions to replenish their supply, the neutrons would have started to decay with a half-life of 10.6 minutes to protons and electrons (and antineutrinos). However, at an age of 1.5 minutes, well before neutron decay went to completion, the temperature would have dropped to 109 K, low enough to allow neutrons to be captured by protons to form a nucleus of heavy hydrogen, or deuterium. (Before that time, the reaction could still have taken place, but the deuterium nucleus would immediately have broken up under the prevailing high temperatures.) Once deuterium had formed, a very fast chain of reactions set in, quickly assembling most of the neutrons and deuterium nuclei with protons to yield helium nuclei. If the decay of neutrons is ignored, an original mix of 10 protons and two neutrons (one neutron for every five protons) would have assembled into one helium nucleus (two protons plus two neutrons), leaving more than eight protons (eight hydrogen nuclei). This amounts to a helium-mass fraction of 4/12 = 1/3—i.e., 33 percent. A more sophisticated calculation that takes into account the concurrent decay of neutrons and other complications yields a helium-mass fraction in the neighbourhood of 25 percent and a hydrogen-mass fraction of 75 percent, which are close to the deduced primordial values from astronomical observations. This agreement provides one of the primary successes of hot big bang theory.
    
    The deuterium abundance
    Not all of the deuterium formed by the capture of neutrons by protons would be further reacted to produce helium. A small residual can be expected to remain, the exact fraction depending sensitively on the density of ordinary matter existing in the universe when the universe was a few minutes old. The problem can be turned around: given measured values of the deuterium abundance (corrected for various effects), what density of ordinary matter needs to be present at a temperature of 109 K so that the nuclear reaction calculations will reproduce the measured deuterium abundance? The answer is known, and this density of ordinary matter can be expanded by simple scaling relations from a radiation temperature of 109 K to one of 2.735 K. This yields a predicted present density of ordinary matter and can be compared with the density inferred to exist in galaxies when averaged over large regions. The two numbers are within a factor of a few of each other. In other words, the deuterium calculation implies much of the ordinary matter in the universe has already been seen in observable galaxies. Ordinary matter cannot be the hidden mass of the universe.
    
    The very early universe
    Inhomogeneous nucleosynthesis
    One possible modification concerns models of so-called inhomogeneous nucleosynthesis. The idea is that in the very early universe (the first microsecond) the subnuclear particles that later made up the protons and neutrons existed in a free state as a quark-gluon plasma. As the universe expanded and cooled, this quark-gluon plasma would undergo a phase transition and become confined to protons and neutrons (three quarks each). In laboratory experiments of similar phase transitions—for example, the solidification of a liquid into a solid—involving two or more substances, the final state may contain a very uneven distribution of the constituent substances, a fact exploited by industry to purify certain materials. Some astrophysicists have proposed that a similar partial separation of neutrons and protons may have occurred in the very early universe. Local pockets where protons abounded may have few neutrons and vice versa for where neutrons abounded. Nuclear reactions may then have occurred much less efficiently per proton and neutron nucleus than accounted for by standard calculations, and the average density of matter may be correspondingly increased—perhaps even to the point where ordinary matter can close the present-day universe. Unfortunately, calculations carried out under the inhomogeneous hypothesis seem to indicate that conditions leading to the correct proportions of deuterium and helium-4 produce too much primordial lithium-7 to be compatible with measurements of the atmospheric compositions of the oldest stars.
    
    Matter-antimatter asymmetry
    A curious number that appeared in the above discussion was the few parts in 109 asymmetry initially between matter and antimatter (or equivalently, the ratio 10−9 of protons to photons in the present universe). What is the origin of such a number—so close to zero yet not exactly zero?
    
    At one time the question posed above would have been considered beyond the ken of physics, because the net “baryon” number (for present purposes, protons and neutrons minus antiprotons and antineutrons) was thought to be a conserved quantity. Therefore, once it exists, it always exists, into the indefinite past and future. Developments in particle physics during the 1970s, however, suggested that the net baryon number may in fact undergo alteration. It is certainly very nearly maintained at the relatively low energies accessible in terrestrial experiments, but it may not be conserved at the almost arbitrarily high energies with which particles may have been endowed in the very early universe.
    
    An analogy can be made with the chemical elements. In the 19th century most chemists believed the elements to be strictly conserved quantities; although oxygen and hydrogen atoms can be combined to form water molecules, the original oxygen and hydrogen atoms can always be recovered by chemical or physical means. However, in the 20th century with the discovery and elucidation of nuclear forces, chemists came to realize that the elements are conserved if they are subjected only to chemical forces (basically electromagnetic in origin); they can be transmuted by the introduction of nuclear forces, which enter characteristically only when much higher energies per particle are available than in chemical reactions.
    
    In a similar manner it turns out that at very high energies new forces of nature may enter to transmute the net baryon number. One hint that such a transmutation may be possible lies in the remarkable fact that a proton and an electron seem at first sight to be completely different entities, yet they have, as far as one can tell to very high experimental precision, exactly equal but opposite electric charges. Is this a fantastic coincidence, or does it represent a deep physical connection? A connection would obviously exist if it can be shown, for example, that a proton is capable of decaying into a positron (an antielectron) plus electrically neutral particles. Should this be possible, the proton would necessarily have the same charge as the positron, for charge is exactly conserved in all reactions. In turn, the positron would necessarily have the opposite charge of the electron, as it is its antiparticle. Indeed, in some sense the proton (a baryon) can even be said to be merely the “excited” version of an antielectron (an “antilepton”).
    
    supernova 1987A in the Large Magellanic Cloud
    supernova 1987A in the Large Magellanic CloudThis picture shows the faint outer rings and bright inner ring characteristic of an hourglass nebula.
    Motivated by this line of reasoning, experimental physicists searched hard during the 1980s for evidence of proton decay. They found none and set a lower limit of 1032 years for the lifetime of the proton if it is unstable. This value is greater than what theoretical physicists had originally predicted on the basis of early unification schemes for the forces of nature. Later versions can accommodate the data and still allow the proton to be unstable. Despite the inconclusiveness of the proton-decay experiments, some of the apparatuses were eventually put to good astronomical use. They were converted to neutrino detectors and provided valuable information on the solar neutrino problem, as well as giving the first positive recordings of neutrinos from a supernova explosion (namely, supernova 1987A).
    
    With respect to the cosmological problem of the matter-antimatter asymmetry, one theoretical approach is founded on the idea of a grand unified theory (GUT), which seeks to explain the electromagnetic, weak nuclear, and strong nuclear forces as a single grand force of nature. This approach suggests that an initial collection of very heavy particles, with zero baryon and lepton number, may decay into many lighter particles (baryons and leptons) with the desired average for the net baryon number (and net lepton number) of a few parts per 109. This event is supposed to have occurred at a time when the universe was perhaps 10−35 second old.
    
    Another approach to explaining the asymmetry relies on the process of CP violation, or violation of the combined conservation laws associated with charge conjugation (C) and parity (P) by the weak force, which is responsible for reactions such as the radioactive decay of atomic nuclei. Charge conjugation implies that every charged particle has an oppositely charged antimatter counterpart, or antiparticle. Parity conservation means that left and right and up and down are indistinguishable in the sense that an atomic nucleus emits decay products up as often as down and left as often as right. With a series of debatable but plausible assumptions, it can be demonstrated that the observed imbalance or asymmetry in the matter-antimatter ratio may have been produced by the occurrence of CP violation in the first seconds after the big bang. CP violation is expected to be more prominent in the decay of particles known as B-mesons. In 2010, scientists at the Fermi National Accelerator Laboratory in Batavia, Illinois, finally detected a slight preference for B-mesons to decay into muons rather than anti-muons.
    
    Superunification and the Planck era
    Planck length: why string theory is hard to test
    Planck length: why string theory is hard to testThe Planck scale is described as the arena in which both quantum mechanical and gravitational effects come into play. Brian Greene explains where the Planck values come from. This video is an episode in his Daily Equation series.
    See all videos for this article
    Why should a net baryon fraction initially of zero be more appealing aesthetically than 10−9? The underlying motivation here is perhaps the most ambitious undertaking ever attempted in the history of science—the attempt to explain the creation of truly everything from literally nothing. In other words, is the creation of the entire universe from a vacuum possible?
    
    The evidence for such an event lies in another remarkable fact. It can be estimated that the total number of protons in the observable universe is an integer 80 digits long. No one of course knows all 80 digits, but for the argument about to be presented, it suffices only to know that they exist. The total number of electrons in the observable universe is also an integer 80 digits long. In all likelihood these two integers are equal, digit by digit—if not exactly, then very nearly so. This inference comes from the fact that, as far as astronomers can tell, the total electric charge in the universe is zero (otherwise electrostatic forces would overwhelm gravitational forces). Is this another coincidence, or does it represent a deeper connection? The apparent coincidence becomes trivial if the entire universe was created from a vacuum since a vacuum has by definition zero electric charge. It is a truism that one cannot get something for nothing. The interesting question is whether one can get everything for nothing. Clearly, this is a very speculative topic for scientific investigation, and the ultimate answer depends on a sophisticated interpretation of what “nothing” means.
    
    The words “nothing,” “void,” and “vacuum” usually suggest uninteresting empty space. To modern quantum physicists, however, the vacuum has turned out to be rich with complex and unexpected behaviour. They envisage it as a state of minimum energy where quantum fluctuations, consistent with the uncertainty principle of the German physicist Werner Heisenberg, can lead to the temporary formation of particle-antiparticle pairs. In flat space-time, destruction follows closely upon creation (the pairs are said to be virtual) because there is no source of energy to give the pair permanent existence. All the known forces of nature acting between a particle and antiparticle are attractive and will pull the pair together to annihilate one another. In the expanding space-time of the very early universe, however, particles and antiparticles may separate and become part of the observable world. In other words, sharply curved space-time can give rise to the creation of real pairs with positive mass-energy, a fact first demonstrated in the context of black holes by the English astrophysicist Stephen W. Hawking.
    
    Yet Einstein’s picture of gravitation is that the curvature of space-time itself is a consequence of mass-energy. Now, if curved space-time is needed to give birth to mass-energy and if mass-energy is needed to give birth to curved space-time, which came first, space-time or mass-energy? The suggestion that they both rose from something still more fundamental raises a new question: What is more fundamental than space-time and mass-energy? What can give rise to both mass-energy and space-time? No one knows the answer to this question, and perhaps some would argue that the answer is not to be sought within the boundaries of natural science.
    
    Hawking and the American cosmologist James B. Hartle have proposed that it may be possible to avert a beginning to time by making it go imaginary (in the sense of the mathematics of complex numbers) instead of letting it suddenly appear or disappear. Beyond a certain point in their scheme, time may acquire the characteristic of another spatial dimension rather than refer to some sort of inner clock. Another proposal states that, when space and time approach small enough values (the Planck values; see below), quantum effects make it meaningless to ascribe any classical notions to their properties. The most promising approach to describe the situation comes from the theory of “superstrings.”
    
    Superstrings represent one example of a class of attempts, generically classified as superunification theory, to explain the four known forces of nature—gravitational, electromagnetic, weak, and strong—on a single unifying basis. Common to all such schemes are the postulates that quantum mechanics and special relativity underlie the theoretical framework. Another common feature is supersymmetry, the notion that particles with half-integer values of the spin angular momentum (fermions) can be transformed into particles with integer spins (bosons).
    
    The distinguishing feature of superstring theory is the postulate that elementary particles are not mere points in space but have linear extension. The characteristic linear dimension is given as a certain combination of the three most fundamental constants of nature: (1) Planck’s constant h (named after the German physicist Max Planck, the founder of quantum physics), (2) the speed of light c, and (3) the universal gravitational constant G. The combination, called the Planck length (Gh/c3)1/2, equals roughly 10−33 cm, far smaller than the distances to which elementary particles can be probed in particle accelerators on Earth.
    
    The energies needed to smash particles to within a Planck length of each other were available to the universe at a time equal to the Planck length divided by the speed of light. This time, called the Planck time (Gh/c5)1/2, equals approximately 10−43 second. At the Planck time, the mass density of the universe is thought to approach the Planck density, c5/hG2, roughly 1093 grams per cubic centimetre. Contained within a Planck volume is a Planck mass (hc/G)1/2, roughly 10−5 gram. An object of such mass would be a quantum black hole, with an event horizon close to both its own Compton length (distance over which a particle is quantum mechanically “fuzzy”) and the size of the cosmic horizon at the Planck time. Under such extreme conditions, space-time cannot be treated as a classical continuum and must be given a quantum interpretation.
    
    The latter is the goal of the superstring theory, which has as one of its features the curious notion that the four space-time dimensions (three space dimensions plus one time dimension) of the familiar world may be an illusion. Real space-time, in accordance with this picture, has 26 or 10 space-time dimensions, but all of these dimensions except the usual four are somehow compacted or curled up to a size comparable to the Planck scale. Thus has the existence of these other dimensions escaped detection. It is presumably only during the Planck era, when the usual four space-time dimensions acquire their natural Planck scales, that the existence of what is more fundamental than the usual ideas of mass-energy and space-time becomes fully revealed. Unfortunately, attempts to deduce anything more quantitative or physically illuminating from the theory have bogged down in the intractable mathematics of this difficult subject. At the present time superstring theory remains more of an enigma than a solution.
    
    Inflation
    One of the more enduring contributions of particle physics to cosmology is the prediction of inflation by the American physicist Alan Guth and others. The basic idea is that at high energies matter is better described by fields than by classical means. The contribution of a field to the energy density (and therefore the mass density) and the pressure of the vacuum state need not have been zero in the past, even if it is today. During the time of superunification (Planck era, 10−43 second) or grand unification (GUT era, 10−35 second), the lowest-energy state for this field may have corresponded to a “false vacuum,” with a combination of mass density and negative pressure that results gravitationally in a large repulsive force. In the context of Einstein’s theory of general relativity, the false vacuum may be thought of alternatively as contributing a cosmological constant about 10100 times larger than it can possibly be today. The corresponding repulsive force causes the universe to inflate exponentially, doubling its size roughly once every 10−43 or 10−35 second. After at least 85 doublings, the temperature, which started out at 1032 or 1028 K, would have dropped to very low values near absolute zero. At low temperatures the true vacuum state may have lower energy than the false vacuum state, in an analogous fashion to how solid ice has lower energy than liquid water. The supercooling of the universe may therefore have induced a rapid phase transition from the false vacuum state to the true vacuum state, in which the cosmological constant is essentially zero. The transition would have released the energy differential (akin to the “latent heat” released by water when it freezes), which reheats the universe to high temperatures. From this temperature bath and the gravitational energy of expansion would then have emerged the particles and antiparticles of noninflationary big bang cosmologies.
    
    Cosmic inflation serves a number of useful purposes. First, the drastic stretching during inflation flattens any initial space curvature, and so the universe after inflation will look exceedingly like an Einstein–de Sitter universe. Second, inflation so dilutes the concentration of any magnetic monopoles appearing as “topological knots” during the GUT era that their cosmological density will drop to negligibly small and acceptable values. Finally, inflation provides a mechanism for understanding the overall isotropy of the cosmic microwave background because the matter and radiation of the entire observable universe were in good thermal contact (within the cosmic event horizon) before inflation and therefore acquired the same thermodynamic characteristics. Rapid inflation carried different portions outside their individual event horizons. When inflation ended and the universe reheated and resumed normal expansion, these different portions, through the natural passage of time, reappeared on our horizon. And through the observed isotropy of the cosmic microwave background, they are inferred still to have the same temperatures. Finally, slight anisotropies in the cosmic microwave background occurred because of quantum fluctuations in the mass density. The amplitudes of these small (adiabatic) fluctuations remained independent of comoving scale during the period of inflation. Afterward they grew gravitationally by a constant factor until the recombination era. Cosmic microwave photons seen from the last scattering surface should therefore exhibit a scale-invariant spectrum of fluctuations, which is exactly what the Cosmic Background Explorer satellite observed.
    
    As influential as inflation has been in guiding modern cosmological thought, it has not resolved all internal difficulties. The most serious concerns the problem of a “graceful exit.” Unless the effective potential describing the effects of the inflationary field during the GUT era corresponds to an extremely gently rounded hill (from whose top the universe rolls slowly in the transition from the false vacuum to the true vacuum), the exit to normal expansion will generate so much turbulence and inhomogeneity (via violent collisions of “domain walls” that separate bubbles of true vacuum from regions of false vacuum) as to make inexplicable the small observed amplitudes for the anisotropy of the cosmic microwave background radiation. Arranging a tiny enough slope for the effective potential requires a degree of fine-tuning that most cosmologists find philosophically objectionable.
    
    Steady state theory and other alternative cosmologies
    Big bang cosmology, augmented by the ideas of inflation, remains the theory of choice among nearly all astronomers, but, apart from the difficulties discussed above, no consensus has been reached concerning the origin in the cosmic gas of fluctuations thought to produce the observed galaxies, clusters, and superclusters. Most astronomers would interpret these shortcomings as indications of the incompleteness of the development of the theory, but it is conceivable that major modifications are needed.
    
    An early problem encountered by big bang theorists was an apparent large discrepancy between the Hubble time and other indicators of cosmic age. This discrepancy was resolved by revision of Hubble’s original estimate for H0, which was about an order of magnitude too large owing to confusion between Population I and II variable stars and between H II regions and bright stars. However, the apparent difficulty motivated Bondi, Hoyle, and Gold to offer the alternative theory of steady state cosmology in 1948.
    
    By that year, of course, the universe was known to be expanding; therefore, the only way to explain a constant (steady state) matter density was to postulate the continuous creation of matter to offset the attenuation caused by the cosmic expansion. This aspect was physically very unappealing to many people, who consciously or unconsciously preferred to have all creation completed in virtually one instant in the big bang. In the steady state theory the average age of matter in the universe is one-third the Hubble time, but any given galaxy could be older or younger than this mean value. Thus, the steady state theory had the virtue of making very specific predictions, and for this reason it was vulnerable to observational disproof.
    
    The first blow was delivered by British astronomer Martin Ryle’s counts of extragalactic radio sources during the 1950s and ’60s. These counts involved the same methods discussed above for the star counts by Dutch astronomer Jacobus Kapteyn and the galaxy counts by Hubble except that radio telescopes were used. Ryle found more radio galaxies at large distances from Earth than can be explained under the assumption of a uniform spatial distribution no matter which cosmological model was assumed, including that of steady state. This seemed to imply that radio galaxies must evolve over time in the sense that there were more powerful sources in the past (and therefore observable at large distances) than there are at present. Such a situation contradicts a basic tenet of the steady state theory, which holds that all large-scale properties of the universe, including the population of any subclass of objects like radio galaxies, must be constant in time.
    
    The second blow came in 1965 with the announcement of the discovery of the cosmic microwave background radiation. Though it has few adherents today, the steady state theory is credited as having been a useful idea for the development of modern cosmological thought as it stimulated much work in the field.
    
    At various times, other alternative theories have also been offered as challenges to the prevailing view of the origin of the universe in a hot big bang: the cold big bang theory (to account for galaxy formation), symmetric matter-antimatter cosmology (to avoid an asymmetry between matter and antimatter), variable G cosmology (to explain why the gravitational constant is so small), tired-light cosmology (to explain redshift), and the notion of shrinking atoms in a nonexpanding universe (to avoid the singularity of the big bang). The motivation behind these suggestions is, as indicated in the parenthetical comments, to remedy some perceived problem in the standard picture. Yet, in most cases, the cure offered is worse than the disease, and none of the mentioned alternatives has gained much of a following. The hot big bang theory has ascended to primacy because, unlike its many rivals, it attempts to address not isolated individual facts but a whole panoply of cosmological issues. And, although some sought-after results remain elusive, no glaring weakness has yet been uncovered.</p>














































</body>


    </html>